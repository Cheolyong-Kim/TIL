# 머신러닝 딥러닝 기초 03

> 머신러닝 기법 중 하나인 선형회귀에 대해서 학습해본다.

<br/>

### 선형회귀

<br/>

- 선형회귀

  - 종족변수 y와 한 개 이상의 독립변수 x와의 선형 상관관계를 모델링하는 회귀분석 기법이다.
  - 기존의 데이터를 활용해 연속형 변수값을 예측한다.
  - ``y=ax+b``꼴의 수식을 만들고 a와 b의 값을 찾아낸다.

  - 하나 이상의 특성과 연속적인 타깃 변수 사이의 관계를 모델링하는 것이 목적이다.
  - 지도 학습의 회귀는 범주형 클래스 레이블이 아니라 연속적인 출력 값을 예측한다.

  <br/>

- 단순 선형 휘귀

  - 단순 선형 회귀는 하나의 특성(설명 변수 x)과 연속적인 타깃(응답 변수 y) 사이의 관계를 모델링한다.

  - 특성이 하나인 선형 모델 공식은 다음과 같다.
    $$
    y= w_{1}x+w_{0}
    $$
    
  - 여기서 w0는 y축 절편을 나타내고 w1은 특성의 가중치를 나타낸다.
  
  - 특성과 타깃 사이의 관계를 나타내는 선형 방정식의 가중치를 학습하는 것이 목적이다.
  
  - 이 방정식으로 훈련 데이터셋이 아닌 새로운 샘풀의 타깃 값을 예측할 수 있다.
  
  - 데이터에 가장 잘 맞는 직선을 회귀 직선이라 한다.
  
  - 회귀 직선과 훈련 샘플 사이의 직선 거리를 오프셋 또는 예측 오차인 잔차라고 한다.
  
  - K-최근접 이웃 회귀와 선형회귀 비교
  
    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsRegressor
    from sklearn.metrics import mean_absolute_error
    from sklearn.linear_model import LinearRegression
    
    X = np.array(
        [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 
         21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 
         22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 
         27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 
         36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 
         40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
         )
    Y = np.array(
        [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 
         110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 
         130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 
         197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 
         514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 
         820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 
         1000.0, 1000.0]
         )
    
    plt.scatter(X, Y)
    ```
  
    ![image_1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/1.png?raw=true)
  
    ```python
    t_x, tt_x, t_y, tt_y = train_test_split(X, Y, random_state= 43)
    
    # 학습과 테스트를 위해 x의 모양을 변경해줌
    n_t_x = t_x.reshape(-1, 1)
    n_tt_x = tt_x.reshape(-1, 1)
    
    # 학습과 테스트
    knr = KNeighborsRegressor()
    knr.fit(n_t_x, t_y)
    
    knr.score(n_tt_x, tt_y)  # 0.9662464614810213
    
    end_tt_y = knr.predict(n_tt_x)
    mae = mean_absolute_error(tt_y, end_tt_y)  # 오차 계산  # 29.69285714285715
    
    knr.score(n_t_x, t_y)  # 훈련 데이터로 점수 계산  # 0.9815010309553626
    
    knr.n_neighbors = 3  # 이웃 범위를 3으로 조정
    knr.fit(n_t_x, t_y)
    knr.score(n_tt_x, tt_y)  # 0.9625437594582437 
    
    knr.score(n_t_x, t_y)  # 모델 변경 후 훈련 데이터로 점수 계산  # 0.9878266455621827
    
    knr1 = KNeighborsRegressor()
    x = np.arange(5, 45).reshape(-1, 1)  # 비교를 위한 데이터 생성
    
    for n in [1, 5, 10]:
        knr1.n_neighbors = n
        knr1.fit(n_t_x, t_y)
        p_data = knr1.predict(x)
        plt.scatter(n_t_x, t_y)
        plt.plot(x, p_data)
        plt.title(f'n_plt{n}')
        plt.show()
    ```
  
    ![image_2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/2.png?raw=true)
  
    ![image_3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/3.png?raw=true)
  
    ![image_4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/4.png?raw=true)
  
    - 학습이 잘 진행된 것을 볼 수가 있다.
  
    ```python
    knr2 = KNeighborsRegressor(n_neighbors= 3)
    knr2.fit(n_t_x, t_y)
    knr2.predict([[50]])  # array([1033.33333333])
    ```
  
    - x의 값을 50으로 주면 1033이 출력된다. 그래프로 봤을 때는 1100이나 1200쯤에 있어야하는 것 처럼 보인다.
  
    ```python
    d, i = knr2.kneighbors([[50]])
    plt.scatter(n_t_x, t_y)
    plt.scatter(n_t_x[i], t_y[i], marker= '*')
    plt.scatter(50, 1033.3, marker= '^')
    ```
  
    ![image_5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/5.png?raw=true)
  
    - x가 50인 점과 그 점의 이웃들을 표시해봤다. k-최근접 이웃 회귀는 주어진 테스트 값의 범위 안에서최근접 값들을 사용하여 학습하기 때문에 x값이 테스트값보다 크게 되면 제대로된 결과가 나오지 않는다.
  
    ```python
    knr2.predict([[1000]])  # array([1033.33333333])
    ```
  
    - 1000을 입력값으로 줘도 1033이 출력으로 나오게 된다.
  
    ```python
    lr = LinearRegression().fit(n_t_x, t_y)
    
    lr.predict([[50]])  # [1187.92199858]
    lr.predict([[1000]])  # [35638.66987063]
    ```
  
    - 선형회귀를 사용하면 우리가 기대해던 값과 근접한 값이 출력된다.
  
    ```python
    plt.scatter(n_t_x, t_y)
    plt.plot([15, 50], [15 * lr.coef_ + lr.intercept_, 50 * lr.coef_ + lr.intercept_], c= 'r')  # 선형회귀를 통해 만들어낸 직선 그리기
    ```
  
    ![image_6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/6.png?raw=true)
  
    - 회귀선을 그어보면 학습이 어떻게 되었는지 확인할 수 있다.
    - 하지만 우리는 점들의 모임이 지수함수 꼴에 더 가깝다고 생각한다.
  
    ```python
    p_t_x = np.column_stack((n_t_x ** 2, n_t_x))
    p_tt_x = np.column_stack((n_tt_x ** 2, n_tt_x))
    ```
  
    - 2차 다항식을 풀게하기 위해 데이터를 조정해준다.
  
    ```python
    lr = LinearRegression()
    lr.fit(p_t_x, t_y)
    
    print(lr.predict([[50 ** 2, 50]]))  # [1573.06792794]
    ```
  
    - 이번엔 좀 더 우리가 기대한 값이 출력된다.
  
    ```python
    print(lr.coef_, lr.intercept_)  # w2, w1, w0 출력  # [  1.05682431 -24.53340579] 157.67745303390137
    ```
  
    - 2차 다항식은 ``y=w2x^2+w1x+w0``꼴로 나타난다. 각 가중치 값들을 출력해봤다.
  
    ```python
    p = np.arange(15, 50)
    plt.scatter(n_t_x, t_y)
    plt.plot(p, (1.05 * p ** 2) - (24.5 * p) + 157.6)
    plt.scatter([50], [1573.06])
    ```
  
    ![image_7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/7.png?raw=true)
  
    - 구한 가중치들로 2차 다항식에 대한 회귀선을 그려봤다. 우리가 원하던 지수함수 꼴의 그래프가 잘 나타난 것을 볼 수 있다.
  
  <br/>
  
- 비용함수의 개념

  - 비용함수 : 머신러닝에서 최소화해야 할 예측값과 실제값의 차이

  - 가설함수 : 예측값을 예측하는 함수
    $$
    f(x)=h_{\theta}(x)
    $$

  - 함수 입력값은 x이고 함수에서 결정할 것은 theta로, 가중치 값인 wn

  - 비용함수가 두 개의 가중치 값으로 결정됨
    $$
    J(w_{0},w_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-(y^i)^2)
    $$

    - 잔차의 제곱합 : 예측값인 가설함수와 실제값인 y값 간의 차이를 제곱해서 모두 합함
    - 손실함수 : 비용함수에서 잔차의 제곱합 부분
    - 평균 제곱 오차 : 잔차의 제곱합을 2m으로 나눈 값

  - 비용함수 그래프의 미분값이 0에 가까워지는, 즉 기울기가 0에 가까워 지는 지점을 찾는 것이 최적의 가중치를 찾는 것이다.

  - 비용함수의 편미분

    - J를 wn에 관해 미분해서 식을 최적화
      $$
      \frac{\partial J}{\partial w_{n}}=\frac{\partial J}{\partial h_{\theta}}\frac{\partial h_{\theta}}{\partial w_{n}}
      $$

      $$
      \frac{\partial J}{\partial w_{0}}=\frac{1}{m}\sum_{i=1}^{m}(w_{1}x^i+w_{0}-y^i)
      $$

      $$
      \frac{\partial J}{\partial w_{1}}=\frac{1}{m}\sum_{i=1}^{m}(w_{1}x^i+w_{0}-y^i)x^i
      $$

  <br/>

- 최소자승법
  
  - 선형대수의 표기법을 사용하여 방정식으로 선형회귀 문제를 푸는 방법
  
  - 미분한 값을 0으로 만드는 최적값 wj을 구한다.

$$
\hat{w}_{0}m+\hat{w}_{1}\sum x^i=\sum y^i
$$

$$
\hat{w}_{0}\sum x^i+\hat{w}_{1}\sum (x^i)^2=\sum y^ix^i
$$

​          -  데이터가 5개만 있다고 가정하고 행렬로 표현
$$
X=
\begin{pmatrix}
1&x^1 \\
1&x^2 \\
1&x^3 \\
1&x^4 \\
1&x^5 \\
\end{pmatrix}
$$

$$
\hat{w}=
\begin{pmatrix}
\hat{w}_{0} \\
\hat{w}_{1}
\end{pmatrix}
$$

$$
y=
\begin{pmatrix}
y^1 \\
y^2 \\
y^3 \\
y^4 \\
y^5 \\
\end{pmatrix}
$$

$$
X^TX=
\begin{pmatrix}
m&\sum x^i \\
\sum x^i&\sum (x^i)^2
\end{pmatrix}
$$

$$
(X^TX)\hat{w}=X^Ty \\
\hat{w}_{0}\sum x^i+\hat{w}_{1}\sum (x^i)^2=\sum y^ix^i\\
$$

​            - 두 식이 같음
$$
\hat{w}=(X^TX)^{-1}X^Ty
$$

​            - 식을 풀어보면
$$
\hat{w}_{0}=\bar{y}-\hat{w}_{1}\bar{x}
$$



<br/>

- 최소자승법의 활용

  - X^TX가 존재할 때 문제의 해를 구할 수 있음
  - 데이터의 개수가 피쳐의 개수보다 많은 경우가 대부분이라서 자주 사용됨
  - 반복과 사용자가 지정하는 하이퍼 매개변수가 존재하지 않아서 데이터만 있으면 쉽게 해를 구할 수 있음
  - 피쳐가 늘어나면 속도가 느려짐

  <br/>

- 경사하강법

  - 경사를 하강하면서 수식을 최소화하는 매개변수의 값을 찾아내는 방법

  - 점이 최솟값을 달성하는 방향으로 점점 내려감

    - 몇 번 적용할 것인가? : 많이 실행할수록 최솟값에 가까워짐
    - 한 번에 얼마나 많이 내려가 것인가? : 한 번에 얼마나 많은 공간을 움직일지를 기울기, 즉 경사라 부름

  - 경사 : 경사하강법의 하이퍼 매개변수

  - 경사하강법 알고리즘

    - 경사하강법의 기본 수식
      $$
      x_{new}=x_{old}-\alpha(2x_{old})
      $$

    - xold는 현재의 x값, xnew는 경사 값이 적용된 후 생성된 값

    - 경사만큼의 변화가 계속 x에 적용되어 x의 최솟값을 찾음

  - 경사하강법에서 개발자가 결정해야할 것

    - 학습률(learning rate)을 얼마로 할 것인가?, 즉 a값을 결정
      - 값이 너무 작으면 충분히 많은 반복을 적용해도 원하는 최적값을 찾지 못하는 경우 발생
      - 값이 너무 크면 발산하여 최솟값에 수렴하지 않거나 시간이 너무 오래 걸림
    - 얼마나 많은 반복으로 돌릴 것인가?
      - 반복 횟수가 충분하지 않다면 최솟값을 찾지 못하는 경우 발생
      - 반복 횟수가 너무 많다면 필요없는 시간을 허비할 수 있음

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  
  x = np.arange(-10, 10, 1)
  fx = x ** 2  # y = x ** 2
  
  plt.plot(x, fx)
  ```

  ![image_8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/8.png?raw=true)

  ```python
  x_new = 10
  d = []  # 변화된 x기록
  y = []  # y기록
  
  learng_rate = 0.1  # 변화량 a
  
  for i in range(100):
      x_old = x_new
      x_new = x_old - learng_rate * 2 * x_old
      d.append(x_new)
      y.append(x_new ** 2)
      
  plt.plot(x, fx)
  plt.scatter(d, y)
  plt.plot(d, y)
  ```

  ![image_9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/9.png?raw=true)

<br/>

- 경사하강법으로 선형회귀

  - 훈련/테스트 분할

    - 머신러닝에서 데이터를 학습하기 위한 학습 데이터셋과 학습의 결과로 생성된 모델의 성능을 평가하기 위한 테스트 데이터셋으로 나눔
    - 모델이 새로운 데이터셋에도 일반화하여 처리할 수 있는지를 확인

  - 모델이 데이터에 과다적합된 경우

    - 생성된 모델이 특정 데이터에만 잘 맞아서 해당 데이터셋에 대해서는 성능을 발휘할 수 있지만 새로운 데이터셋에서는 전혀 성능을 낼 수 없다

  - 모델이 데이터에 과소적합된 경우

    - 기존 학습 데이터를 제대로 예측하지 못함

  - 홀드아웃 메소드

    - 전체 데이터셋에서 일부를 학습 데이터와 테스트 데이터로 나누는 일반적인 데이터 분할 기법

      - 전체 데이터에서 랜덤하게 학습 데이터셋과 테스트 데이터셋을 나눔

      - 일반적으로 7:3 또는 8:2 정도의 비율

      - sklearn 모듈이 제공하는 train_test_split함수를 사용

        ```python
        from sklearn.model_selection import train_test_split
        
        x, y = np.arange(10).reshape((5, 2)), range(5)
        
        t_x, tt_x, t_y, tt_y = train_test_split(x, y, test_size= 0.33, random_state= 42, )  # test_size로 테스트, 학습용 데이터의 비율을 결정할 수 있다
        ```

  - 선형회귀의 성능 측정 지표

    - MAE(Mean Absolute Error): 평균 절대 잔차

      - 모든 테스트 데이터에 대해 예측값과 실제값의 차이에 대해 절댓값을 구하고, 이 값을 모두 더한 후에 데이터의 개수만큼 나눈 결과

      - 직관적으로 예측값과 실측값의 차이를 알 수 있음

      - sklearn 모듈에서는 median_absolute_error 함수 사용

        ```python
        from sklearn.metrics import mean_absolute_error
        
        y_true = [3, -0.5, 2, 7]
        y_pred = [2.5, 0.0, 2, 8]
        
        mean_absolute_error(y_true, y_pred)  # 0.5
        ```

    - RMSE(Root Mean Squared Error) : 평균제곱근 오차

      - 오차에 대해 제곱을 한 다음 모든 값을 더하여 평균을 낸 후 제곱근을 구함
      - MAE에 비해 상대적으로 값의 차이가 더 큼
      - 차이가 크게 나는 값에 대해 패널티를 주고 싶다면 RMSE 값을 사용
      - sklearn 모듈에서 RMSE를 직접적으로 지원하지는 않고 mean_squared_error 함수활용 간접지원 및 squared=False로 사용 가능

    - R-squared : 결정계수

      - 두 개의 값의 증감이 얼마나 일관성을 가지는지 나타내는 지표
      - 예측값이 크면 클수록 실제값도 커지고, 예측값이 작으면 실제값도 작아짐
      - 두 개의 모델 중 어떤 모델이 조금 더 상관성이 있는지를 나타낼 수 있지만, 값의 차이 정도가 얼마인지는 나타낼 수 없다는 한계가 있음
      - sklearn 모듈에서 r2_score 사용

      <br/>

```python
import matplotlib.pyplot as plt
import numpy as np
import random

# 데이터 생성
def data_gen(n, b, v):
    x = np.zeros(shape= (n, 2))
    y = np.zeros(shape= n)
    
    for i in range(0, n):
        x[i][0] = 1
        x[i][1] = i
        
        y[i] = (i + b) + random.uniform(0, 1) * v
        
    return x, y

x, y = data_gen(100, 25, 10)
plt.plot(x[:, 1], y, 'o')
```

![image_10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/10.png?raw=true)

```python
# 경사하강법
def g_descent(x, y, th, a, m, ns):
    xT = x.T  # np.transpose(x), x.transpose(), x.T
    th_l = []  # xnew의 변화를 기록
    cost_l = []  # 비용 함수의 변화를 기록
    
    for i in range(0, ns):
        h = np.dot(x, th)
        loss = h - y  # 오차 생성
        cost = (np.sum(loss ** 2)) / (2 * m)  # 비용 함수
        g = np.dot(xT, loss) / m
        th= th - (a * g)
        
        if i % 250 == 0:  # 250번째 반복에만 변화 기록
            th_l.append(th)
        
        cost_l.append(cost)
        
    return th, np.array(th_l), cost_l

m, n = np.shape(x)
ns = 5000  # 반복 횟수
a = 0.0005  # 학습률
th = np.ones(n)

th, th_l, cost_l = g_descent(x, y, th, a, m, ns)

y_p_step = np.dot(x, th_l.T)

plt.plot(x[:, 1], y, 'o')

for i in range(0, 20, 5):
    plt.plot(x[:, 1], y_p_step[:, i], label= f'L_{i}')

plt.legend()
```

![image_11](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/11.png?raw=true)

- 반복이 진행될수록 정확해짐

```python
y_p = np.dot(x, th)

plt.plot(x[:, 1], y, 'o')
plt.plot(x[:, 1], y_p)
```

![image_12](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/12.png?raw=true)

```python
ck_i = range(len(cost_l))

plt.scatter(ck_i, cost_l)
```

![image_13](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/13.png?raw=true)

- 코스트함수가 반복할 때마다 0에 가까워지는 것을 보여줌

<br/>

- 경사하강법 종류

  - 전체-배치 경사하강법

    - 모든 데이터를 한 번에 입력하는 경사하강법
      - 배치(batch) : 하나의 데이터셋
    - 하나의 값에 대한 경사도를 구한 다음 값들을 업데이트
    - 실제로는 각 데이터의 경사도를 모두 더해 하나의 값으로 가중치를 업데이트
    - 점 한 개씩을 사용하여 가중치를 업데이트하지 않는 이유
      -  시간이 오래 걸림
      - 시작점에 따라 지역 최적화에 빠짐 : 그래프 전체에서 최솟점을 찾지 못하고 부분최솟점에 최적점이 위치
    - 전체-배치 경사하강법의 특징
      - 업데이트 횟수 감소
        - 가중치 업데이트 횟수가 줄어 계산상 효율성 상승
      - 안정적인 비용함수 수렴
        - 모든 값의 평균을 구하기 때문에 일반적으로 경사하강법이 갖는 지역 최적화 문제를 만날 가능성도 있음
      - 업데이트 속도 증가
        - 대규모 데이터셋을 한 번에 처리하면 모델의 매개변수 업데이트 속도에 문제 발생이 적어짐
        - 데이터가 백만 단위 이상을 넘어가면 하나의 머신에서는 처리가 불가능해져서 메모리 문제가 발생

  - 확률적 경사하강법(SGD)

    - 학습용 데이터에서 샘플들을 랜덤하게 뽑아서 사용
    - 대상 데이터를 섞은 후, 일반적인 경사하강법처럼 데이터를 한 개씩 추출하여 가중치 업데이트
    - SGD의 장점
      - 빈번한 업데이트를 하기 때문에 데이터 분석가가 모델의 성능 변화를 빠르게 확인
      - 데이터의 특성에 따라 훨씬 더 빠르게 결과값을 냄
      - 지역 최적화를 회피
    - SGD의 단점
      - 대용량 데이터를 사용하는 경우 시간이 매우 오래 걸림
      - 결과의 마지막 값을 확인하기 어려움
      - 흔히 ''튀는 현상'이라고 불리는 현상으로 비용함수의 값이 줄어들지 않고 계속 변화할 때 정확히 언제 루프가 종료되는지 알 수 없어 판단이 어렵다.

  - 미니-배치 경사하강법 또는 미니-배치 SGD

    - 데이터의 랜덤한 일부분만 입력해서 경사도 평균을 구해 가중치 업데이트
    - 에포크(epoch)
      - 데이터를 한 번에 모두 학습시키는 횟수
      - 전체-배치 SDG를 한 번 학습하는 루프가 실행될 때 1 에포크의 데이터가 학습된다고 말함
    - 배치 사이즈
      - 한 번에 학습되는 데이터의 개수
      - 총 데이터가 5012개 있고 배치 사이즈가 512라면 10번의 루프가 돌면서 1에포크를 학습했다고 말함
    - 에포크와 배치 사이즈는 하이퍼 매개변수이므로 데이터 분석가가 직접 선정함

    ```python
    import numpy as np
    class LinearRegressionGD(object):
        def __init__(self, fit_intercept=True, copy_X=True,
                     eta0=0.001, epochs=1000, batch_size = 1, 
                     weight_decay=0.9, shuffle = True):
            self.fit_intercept = fit_intercept
            self.copy_X = copy_X
            self._eta0 = eta0
            self._epochs = epochs
    
            self._cost_history = []
    
            self._coef = None
            self._intercept = None
            self._new_X = None
            self._w_history = None
            self._weight_decay = weight_decay
            self._batch_size = batch_size
            self._is_SGD = shuffle
    
        def gradient(self, X, y, theta):
            return X.T.dot(self.hypothesis_function(X, theta)-y) / len(X)
    
        def fit(self, X, y):
            self._new_X = np.array(X)
            y = y.reshape(-1, 1)
    
            if self.fit_intercept:
    
                intercept_vector = np.ones([len(self._new_X), 1])
                self._new_X = np.concatenate(
                        (intercept_vector, self._new_X), axis=1)
    
            theta_init = np.random.normal(0, 1, self._new_X.shape[1])         
            # weight값 초기화
            self._w_history = [theta_init]
            self._cost_history = [self.cost(
                            self.hypothesis_function(self._new_X, theta_init), y)]
    
            theta = theta_init
    
            for epoch in range(self._epochs):
                X_copy = np.copy(self._new_X)
    
                if self._is_SGD:
                    np.random.shuffle(X_copy)
    
                batch = len(X_copy) // self._batch_size 
    
    
                for batch_count  in range(batch):
                    X_batch = np.copy(
                                X_copy[batch_count * self._batch_size : (batch_count+1) & self._batch_size])
    
                    gradient = self.gradient(X_batch  , y, theta).flatten()
                    theta = theta - self._eta0 * gradient
    
                if epoch % 100 == 0:
                    self._w_history.append(theta)
                    cost = self.cost(
                        self.hypothesis_function(self._new_X, theta), y)
                    self._cost_history.append(cost)
                self._eta0 = self._eta0 * self._weight_decay
    
            if self.fit_intercept:
                self._intercept = theta[0]
                self._coef = theta[1:]
            else:
                self._coef = theta
        def cost(self, h, y):
            return 1/(2*len(y)) * np.sum((h-y).flatten() ** 2)
    
        def hypothesis_function(self, X, theta):
            return X.dot(theta).reshape(-1, 1)
    
        def gradient(self, X, y, theta):
            return X.T.dot(self.hypothesis_function(X, theta)-y) / len(X)
    
        def fit(self, X, y):
            self._new_X = np.array(X)
            y = y.reshape(-1, 1)
    
            if self.fit_intercept:
                intercept_vector = np.ones([len(self._new_X), 1])
                self._new_X = np.concatenate(
                        (intercept_vector, self._new_X), axis=1)
    
            theta_init = np.random.normal(0, 1, self._new_X.shape[1])
            self._w_history = [theta_init]
            self._cost_history = [self.cost(
                            self.hypothesis_function(self._new_X, theta_init), y)]
    
            theta = theta_init
    
            for epoch in range(self._epochs):
                gradient = self.gradient(self._new_X, y, theta).flatten()
                theta = theta - self._eta0 * gradient
    
                if epoch % 100 == 0:
                    self._w_history.append(theta)
                    cost = self.cost(
                        self.hypothesis_function(self._new_X, theta), y)
                    self._cost_history.append(cost)
                self._eta0 = self._eta0 * self._weight_decay
    
            if self.fit_intercept:
                self._intercept = theta[0]
                self._coef = theta[1:]
            else:
                self._coef = theta
    
        def predict(self, X):
            test_X = np.array(X)
    
            if self.fit_intercept:
                intercept_vector = np.ones([len(test_X), 1])
                test_X = np.concatenate(
                        (intercept_vector, test_X), axis=1)
    
                weights = np.concatenate(([self._intercept], self._coef), axis=0)
            else:
                weights = self._coef
    
            return test_X.dot(weights)
    
        @property
        def coef(self):
            return self._coef
    
        @property
        def intercept(self):
            return self._intercept
    
        @property
        def weights_history(self):
            return np.array(self._w_history)
    
        @property
        def cost_history(self):
            return self._cost_history
    ```

    ```python
    import pandas as pd
    df = pd.read_csv('train.csv')
    ```

    ![image_14](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/14.png?raw=true)

    ```python
    import matplotlib.pyplot as plt
    
    X = df['x'].values.reshape(-1, 1)
    Y = df['y'].values
    
    gd_lr = LinearRegressionGD(batch_size= 1, shuffle= False, eta0= 0.001, epochs= 10000)
    gd_lr.fit(X, Y)
    
    plt.plot(range(len(gd_lr.cost_history)), gd_lr.cost_history)
    ```

    ![image_15](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/15.png?raw=true)

    ```python
    sgd_lr = LinearRegressionGD(batch_size= 1, shuffle= True)  # SGD
    bgd_lr = LinearRegressionGD(batch_size= len(X), shuffle= False)  # 전체-배치
    msgd_lr = LinearRegressionGD(batch_size= 200, shuffle= True)  # 미니-배치
    sgd_lr.fit(X, Y)
    bgd_lr.fit(X, Y)
    msgd_lr.fit(X, Y)
    
    plt.plot(range(len(sgd_lr.cost_history)), sgd_lr.cost_history, label= 'sgd')
    plt.plot(range(len(bgd_lr.cost_history)), bgd_lr.cost_history, label= 'bgd')
    plt.plot(range(len(msgd_lr.cost_history)), msgd_lr.cost_history, label= 'msgd')
    plt.legend()
    plt.show()
    ```

    ![image_16](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2003/16.png?raw=true)

  - 과대 적합

    - 머신러닝 모델을 학습할 때 학습 데이터셋에 지나치게 최적화하여 발생하는 문제
    - 모델을 지나치게 복잡하게 학습하여 학습 데이터셋에서는 모델 성능이 높게 나타나지만 정작 새로운 데이터가 주어졌을 때 정확한 예측/분류를 수행하지 못함
    - 데이터의 피쳐가 너무 많을 때 발생

  - 과소적합

    - 과대적합의 반대 개념으로서 머신러닝 모델이 충분히 복잡하지 않아(최적화가 제대로 수행되지 않아) 학습 데이터의 구조/패턴을 정확히 반영하지 못하는 문제
    - 데이터의 피쳐가 없을 때 발생

  - 평향(bias)

    - 학습된 모델이 학습 데이터에 대해 만들어 낸 예측값과 실제값과의 차이
    - 모델의 결과가 얼마나 한쪽으로 쏠려 있는지 나타냄
    - 편향이 크면 학습이 잘 진행되기는 했지만 해당 데이터에만 잘 맞음

  - 분산(variance)

    - 학습된 모델이 테스팅 데이터에 대해 만들어 낸 예측값과 실제값과의 차이
    - 모델의 결과가 얼마나 퍼져 있는지 나타냄

  - 과대적합이 발생할 때 경사하강법 루프가 진행될수록 학습 데이터셋에 대한 비용함수의 값은 줄어들지만 테스트 데이터셋의 비용함수 값은 증가

  - 선형회귀 외에도 결정트리나 딥러닝처럼 연산에 루프가 필요한 모든 알고리즘에서 똑같이 발생

  - 오컴의 면도날 원리

    - 보다 적은 수의 논리로 설명이 가능한 경우, 많은 수의 논리를 세우지 않는다.
    - 경제성의 원리 또는 단순성의 원리
    - 머신러닝에서는 너무 많은 피쳐를 사용하지 마라는 것을 의미한다

  - 선형회귀에서 과대적합 해결책

    - 더 많은 데이터 활용하기 : 오류가 없고, 분포가 다양한 데이터를 많이 확보
    - 피쳐의 개수 줄이기 : 필요한 피쳐만 잘 찾아 사용
    - 적절한 매개변수 선정하기 : SGD의 학습률이나 루프의 횟수처럼 적절한 하이퍼 매개변수를 선정
    - 정규화 적용하기 : 데이터 편향성에 따라 필요 이상으로 증가한 피쳐의 가중치 값을 적절히 줄이는 규제 수식을 추가

  - 리지 회귀

    - L2 정규화

    - 좌표평면의 원점에서 점까지의 거리를 나타내어 벡터의 크기를 측정하는 기법

    - X는 하나의 벡터

    - L2 놈(L2 norm) : 벡터 각 원소들의 제곱합에 제곱근을 취함

    - 리지 회귀는 L2 놈을 선형회귀의 비용함수 수식에 적용
      $$
      J(w_{0},w_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-(y^i)^2)+\frac{\lambda}{2}\sum_{j=1}^{n} \theta^2_{j}
      $$
      

    - 뒷부분에 새로 붙인 수식은 페널티텀으로, 모델의 가중치 값들의 제곱의 합

      - 가중치 값이 조금이라도 커질 때 비용함수에 매우 큰 영향을 줌
      - 람다가 클수록 전체 페널티텀의 값이 커져 세타값이 조절됨
      - 람다는 사람이 직접 값을 입력하는 하이퍼 매개변수

    - 리지 회귀 수식을 미분하면 j의 값이 1 이상일 때 페널티가 적용됨

  - 라쏘 회귀

    - L1 정규화

    - 가중치에 페널티텀을 추가하는데, 기존 수식에 L1 놈 페널티를 추가하여 계산
      $$
      J(w_{0},w_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i)-(y^i)^2)+\frac{\lambda}{2}\sum_{j=1}^{n} |\theta_{j}|
      $$
      

    - L1 놈 : 절대값을 사용하여 거리를 측정