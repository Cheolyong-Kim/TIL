# 통계기반 비정형 텍스트 분석 03

<br/>

### 언어 모델

<br/>

#### 언어 모델

<br/>

- 언어 모델이란?

  - 단어 시퀀스에 확률을 할당하는 모델, 이전 단어들을 이용하여 다음 단어를 예측함
  - 통계를 이용한 방법과 인공신경망을 이용한 두가지 방법이 있음
  - 최근에는 인공신경망을 이용한 모델이 좋은 성능을 내는 경우가 많기 때문에 더 많이 사용.

  <br/>

- 통계적 언어 모델

  - 확률 기반의 언어모델
  - m개의 단어가 주어졌을 때 문장으로써 성립될 확률을 출력함으로써 이 단어 열이 실제로 현실에서 사용될 수 있는 문장인지를 판별하는 모델
  - 조건부 확률을 사용
  - 문장에 대한 확률을 카운트 기반의 접근으로 계산

  <br/>

- 조건부 확률

  - 주어진 사건이 일어났다는 가정 하에 다른 한 사건이 일어날 확률

  - 기존의 단어를 바탕으로 만들어질 다음 단어의 확률

  - 문장의 확률을 수식으로

    ![m1](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/m1.png?raw=true)

  - 3가지 모형

    1. 유니그램 모형
    2. 바이그램 모형
    3. N그램 모형

<br/>

---

#### 언어 모델 (유니그램, 바이그램, N그램)

<br/>

- 유니그램 모델

  - 모든 단어의 활용이 완전히 독립이라면 단어 열의 확률은 다음과 같이 각 단어의 확률의 곱이 되는 기반의 모델

    ![m2](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/m2.png?raw=true)

  <br/>

- 바이그램 모델 or 마코프 모형

  - 단어의 활용이 바로 전 단어에만 의존할 때의 단어 열의 확률

    ![m3](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/m3.png?raw=true)

  <br/>

- 통계적 언어 모델의 한계

  1. 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다
  2. 확률을 계산하고 싶은 문장이 길어질수록 갖고 있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높다

  <br/>

- N그램 모델

  - 통계적 언어 모델의 한계를 극복하기 위해 기준 단어의 앞 단어 중 임의의 n개 개수만 포함해서 카운트하여 확률을 계산하는 모델. 
  - n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주
  - n그램을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존

  <br/>

- N그램 모델의 한계

  1. 희소 문제 : 충분한 데이터가 주어지지 못해서 언어를 정확히 모델링하지 못하는 문제가 발생할 수 있음
  2. n을 선택하는 trade-off 문제

<br/>

---

#### 파이썬 코드를 통해 보는 언어 모델

<br/>

- 영어 문장

  ![j1](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j1.png?raw=true)

  - 바이그램은 ('전 단어', '기준 단어') 모양으로 나눠진다.
  - n그램은 지정한 n-1개 만큼 '전 단어'와 1개의 '기준 단어'로 나눠진다.

  <br/>

  ![j2](통계기반 비정형 텍스트 분석 03.assets/j2.png)

  - n그램의 n을 2로 지정하면 바이그램과 동일한 동작을 한다
  - ``pad_left``와 ``pad_right``를 True로 설정하면 첫 단어, 마지막 단어의 전 단어를 빈 공간으로 생성한다.
  - ``left_pad_symbol``와 ``right_pad_symbol``에 문자열을 설정하면 ``pad_left``와 ``pad_right``으로 설정한 빈 공간을 설정한 문자열로 채운다

  <br/>

  ![j3](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j3.png?raw=true)

  - ConditionalFreqDist모듈 로 조건부 빈도 분포를 기록할 수 있다. (API 참고 [링크](https://tedboy.github.io/nlps/generated/generated/nltk.ConditionalFreqDist.html))
  - 생성된 모델에 'SS', 즉 가장 첫 단어가 무엇이 올 지 테스트해보니 'I'가 빈도수 1로 출력되는 것을 볼 수 있다.

  <br/>

  ![j4](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j4.png?raw=true)

  - nltk.corpus에 존재하는 영화 리뷰 데이터를 사용한다

  <br/>

  ![j5](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j5.png?raw=true)

  - 'SS'로 테스트 했을 때, 가장 빈도수가 높은 단어 10가지를 출력해본 결과이다. 아래는 순서대로 'the', 'moive', '.'을 테스트한 결과이다.

  ![j6](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j6.png?raw=true)

  <br/>

  ![j7](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j7.png?raw=true)

  ![j8](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j8.png?raw=true)

  <br/>

  ![j9](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j9.png?raw=true)

  - ConditionalProbDist 모듈로 설정한 기준 단어의 다음에 입력한 단어가 올 확률을 구할 수 있다. (API 참고 [링크](https://docs.huihoo.com/nltk/0.9.5/api/nltk.probability.ConditionalProbDist-class.html))

  <br/>

  ![j10](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j10.png?raw=true)

  - s_sc_f는 바이그램 언어 모델의 확률 연산 수식을 함수로 구현한 것이다.
  - 'the', 'movie', '.' 의 경우 0.0039 정도의 확률로 어느정도 실현 가능한 문장이라는 것을 알 수 있다.
  - 'movie', '.', 'the' 의 경우 3.08*10^-17 정도로 거의 실현 불가능한 문장이라는 것을 알 수 있다.

  <br/>

  ![j11](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j11.png?raw=true)

  - ConditionalProbDist 객체의 메소드 generate를 사용하면 확률에 근거해서 랜덤으로 기준 단어의 다음 단어를 생성해준다.
  - random의 seed 값을 고정하면 텍스트 생성의 결과가 고정된다.
  - 각각 'SS', 'she', 'and', 'fine', 'effect' 기준 단어의 결과로 나온 단어를 또 generate하는 코드이다.

  <br/>

  ![j12](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j12.png?raw=true)

  - 무한반복을 통해 생성된 단어가 'SE', 즉 문장의 끝이 나올 때까지 단어를 생성하고 그 단어를 배열에 저장
  - 배열에 저장된 단어들을 join하여 문장으로 출력
  - 해당 예제에서는 언어 생성에서 고려해야 할 것들을 고려하지 않았음. (소문자 -> 대문자, 아포스트로피 등)

<br/>

- 영어 문장 생성에 대한 큰 틀

  1. 데이터 수집

     ```python
     from nltk.corpus import movie_reviews
     
     data = movie_reviews.sents()
     ```

     <br/>

  2. 데이터 전처리

     ```python
     data_l = []
     for i in data:
         bg = ngrams(i, 2, pad_left = True, pad_right = True, left_pad_symbol = 'SS', right_pad_symbol = 'SE')
         data_l += [t for t in bg]
     ```

     <br/>

  3. 모델 학습

     ```python
     cfd = ConditionalFreqDist(data_l)
     cpd = ConditionalProbDist(cfd, MLEProbDist)
     ```

     <br/>

  4. 검증(생략)

     <br/>

  5. 동작

     ```python
     st = 'SS'
     all_str = []
     while True:
         random.seed(10)
         st = cpd[st].generate()
         all_str.append(st + ' ')
         if st == 'SE':
             all_str.pop()
             break
         
     ''.join(all_str)
     ```

  <br/>

- 한국어 문장

  ![j13](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j13.png?raw=true)

  - 영화 리뷰 데이터를 받아와서 사용
  - 첫 열은 필요없는 정보라 슬라이싱하여 없애줌
  - 데이터 수집 단계

  <br/>

  ![j14](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j14.png?raw=true)

  - 필요한 리뷰 문장들만 사용
  - 데이터 전처리 단계

  <br/>

  ![j15](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j15.png?raw=true)

  - konlpy 패키지를 사용해서 단어 토큰화
  - 형태소 분석을 하면 토큰화된 단어와 품사가 각각 출력되기 때문에 두 데이터를 합쳐줌
  - 파이썬의 tqdm 패키지로 코드 동작 현황을 볼 수 있음 (이미지상 검은 막대)
  - nltk는 기본적으로 영어에 관한 패키지이지만 언어 모델에 관해서는 언어의 종류가 딱히 문제가 되지 않으니 잘 동작함

  <br/>

  ![j16](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j16.png?raw=true)

  - 랜덤 시드값은 고정
  - 단어 생성을 하면 단어와 함께 품사가 붙어나오기 때문에 미리 설정한 '/'를 기준으로 잘라줌
  - pykospacing 패키지를 활용해서 최대한 자연스럽게 띄워쓰기 구현
  - 해당 예제에서도 띄워쓰기, 품사 등 한국어 언어 생성에서 고려해야 하는 것들을 고려하지 않은 예제

<br/>

---

#### 단어 표현

<br/>

- 단어의 표현 방법

  1. 국소 표현

     - 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법
     - 이산 표현이라고도 함
     - ex) BoW, DTM, TF-IDF

     <br/>

  2. 분산 표현

     - 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법
     - 연속 표현이라고도 함
     - ex) Word2Vec, FastText

<br/>

- Bag of Words(BoW)

  - 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법

  <br/>

- 문서 단어 행렬(Document-Term Matrix, DTM)

  - 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현
  - BoW들을 결합한 표현 방법

  <br/>

- 문서 단어 행렬의 한계

  1. 희소 표현
  2. 단순 빈도 수 기반 접근 : 단순히 빈도만 표현하기 때문에 쓸모없는 단어의 빈도수가 높게 나오면 모델의 학습 시 과대적합의 문제가 발생할 수 있다

  <br/>

- TF-IDF(Term Frequency-Inverse Document Frequency)

  - 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 표현하는 방법

  - 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 주요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에서 사용

  - 함수 설명

    - tf(d, t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수

    - df(t) : 특정 단어 t가 등장한 문서의 수

    - idf(d, t) : df(t)에 반비례하는 수

      ![m3](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/m4.png?raw=true)

<br/>

- 파이썬 코드로 보는 단어 표현

  ![j17](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j17.png?raw=true)

  - konlpy 패키지를 통해 단어 토큰화
  - '?'는 필요없기 때문에 없애줌
  - 형태소 분석한 단어를 순서대로 딕셔너리에 {키 : 순서} 모양으로 저장
  - d_l에는 단어의 출현 빈도를 저장

  <br/>

  ![j18](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j18.png?raw=true)

  - sklearn 패키지의 CountVectorizer 모듈로 단어를 벡터화 (API 참고 [링크](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html))
  - CountVectorizer 클래스의 vocabulary_ 메소드로 벡터화된 단어들과 각각의 인덱스를 확인할 수 있다.

  <br/>

  ![j19](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j19.png?raw=true)

  - 여러 문장들로 이루어진 sentence를 정의
  - 미리 학습시켜둔 v 모델에 sentence를 넣어 각 문장들의 단어 벡터를 확인
  - v2 모델에는 stop_word를 통해 불용어를 지정하여 학습, vocabulary를 확인하면 불용어인 '나는'을 제외하고 벡터화된 것을 확인할 수 있다.

  <br/>

  ![j20](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j20.png?raw=true)

  - sentence의 문장들을 형태소 단위로 단어 토큰화하여 t_l에 저장해둔다.

  <br/>

  ![j21](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j21.png?raw=true)

  - v3모델은 형태소 토큰화된 단어들을 학습시킨 모델이다

<br/>

- 파이썬으로 보는 TF-IDF

  ![j22](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j22.png?raw=true)

  - text의 문장들을 공백을 기준으로 split하고 set으로 만들어 중복을 제거한다. 그리고 그렇게 만들어진 set객체를 list로 형변환한다.

  <br/>

  ![j23](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j23.png?raw=true)

  - 각각 tf, idf, tf-idf 수식을 함수로 구현한 것이다.

  <br/>

  ![j24](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j24.png?raw=true)

  - tf함수의 결과를 t_l 리스트에 저장(각 문장이 d, 단어는 voc에서 하나씩)
  - t_l 리스트를 판다스를 사용해 데이터 프레임화 시킴

  <br/>

  ![j25](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j25.png?raw=true)

  - idf함수의 결과를 idf_l 리스트에 저장
  - idf_l 리스트를 판다스를 사용해 데이터 프레임화

  <br/>

  ![j26](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j26.png?raw=true)

  - tfidf함수의 결과를 tf_idf_l 리스트에 저장
  - tf_idf_l 리스트를 판다스를 사용하여 데이터 프레임화
  - 각 단어들이 빈도가 아닌 가중치로 표현된 것을 볼 수 있음

  <br/>

  ![j27](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%203/j27.png?raw=true)

  - sklearn 패키지의 TfidVectorizer로 간단하게 tf-idf 구현 가능 (API 참고 [링크](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))

