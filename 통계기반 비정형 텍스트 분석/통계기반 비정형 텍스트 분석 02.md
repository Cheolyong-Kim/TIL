# 통계기반 비정형 텍스트 분석 02

<br/>

### 유용한 패키지

<br/>

#### pykospacing

<br/>

- pykospacing 패키지

  - 띄워쓰기가 없는 한국어 문장을 형태소 단위로 분석해 띄워쓰기를 만들어 주는 패키지이다.
  - [git 링크](https://github.com/haven-jeon/PyKoSpacing)

  ```python
  t = '인수위, 네이버·카카오 정조준... "포털 뉴스 알고리즘 검증·공개"'
  n_t = t.replace(' ','')  # 띄워쓰기 제거
  ```

  ```python
  from pykospacing import Spacing
  sc = Spacing()
  e_t = sc(n_t)  # 띄워쓰기 생성
  
  # e_t 출력 결과 : '인수위, 네이버·카카오정조준..."포털 뉴스 알고리즘 검증·공개"'
  ```

<br/>

---

#### hanspell

<br/>

- hanspell 패키지

  - 맞춤법에 맞지 않은 문장을 고쳐주는 패키지
  - [git 링크](https://github.com/9beach/hanspell)

  ```python
  t1 = '나는 외 않되? 나도 할수있으면 돼지 '
  
  from hanspell import spell_checker
  ck_t = spell_checker.check(t1)  # 맞춤법 검사
  ```

  ![j1](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j1.png?raw=true)

  <br/>

  ```python
  end_t = ck_t.checked
  # end_t 출력 결과 : '나는 왜 안돼? 나도 할 수 있으면 되지'
  ```

  <br/>

  ```python
  ck_t1 = spell_checker.check(n_t).checked
  
  # ck_t1 출력 결과 : '인수위, 네이버·카카오 정조준..."포털 뉴스 알고리즘 검증·공개"'
  ```

  - 띄워쓰기도 체크해주지만 기본적으로 맞춤법을 맞춰주는 모듈이기 때문에 띄워쓰기 뿐만 아니라 본문의 내용이 바뀔 수도 있으니 주의

<br/>

---

#### soynlp

<br/>

- soynlp 패키지

  - 한국어 분석을 위한 패키지
  - 학습데이터를 이용하지 않으면서 데이터에 존재하는 단어를 찾거나, 문장을 단어열로 분해, 혹은 품사 판별을 할 수 있는 비지도학습 접근법을 지향
  - [git 링크](https://github.com/lovit/soynlp)

  ```python
  from soynlp import DoublespaceLineCorpus
  from soynlp.word import WordExtractor
  
  all_data = DoublespaceLineCorpus('data1.txt')
  type(all_data)
  
  # all_data의 타입 : <soynlp.utils.utils.DoublespaceLineCorpus>
  ```

  ```python
  i = 0
  for w in all_data:
      if len(w) > 0:
          print(w)
          i += 1
      if i == 2:
          break
  ```

  ![j2](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j2.png?raw=true)

  - 반복문으로 하나씩 꺼내서 확인할 수 있음

  <br/>

  ```python
  w_e = WordExtractor()
  ```

  - 문서집합에서 자주 등장하는 연속된 단어열을 단어라 정의한다면, 통계를 이용하여 이를 추출할 수 있음. (soynlp github 문서 Word Extraction의 Word Extraction 소개)

  <br/>

  ```python
  w_e.train(all_data)
  w_e_t = w_e.extract()
  ```

  - all_data를 학습

  <br/>

  ```python
  w_e_t['반포한'].cohesion_forward
  w_e_t['반포한강공원'].cohesion_forward
  w_e_t['반포한강공원에'].cohesion_forward
  
  # 출력결과
  # 0.08838002913645132
  # 0.37891487632839754
  # 0.33492963377557666
  ```

  - '반포한강공원' 에서 '에'가 추가되자 확률값이 떨어지는 것을 볼 수 있다.

  <br/>

  ```python
  n = 0
  for i in w_e_t.items():
      print(i)
      n += 1
      if n == 3:
          break
  ```

  ![j3](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j3.png?raw=true)

  - w_e_t의 타입이 딕셔너리이기 때문에 키, 벨류 값을 꺼내어 확인할 수 있다.
  - 각 단어의 점수값에 대한 정보는 해당 링크 참조 [링크](https://github.com/lovit/soynlp/blob/master/tutorials/wordextractor_lecture.ipynb)

  <br/>

  ```python
  from soynlp.tokenizer import LTokenizer
  sc = {w : sc.cohesion_forward for w, sc in w_e_t.items()}
  
  # 스코어값을 딕셔너리로 저장
  ```

  ```python
  l_tk = LTokenizer(scores = sc)
  ck_t = '자료의 정보를 구분하기 위해 문서 작성을 했다.'
  l_tk.tokenize(ck_t, flatten = False)
  ```

  ![j4](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j4.png?raw=true)

  - L parts 에는 명사/동사/형용사/부사가 위치할 수 있다. 어절에서 L만 잘 인식한다면 나머지 부분이 R parts가 된다.
  - cohesion_forward를 단어 점수로 하여 나눈 결과 '자료'와 '의'는 잘 구분 됐는데 '정보를', '작성을' 같은 경우는 구분이 되지 않은 것을 볼 수 있다.

  <br/>

  ```python
  from soynlp.tokenizer import MaxScoreTokenizer
  m_tk = MaxScoreTokenizer(scores = sc)
  ck_t = '자료의 정보를 구분하기 위해 문서 작성을 했다.'
  m_tk.tokenize(ck_t, flatten = False)
  ```

  ![j5](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j5.png?raw=true)

  - MaxScoreTokenizer는 단어를 각 단어의 점수가 가장 높은 것을 기준으로 구분한다
  - cohesion_forward를 기준으로 나눈 결과 LTokenizer와 크게 다른 것이 없는 것을 볼 수 있다.

  <br/>

  ```python
  w_e_t['정'].cohesion_forward
  w_e_t['정보'].cohesion_forward
  w_e_t['정보를'].cohesion_forward
  
  # 출력 결과
  # 0
  # 0.0745964316057774
  # 0.13143652206154155
  ```

  - cohesion_forward 점수를 확인해보면 '점수'보다 '점수를'의 점수가 높아 '점수를'이 채택된 것을 볼 수 있다.

  <br/>

  ```python
  from soynlp import DoublespaceLineCorpus
  from soynlp.word import WordExtractor
  
  all_data = DoublespaceLineCorpus('data1.txt', iter_sent = True)
  ```

  - DoublespaceLineCorpus메소드에 iter_sent를 True값으로 주면 한 줄 씩 문장으로 가져온다

  <br/>

  ```python
  n = 0 
  for i in all_data:
      print(i)
      n += 1
      if n == 4:
          break
  ```

  ![j6](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j6.png?raw=true)

  <br/>

- Nomalizer

  - 반복되는 이모티콘의 정리 및 한글, 혹은 텍스트만 남기기 위한 함수

  ```python
  t1 = '영화가 너무 웃겨'
  t2 = '영화가 너무 웃겨 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ'
  t3 = '영화가 너무 웃겨 ㅋㅋ'
  t4 = '영화가 너무 웃겨 ㅋㅋㅋ'
  ```

  ```python
  from soynlp.normalizer import *
  
  emoticon_normalize(t1)
  emoticon_normalize(t2)
  
  # 출력 결과
  # '영화가 너무 웃겨'
  # '영화가 너무 웃겨 ㅋㅋㅋ'
  ```

  - 'ㅋㅋㅋㅋㅋㅋ' 의 'ㅋ' 수가 normalize 후에 수가 줄어든 것을 볼 수 있다.

  <br/>

  ```python
  emoticon_normalize(t2, num_repeats = 2)
  
  # 출력 결과 : '영화가 너무 웃겨 ㅋㅋ'
  ```

  - num_repeats로 몇 글자까지 줄일 것인지 선택할 수 있다.

  <br/>

  ```python
  t_e = '후후후후후후훗'
  emoticon_normalize(t_e)
  
  # 출력 결과 : '후후훗'
  ```

  - 이모티콘이 아니더라도 반복되는 글자면 적용가능

<br/>

---

#### ckonlpy

<br/>

- ckonlpy 패키지

  - ckonlpy는 konlpy를 사용하면서 불편한 부분을 직접 커스텀하여 사용할 수 있게 만들어진 패키지이다.

  ```python
  t = '은호가 교실로 들어갔다.'
  tw = Okt()
  
  tw.nouns(t)  # 명사 추출
  tw.morphs(t)  # 형태소 추출
  
  # 출력 결과
  # ['은', '호가', '교실']
  # ['은', '호가', '교실', '로', '들어갔다', '.']
  ```

  ```python
  tw.pos(t)  # 품사 추출
  ```

  ![j7](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j7.png?raw=true)

  - 명사, 형태소, 품사 추출 모두 '은호'를 '은',  '호가' 따로따로 추출하고 있다.

  <br/>

  ```python
  from ckonlpy.tag import Twitter
  tw2 = Twitter()
  tw2.morphs(t)
  
  # 출력 결과 : ['은', '호가', '교실', '로', '들어갔다', '.']
  ```

  - 따로 설정을 하지 않으면 ckonlpy도 konlpy와 동일하게 동작한다.

  <br/>

  ```python
  tw2.add_dictionary('은호', 'Noun')
  
  tw2.morphs(t)
  tw2.nouns(t)
  
  # 출력 결과
  # ['은호', '가', '교실', '로', '들어갔다', '.']
  # ['은호', '교실']
  ```

  ```python
  tw2.pos(t)
  ```

  ![j8](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j8.png?raw=true)

  - add_dictionary로 '은호'를 명사로 지정해주면서 의도대로 추출되도록 한다.

<br/>

---

#### 한국어 비정형 텍스트 정형화

<br/>

- konlpy를 활용한 정형화

  ```python
  from konlpy.corpus import kobill
  kobill.fileids()
  ```

  ![j9](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j9.png?raw=true)

  - konlpy에서 제공하는 kobill데이터를 활용하여 예제 진행

  <br/>

  ```python
  t_data2 = kobill.open('1809890.txt').read()
  ```

  - 불러온 데이터는 [링크](https://konlpy-ko.readthedocs.io/ko/v0.4.3/data/)참조

  <br/>

  ```python
  from konlpy.tag import Okt
  tw1 = Okt()
  s_data = tw1.nouns(t_data2)
  ```

  ![j10](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j10.png?raw=true)

  - 형태소 분석기를 이용하여 단어 토큰화

  <br/>

  ```python
  s_data = [x for x in s_data if len(x) > 1]
  ```

  - 한 글자 단어들은 의미가 없는 경우가 많으니 제외

  <br/>

  ```python
  from tensorflow.keras.preprocessing.text import Tokenizer
  ck_t = Tokenizer()
  ck_t.fit_on_texts(s_data)
  encoded = ck_t.texts_to_sequences(s_data)
  ```

  ![j11](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j11.png?raw=true)

  - 단어 리스트를 이용해 단어 정형화

  <br/>

- 정리

  - 문서 로드

    ```python
    t_data3 = kolaw.open('constitution.txt')
    ```

    <br/>

  - 문장 토큰화

    ```python
    from soynlp import DoublespaceLineCorpus
    all_data = DoublespaceLineCorpus(t_data3.name, iter_sent = True)
    ```

    <br/>

  - 단어 토큰화

    ```python
    from konlpy.tag import Okt
    
    tw = Okt()
    tw_l = []
    for data in all_data:
        tw_l.append([x for x in tw.nouns(data) if len(x) > 1])
    ```

    ![j12](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j12.png?raw=true)

    <br/>

  - 정수 인코딩

    ```python
    end_ck = Tokenizer()
    end_ck.fit_on_texts(tw_l)
    end_ck.word_index
    ```

    ![j13](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j13.png?raw=true)

    <br/>

    ```python
    encoded = end_ck.texts_to_sequences(tw_l)
    ```

    ![j14](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j14.png?raw=true)

    <br/>

  - 패딩

    ```python
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    
    end_data = pad_sequences(encoded)
    ```

    ![j15](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j15.png?raw=true)

<br/>

---

#### 시각화

<br/>

- nltk 패키지를 활용한 시각화

  - 데이터 로드

  ```python
  from konlpy.corpus import kolaw
  t_data = kolaw.open(kolaw.fileids()[0]).read()
  ```

  <br/>

  - 단어 토큰화 

  ```python
  from konlpy.tag import Okt
  tw = Okt()
  n_t_data = tw.nouns(t_data)
  ```

  <br/>

  - nltk 패키지 Text 모듈 사용

  ```python
  from nltk import Text
  ck_data = Text(n_t_data, name = 'kolaw')
  ```

  <br/>

  - 선 그래프

  ```python
  import matplotlib.pyplot as plt
  ck_data.plot(15)
  plt.show()
  ```

  ![j16](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j16.png?raw=true)

  - 워드 크라우드
    - vocab메소드로 단어의 빈도수를 간단하게 구할 수 있음

  ```python
  ck_data.vocab()
  ```

  ![j17](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j17.png?raw=true)

  <br/>

  ```python
  from wordcloud import WordCloud
  f = 'C:\Windows\Fonts\malgun.ttf'
  wc = WordCloud(font_path = f, width = 1000, height = 600, background_color = 'white') 
  plt.imshow(wc.generate_from_frequencies(ck_data.vocab()))
  plt.axis('off')
  ```

  ![j18](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j18.png?raw=true)

  <br/>

- 정리

  - 데이터 수집

    ```python
    from konlpy.corpus import kolaw
    t_data = kolaw.open(kolaw.fileids()[0]).read()
    ```

    <br/>

  - 전처리(형태소 분석기 이용)

    ```python
    from konlpy.tag import Okt
    tw = Okt()
    n_t_data = tw.nouns(t_data)
    ```

    <br/>

  - 시각화 작업을 위한 NLTK Text 클래스 이용

    ```python
    from nltk import Text
    ck_data = Text(n_t_data, name = 'kolaw')
    ```

    <br/>

  - 정리한 내용을 이용하여 데이터 분석

    ```python
    from wordcloud import WordCloud
    f = 'C:\Windows\Fonts\malgun.ttf'
    wc = WordCloud(font_path = f, width = 1000, height = 600, background_color = 'white') 
    plt.imshow(wc.generate_from_frequencies(ck_data.vocab()))
    plt.axis('off')
    ```

    ![j18](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j18.png?raw=true)

<br/>

---

#### sklearn을 활용한 데이터 전처리

<br/>

- DictVectorizer

  - 딕셔너리로 존재하는 단어, 빈도 모음을 간단한 매트릭스로 표현
    - 행 = 문장 수, 열 = 단어 수,  값 = 빈도 수

  ```python
  from sklearn.feature_extraction import DictVectorizer
  
  v = DictVectorizer(sparse = False)  # sparse = False : 매트릭스로 출력
  D = [{'A': 1, 'B': 2}, {'B': 3, 'C': 1}]
  X = v.fit_transform(D)
  print(X)
  
  # 출력 결과
  # [[1. 2. 0.]
  #  [0. 3. 1.]]
  ```

  <br/>

  - 학습 데이터에 없는 단어는 세지 못함

  ```python
  D2 = {'A': 5, 'B': 1, 'D': 100}
  
  v.transform(D2)
  
  # 출력 결과
  # array([[5., 1., 0.]])
  ```

<br/>

- CountVectorizer

  - 각 문장에 존재하는 단어들의 토큰을 만들고 그 수를 셈

  ```python
  from sklearn.feature_extraction.text import CountVectorizer
  
  corpus = [
      'This is the first document.',
      'This is the second second document.',
      'And the third one.',
      'Is this the first document?',
      'The last document?',
  ]
  ```

  ```python
  v1 = CountVectorizer()
  v1.fit(corpus)
  v1.vocabulary_
  ```

  ![j19](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j19.png?raw=true)

  <br/>

  - 원-핫 인코딩이 아니기 때문에 0과 1로만 구분되지 않음

  ```python
  v1.transform(['This is the first document. This This']).toarray()
  ```

  ![j20](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j20.png?raw=true)

  <br/>

  - 마찬가지로 학습 데이터에 없는 단어는 배제

  ```python
  v1.transform(['This is the first document. data']).toarray()
  ```

  ![j21](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%202/j21.png?raw=true)

