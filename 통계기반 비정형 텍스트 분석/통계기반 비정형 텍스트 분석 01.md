# 통계기반 비정형 텍스트 분석 01

<br/>

### 텍스트 전처리

<br/>

##### 말뭉치(코퍼스)

<br/>

- 말뭉치(코퍼스)란?

  - 말뭉치 또는 코퍼스(영어 : corpus, 복수형 : corpora)

  - 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합을 얘기함

  - 코퍼스를 데이터세트라고도 부름

    <br/>

- 말뭉치의 종류

  1. 단일 언어 코퍼스 : 하나의 언어로 이루어짐

  2. 이중 언어 코퍼스 : 2개의 언어로 이루어짐

  3. 다국어 코퍼스 : 3개 이상의 언어로 이루어짐

  <br/>

- 텍스트 전처리

  - 자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태시 해당 데이터를 용도에 맞게 토큰화 & 정제 & 정규화를 진행해야 한다.

  - 데이터 분석

    - 데이터 수집

    - 데이터 전처리

      -> 토큰화

      -> 정제

      -> 정규화

  <br/>

---

##### 토큰화

<br/>

1. 단어 토큰화

   - 토큰의 기준을 단어로 하여 토큰화 하는 것

   - 단어는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주됨

   - 보통 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제 작업을 수행하는 것만으로 해결되지 않음

     -> 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생함

   - 띄어쓰기 단위로 자를 시 단어 토큰 구분이 망가지는 언어도 존재

     -> 대표적으로 한국어

   - 이미지로 보는 단어 토큰화

     ![i1](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/i1.png?raw=true)

   - 토큰화 진행 시, 예상하지 못한 경우가 있어서 토큰화의 기준을 생각해봐야 하는 경우가 발생 (ex, 영어 아포스트로피 토큰화 문제)

   - 토큰화 고려 사항

     -> 구두점이나 특수 문자를 단순 제외해서는 안된다.

     - 단어들을 걸러낼 때, 구두점이나 특수 문자를 단순히 제외하는 것은 옳지 않음

     - 구두점 조차도 하나의 토큰으로 분류하기도 함

     - Ex)

       => 마침표

       => 단어 자체에 구두점 : m.p.h나 Ph.D나 AT&T 특수 문자의 달러($)나 슬래시(/)

       => 숫자 사이에 컴마

     -> 줄임말과 단어 내에 띄어쓰기가 있는 경우

     - 토큰화 작업에서 종종 영어권 언어의 아포스트로피(`)는 압축된 단어를 다시 펼치는 역할을 하기도 함

   - Penn Treebank Tokenization의 규칙

     -> 영어 토큰화 표준으로 사용되는 규칙

     -> 규칙 1. 하이푼으로 구성된 단어는 하나로 유지

     -> 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리
   
   <br/>
   
2. 문장 토큰화

   - 문장 단위로 구분하는 작업
   - 코퍼스는 문장 단위로 구분되어 있지 않아 이를 사용하고자 하는 용도에 맞게 문장 토큰화가 필요
   - 단어 토큰화와 같이 문장의 끝을 구분할 때 주의가 필요함.
   
   <br/>
   
3. 한국어 토큰화
   
   - 영어는 어느 정도를 제외하면 띄어쓰기를 기준 토큰화를 해도 잘 작동함
   
   - 한국어는 영어와는 달리 띄어쓰기만으로 토큰화를 하기에 부족
   
   - 한국어의 경우에는 띄어쓰기 단위가 되는 단위를 '어절'이라고 하는데 어절 토큰화는 한국NLP에서 지양되고 있음
   
   - 한국어가 영어와는 다른 형태를 가지는 교착어라는 점에서 기인(교착어 : 조사, 어미 등을 붙여서 말을 만드는 언어)
   
   - 영어와는 달리 한국어에는 조사가 존재. 대부분의 한국어 NLP에서 조사는 분리해줄 필요가  있음
   
   - 한국어 토큰화에서는 형태소라는 개념을 이해해야 함
   
     - 형태소
   
     ​        -> 뜻을 가진 가장 작은 말의 단위
   
     ​        -> 자립 형태소
   
     ​             ==> 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소
   
     ​             ==> 그 자체로 단어가 됨
   
     ​             ==> 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있음
   
     ​        -> 의존 형태소
   
     ​             ==> 다른 형태소와 결합하여 사용되는 형태소
   
     ​             ==> 접사, 어미, 조사, 어간 
   
   - 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.
   
     - 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어이기 때문
     - 한국어에 띄어쓰기가 보편화된 것도 근대(1933년)의 일이다.
     - EX1) 이렇게띄어쓰기를전혀하지않고글을썼다고하더라고글을이해할수있다.
     - EX2) Tobeornottobethatisthequestion

   
   <br/>
   
4. 토큰화 예제

   - 영어는 nltk, 한국어는 kss, konlpy 라이브러리를 활용해 토큰화를 진행한다.
     - [nltk API 문서](https://www.nltk.org/)
     - [kss API 문서](https://pypi.org/project/kss/)
     - [konlpy API 문서](https://konlpy.org/ko/latest/index.html)

   ```python
   import nltk, kss, konlpy
   from nltk.tokenize import word_tokenize, WordPunctTokenizer
   from tensorflow.keras.preprocessing.text import text_to_word_sequence
   ```

   - 영어 토큰화 방법 1

     ```python
     t = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
     ```

     ```python
     word_tokenize(t)
     ```

     ![j1](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j1.png?raw=true)

   - 영어 토큰화 방법 2

     ```python
     WordPunctTokenizer().tokenize(t)
     ```

     ![j2](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j2.png?raw=true)

   - 영어 토큰화 방법 3

     ```python
     text_to_word_sequence(t)
     ```

     ![j3](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j3.png?raw=true)

     - 각 방법마다 토큰화의 결과가 조금씩 다른 것을 확인할 수 있다. 
     - 내가 어떤 데이터를 가지고 분석을 할 지 잘 생각한 뒤에 상황에 맞게 여러 도구를 사용해서 토큰화를 하는 것이 중요하다.

   - TreebankWordTokenizer

     ```python
     from nltk.tokenize import TreebankWordTokenizer
     t2 = "Don't be fooled by the dark sounding name, hom-data Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
     
     TreebankWordTokenizer().tokenize(t2)
     ```

     ![j4](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j4.png?raw=true)

     - Penn Treebank Tokenization의 규칙을 적용한 단어 토큰화이다.

   - 문장 토큰화

     ```python
     t3 = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
     
     from nltk.tokenize import sent_tokenize
     sent_tokenize(t3)
     ```

     ![j6](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j6.png?raw=true)

     <br/>

     ```python
     t4 = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, a PH.D. the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
     
     sent_tokenize(t4)
     ```

     ![j7](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j7.png?raw=true)

     - 문장에 a PH.D. 라는 단어를 추가해도 문장 단위로 잘 분리된다.

   - 한국어 문장 토큰화

     ```python
     import kss
     t5 = '이번 수업은 오늘 진행 됩니다. 그리고 내일 부터는 휴일 입니다. 다음주 월요일날 2차 수업이 진행 됩니다.'
     
     kss.split_sentences(t5)
     
     # ['이번 수업은 오늘 진행 됩니다.', '그리고 내일 부터는 휴일 입니다.', '다음주 월요일날 2차 수업이 진행 됩니다.']
     ```

<br/>

---

##### 태깅

<br/>

- 태깅이란?

  - 단어는 표기는 같지만 품사에 따라 단어의 의미가 달라지기도 함
  - 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 아는 것이 도움이 됨

  <br/>

- 품사 태깅

  - 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓는 작업

  <br/>

- 태깅 예제

  ```python
  from nltk.tokenize import word_tokenize
  from nltk.tag import pos_tag
  
  t6 = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
  t_t = word_tokenize(t6)
  ```

  ![j8](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j8.png?raw=true)

  ```python
  pos_tag(t_t)
  ```

  ![j9](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j9.png?raw=true)

  - 단어 토큰화를 우선 한다음, 태깅을 진행한다.

  ```python
  from konlpy.tag import Okt
  from konlpy.tag import Kkma
  
  n1 = Okt()
  
  t7 = "우리는 즐거운 어린이날 부터 휴일 입니다. 고생한 여러분 휴일을 즐기세요."
  n1.morphs(t7)
  ```

  ![j10](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j10.png?raw=true)

  ```python
  n1.pos(t7)
  ```

  ![j11](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j11.png?raw=true)

  - 한국어도 영어와 동일한 순서로 진행.

  ```python
  n1.nouns(t7)
  
  # ['우리', '어린이날', '부터', '휴일', '고생', '여러분', '휴일']
  ```

  - 명사를 따로 추출할 수도 있다.

  ```python
  n2 = Kkma()
  n2.morphs(t7), n2.pos(t7), n2.nouns(t7)
  ```

  ![j12](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j12.png?raw=true)

  <br/>

---

##### 정제와 정규화

<br/>

- 정제

  - 갖고 있는 코퍼스로부터 노이즈 데이터를 제거

  - 완벽한 정제 작업은 어렵기도 하고 할 필요도 없음

    -> 노이즈 데이터가 하나도 없는 상태라면 과도한 피쳐때문에 과대적합이 발생할 수 있음

  <br/>

- 정규화

  - 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들기
  
  <br/>
  
- 규칙에 기반한 통합

  - 정규화 규칙의 예로서 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용할 수 있음

  <br/>

- 대, 소문자 통합

  - 대부분의 글은 소문자로 작성되기 때문에, 대, 소문자 통합 작업은 대부분 대문자를 소문자로 변환하는 소문자 변환작업
  - 대문자와 소문자는 인간의 입장에서 봤을 땐 같은 문자지만 컴퓨터는 두 문자가 다르다고 인식하기 때문에 통합이 필요함

  <br/>

- 불필요한 단어의 제거

  - 등장 빈도가 적은 단어
  - 길이가 짧은 단어

  <br/>

- 정규화 기법 중 단어의 개수를 줄일 수 있는 방법

  1. 표제어 추출
  2. 어간 추출

  - 단어의 빈도수를 기반으로 문제를 풀고자 하는 자연어 처리 문제에서 주로 사용
  - 자연어 처리에서 전처리 정규화의 지향점은 언제나 갖고 있는 복잡성을 줄이는 일

  <br/>

- 표제어 추출

  - '표제어' 또는 '기본 사전형 단어' 정도의 의미
  - 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단
  - 표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행

  <br/>

- 어간

  - 단어의 의미를 담고 있는 단어의 핵심 부분

  <br/>

- 접사

  - 단어에 추가적인 의미를 주는 부분

  <br/>

- 형태학적 파싱은 어간, 접사를 분리하는 작업을 말함

  <br/>

- 어간 추출

  - 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업
  - 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있음

  <br/>

- 한국어

  - 한국어는 5언 9품사의 구조를 가지고 있음

    1. 체언 : 명사, 대명사, 수사
    2. 수식언 : 관형사, 부사
    3. 관계언 : 조사
    4. 독립언 : 감탄사
    5. 용언 : 동사, 형용사

    - 이 중 용언에 해당되는 동사와 형용사는 어간과 어미의 결합으로 구성

  - 동사변화

    - 용언의 어간이 어미를 가지는 일
    - 규칙, 불규칙 형이 있음

  - 어간

    - 용언을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(ex. 긋다, 긋고, 그어서, 그어라)

  - 어미

    - 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행

  <br/>

- 불용어

  - 실제 의미 분석을 하는데 거의 기여하는 바가 없는 단어들
  - 한국어에서 불용어를 제거하는 방법으로 간단하게는 토큰화 후에 조사, 접속사 등을 제거
  - 사용자가 직접 불용어 사전을 만들어 사용 가능
  - 불용어가 많은 경우에는 파일을 불러와서 사용 가능

  <br/>

- 정제와 정규화 예제

  ```python
  words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
  ```

  ```python
  from nltk.stem import WordNetLemmatizer
  
  f = WordNetLemmatizer().lemmatize
  [f(word) for word in words]
  ```

  ![j14](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j14.png?raw=true)

  - 표제어를 추출한 결과이다.
  - 결과가 이상한 것도 조금씩 보인다.

  ```python
  f('has', 'v'), f('dies', 'v')
  
  # 'have', 'die'
  ```

  - 품사를 지정해주면 깔끔하게 표제어가 추출된다.

  ```python
  t1 = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
  
  from nltk.tokenize import word_tokenize
  
  data = word_tokenize(t1)
  ```

  ![j15](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j15.png?raw=true)

  ```python
  from nltk.stem import PorterStemmer
  
  f1 = PorterStemmer().stem
  [f1(word) for word in data]
  ```

  ![j16](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j16.png?raw=true)

  - 토큰화를 진행한 다음에 표제어 추출을 진행한다

  ```python
  from nltk.stem import PorterStemmer
  from nltk.stem import LancasterStemmer
  
  f2 = PorterStemmer().stem
  f3 = LancasterStemmer().stem
  
  d1 = [f2(word) for word in words]
  d2 = [f3(word) for word in words]
  
  print(words, d1, d2, sep = '\n')
  ```

  ![j17](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j17.png?raw=true)

  - 모듈마다 어간이 각각 다르게 추출되는 것을 볼 수 있다. 
  - 토큰화와 마찬가지로 내가 분석할 내용에 따라 다른 모듈들을 사용하는 것이 좋다.

  <br/>

- 불용어 예제

  ```python
  from nltk.corpus import stopwords
  from nltk.tokenize import word_tokenize
  from konlpy.tag import Okt
  
  d = word_tokenize(t1)
  ```

  ![j18](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j18.png?raw=true)

  ```python
  ck_d = stopwords.words('english')
  ```

  - 해당 모듈의 메소드로 입력한 국가의 자주 사용되는 불용어를 가져올 수 있다.

  ```python
  end_l = []
  for i in d:
      if i not in ck_d:
          end_l.append(i)
          
  print(d, end_l, sep = '\n')
  ```

  ![j19](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j19.png?raw=true)

  - 불용어 처리, 리스트를 사용해서 간단하게 구현 가능

  ```python
  t_d = '오늘은 몸이 아파요. 오늘은 강의 내용이 빠르게 진행이 되나요 모르겠어요'
  stop_t_d = '이 은 요 빠르게 모르겠어요'.split()
  ```

  ```python
  okt = Okt()
  t_dd = okt.morphs(t_d)
  ```

  ![j20](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j20.png?raw=true)

  ```python
  end_d = [i for i in t_dd if i not in stop_t_d]
  # ['오늘', '몸', '아파요', '.', '오늘', '강의', '내용', '진행', '되나요']
  ```

  - 굳이 모듈로 불용어를 불러오지 않고 직접 설정해서 처리해줄 수도 있다.

<br/>

---

##### 정규표현식

<br/>

- 정규표현식

  |   정규표현식   |                             의미                             |
  | :------------: | :----------------------------------------------------------: |
  |       .        |        한 개의 임의의 문자를 나타냄 (개행 문자 제외)         |
  |       ?        |    앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있음     |
  |       *        | 앞의 문자가 무한대로 존재할 수 있고, 존재하지 않을 수도 있음 |
  |       +        |               앞의 문자가 최소 한 개 이상 존재               |
  |       ^        |                 뒤의 문자열로 문자열이 시작                  |
  |       $        |                 앞의 문자열로 문자열이 끝남                  |
  |     {숫자}     |                        숫자만큼 반복                         |
  | {숫자1, 숫자2} |                숫자1 이상 숫자2 이하만큼 반복                |
  |    {숫자, }    |                      숫자 이상만큼 반복                      |
  |       []       |          대괄호 안의 문자들 중 한 개의 문자와 매치           |
  |    [^문자]     |                해당 문자를 제외한 문자를 매치                |
  |       \|       |                       A 또는 B의 의미                        |

  <br/>

  | 자주 쓰이는 문자 규칙 |               의미                |
  | :-------------------: | :-------------------------------: |
  |           \           |    역 슬래쉬 문자 자체를 의미     |
  |          \d           |         모든 숫자를 의미          |
  |          \D           |  숫자를 제외한 모든 문자를 의미   |
  |          \s           |            공백을 의미            |
  |          \S           |     공백을 제외한 문자를 의미     |
  |          \w           |       문자 또는 숫자를 의미       |
  |          \W           | 문자 또는 숫자가 아닌 문자를 의미 |

  <br/>

  | 정규표현식 모듈 함수 |                             동작                             |
  | :------------------: | :----------------------------------------------------------: |
  |     re.compile()     |                 정규표현식을 컴파일하는 함수                 |
  |     re.search()      |      문자열 전체에 대해 정규표현식과 매치되는지를 검색       |
  |      re.match()      |        문자열의 처음이 정규표현식과 매치되는지를 검색        |
  |      re.split()      |    정규표현식을 기준으로 문자열을 분리하여 리스트로 리턴     |
  |     re.findall()     | 문자열에서 정규표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴. 만약, 매치디는 문자열이 없다면 빈 리스트가 리턴됨. |
  |    re.finditer()     | 문자열에서 정규표현식과 매치되는 모든 경우의 문자열에 대한 이터레이터 객체를 리턴 |
  |       re.sub()       | 문자열에서 정규표현식과 일치하는 부분에 대해서 다른 문자열로 대체 |

  <br/>

- 정규표현식 예제

  ```python
  import re
  ```

  - . 기호 (단어 한개)

    ```python
    r = re.compile('d.t')  # d와 t사이의 아무 문자 하나
    r.search('ggg')
    ```

    - ggg에서는 d와 t도 없을 뿐더러 사이의 문자도 없으므로 아무런 결과가 나오지 않는다.

    ```python
    r.search('dat')
    # <re.Match object; span=(0, 3), match='dat'>
    ```

    - 해당되는 문자가 있다면 그 범위와 매치된 문자열의 정보를 보여준다.

    ```python
    r.search('datwwwdat')
    # <re.Match object; span=(0, 3), match='dat'>
    ```

  - ? 기호 (입력 이후 중간 문자의 유무 상관없이 끝문자가 나오면 인식)

    ```python
    r = re.compile('d?t')
    r.search('ggg')
    # 출력 없음
    ```

    ```python
    r.search('data')
    # <re.Match object; span=(2, 3), match='t'>
    ```

    ```python
    r.search('t')
    # <re.Match object; span=(0, 1), match='t'>
    ```

  - *기호 (연결된 문자의 중복 개수 상관없이 인식, 0개도 가능)

    ```python
    r = re.compile('ab*c')
    r.search('ac')
    # <re.Match object; span=(0, 2), match='ac'>
    ```

    ```python
    r.search('abbbbbbbc')
    # <re.Match object; span=(0, 9), match='abbbbbbbc'>
    ```

    ```python
    r.search('abc')
    # <re.Match object; span=(0, 3), match='abc'>
    ```

  - +기호 (연결된 문자의 중복 개수 상관없이 인식, 0개 불가능)

    ```python
    r = re.compile('ab+c')
    r.search('abc')
    # <re.Match object; span=(0, 3), match='abc'>
    ```

  - ^기호 (기호 이후 문자열로 시작되는 문자열 인식)

    ```python
    r = re.compile('^ab')
    r.search('abc')
    # <re.Match object; span=(0, 2), match='ab'>
    ```

    ```python
    r.search('asdasffasdbc')
    # 출력 없음
    ```

    ```python
    r.search('abqwjgjsdhgjkhg')
    # <re.Match object; span=(0, 2), match='ab'>
    ```

  - {숫자} 기호 (기호 앞에 있는 단어의 개수가 기호 안의 숫자만큼 있을 때 인식)

    ```python
    r = re.compile('ab{2}c')
    r.search('abc')
    # 출력 없음
    ```

    ```python
    r.search('abbc')
    # <re.Match object; span=(0, 4), match='abbc'>
    ```

  - {숫자, 숫자} 기호 (기호 앞에 있는 단어의 개수가 기호 안의 숫자 범위만큼 있을 때 인식)

    ```python
    r = re.compile('ab{2,5}c')
    r.search('abbc')
    # <re.Match object; span=(0, 4), match='abbc'>
    ```

    ```python
    r.search('abbbc')
    # <re.Match object; span=(0, 5), match='abbbc'>
    ```

    ```python
    r.search('abbbbc')
    # <re.Match object; span=(0, 6), match='abbbbc'>
    ```

    ```python
    r.search('abbbbbbc')
    # 출력 없음
    ```

  - {숫자, }기호 (슬라이싱과 비슷)

    ```python
    r = re.compile('ab{2,}c')
    r.search('abbc')
    # <re.Match object; span=(0, 4), match='abbc'>
    ```

    ```python
    r.search('abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbc')
    # <re.Match object; span=(0, 32), match='abbbbbbbbbbbbbbbbbbbbbbbbbbbbbbc'>
    ```

    ```python
    r = re.compile('ab{,2}c')
    r.search('abbc')
    # <re.Match object; span=(0, 4), match='abbc'>
    ```

    ```python
    r.search('abbbc')
    # 출력 없음
    ```

    ```python
    r.search('abc')
    # <re.Match object; span=(0, 3), match='abc'>
    ```

  - []기호 (괄호 안에 있는 문자의 유무)

    ```python
    r = re.compile('[abc]')
    r.search('abbc')
    # <re.Match object; span=(0, 1), match='a'>
    ```

    ```python
    r.search('d')
    # 출력 없음
    ```

    ```python
    r.search('cab')
    # <re.Match object; span=(0, 1), match='c'>
    ```

    ```python
    r.search('dcba')
    # <re.Match object; span=(1, 2), match='c'>
    ```

    <br/>

    ```python
    r = re.compile('[a-z]')  # -로 범위 선정 가능
    r.search('a')
    # <re.Match object; span=(0, 1), match='a'>
    ```

    ```python
    r.search('y')
    # <re.Match object; span=(0, 1), match='y'>
    ```

  - [^]기호 (괄호 내부에 있는 문자가 없는 문자열 인식)

    ```python
    r = re.compile('[^abc]')
    r.search('y')
    # <re.Match object; span=(0, 1), match='y'>
    ```

    ```python
    r.search('a')
    # 출력 없음
    ```

  - 내장 함수

    ```python
    r = re.compile('[a.c]')  # 조건 설정
    r.search('data0- aac')  # 조건을 만족하기만 하면 인식
    # <re.Match object; span=(1, 2), match='a'>
    ```

    ```python
    r.match('data0- aac')  # 시작부터 조건에 맞는지 확인
    # 출력 없음
    ```

    ```python
    data = 'data1 data2 data3'
    re.split(' ', data) # 자르기
    # ['data1', 'data2', 'data3']
    ```

    ```python
    re.findall('dat', data)
    # ['dat', 'dat', 'dat']
    ```

    ```python
    re.sub('\W+', "수정", data)
    # 'data1수정data2수정data3'
    ```

  - 문자규칙

    ```python
    t = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
    ```

    ```python
    from nltk.tokenize import RegexpTokenizer
    ck1 = RegexpTokenizer('[\w]+')
    ck1.tokenize(t)
    ```

    ![j21](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j21.png?raw=true)

    ```python
    ck2 = RegexpTokenizer('\W+', gaps = True)
    ck2.tokenize(t)
    ```

    ![j22](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j22.png?raw=true)

    - gaps 옵션을 True로 지정하면 입력한 문자 규칙을 토큰화의 기준으로 삼게 된다.
    - 위 코드에서 gaps = True를 지우면 공백밖에 나오질 않는다.

<br/>

---

##### 정수 인코딩

<br/>

- 정수 인코딩

  - 각 단어를 고유한 정수에 맵핑시키는 전처리 작업
  - 단어를 빈도수 순으로 정렬한 단어 집합을 만들고, 빈도수가 높은 순서대로 낮은 숫자까지 정수를 부여하는 방법

  <br/>

- 패딩

  - 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업

  <br/>

- 원-핫 인코딩

  - 단어집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식
  - 단어집합
    - 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주
  - 한계
    - 벡터를 저장하기 위해 필요한 공간이 계속 늘어남
    - 즉, 벡터의 차원이 늘어남 (= 과대적합의 위험이 늘어남)

  <br/>

- 정수 인코딩 예제

  ```python
  from nltk.tokenize import sent_tokenize, word_tokenize
  from nltk.corpus import stopwords
  ```

  ```python
  text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."
  ```

  - 데이터 전처리

    - 문장 토큰화

    ```python
    data1 = sent_tokenize(text)
    ```

    ![j23](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j23.png?raw=true)

    - 단어 토큰화

    ```python
    word_l = {}
    pr_data = []
    stop_w = set(stopwords.words('english'))
    
    for sentence in data1:
        t_data = word_tokenize(sentence)
        l = []
        for word in t_data:
            word = word.lower()  # 소문자로 변환
            if word not in stop_w:  # 불용어 처리
                if len(word) > 2:  # 짧은 길이의 단어 처리
                    l.append(word)
                    if word not in word_l:
                        word_l[word] = 0
                    word_l[word] += 1
        pr_data.append(l)
    pr_data
    ```

    ![j24](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j24.png?raw=true)

    ```python
    word_l
    ```

    ![j25](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j25.png?raw=true)

    ```python
    sorted_word_l = sorted(word_l.items(), key = lambda x : x[1], reverse = True)  # 빈도 순으로 정렬
    ```

    ![j26](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j26.png?raw=true)

    ```python
    # 2회 이상의 빈도로 사용된 단어들을 높은 빈도 순으로 딕셔너리에 저장
    word_idx = {}
    i = 0
    for (word, fq) in sorted_word_l:
        if fq > 1:
            i += 1
            word_idx[word] = i
    ```

    ![j27](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j27.png?raw=true)

    ```python
    # 단어 4개만 사용
    n = 4
    selected_data = [word for word, i in word_idx.items() if i > n]
    for i in selected_data:
        del word_idx[i]
    
    # {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4}
    ```

    ```python
    word_idx['OOV'] = len(word_idx) + 1  # 단어 추가, OOV는 집합에 없는 단어를 의미함
    # {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'OOV': 5}
    ```

  - 데이터 정형화

    ```python
    ec_data = []
    for i in pr_data:
        ec_d = []
        for word in i:
            try:
                ec_d.append(word_idx[word])
            except KeyError:
                ec_d.append(word_idx['OOV'])
        ec_data.append(ec_d)
    ```

    ![j28](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j28.png?raw=true)

  <br/>

- Counter 기반 정수 인코딩

  - 위에서 전처리된 데이터를 사용

    ```python
    from collections import Counter
    pr_data
    ```

    ![j29](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j29.png?raw=true)

    ```python
    word_li = sum(pr_data, [])
    ```

    ![j30](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j30.png?raw=true)

    - Counter 사용을 위해 차원을 낮춰줌

    ```python
    l_word_li = Counter(word_li)  # Counter 클래스가 알아서 각 단어의 출현횟수를 세줌
    ```

    ![j31](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j31.png?raw=true)

    ```python
    # 가장 빈도 수가 높은 단어 4개 선택
    n = 4
    fq_word = l_word_li.most_common(n)
    # [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4)]
    ```

    ```python
    # 정수 인코딩
    word_idx2 = {}
    i = 0
    for word, fq in fq_word:
        i += 1
        word_idx2[word] = i
    # {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4}
    ```

    ```python
    # OOV 추가
    word_idx2['OOV'] = len(word_idx2) + 1
    # {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'OOV': 5}
    ```

    ```python
    # 정형화
    ec_data2 = []
    for i in pr_data:
        ec_d2 = []
        for word in i:
            try:
                ec_d2.append(word_idx2[word])
            except KeyError:
                ec_d2.append(word_idx2['OOV'])
        ec_data2.append(ec_d2)
    ```

    ![j32](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j32.png?raw=true)

  <br/>

- NLTK 기반 정수 인코딩

  ```python
  from nltk import FreqDist
  import numpy as np
  
  word_li = FreqDist(np.hstack(pr_data))
  ```

  ![j33](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j33.png?raw=true)

  ```python
  word_li = word_li.most_common(4)
  # [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4)]
  ```

  - 이후 정수 인코딩, 정형화 과정은 위와 동일

  <br/>

- keras 기반 토큰화 및 정수 인코딩

  ```python
  from tensorflow.keras.preprocessing.text import Tokenizer
  pr_data
  ```

  ![j34](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j34.png?raw=true)

  ```python
  ck_t = Tokenizer()
  ck_t.fit_on_texts(pr_data)
  ck_t.word_index
  ```

  ![j35](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j35.png?raw=true)

  ```python
  # 각 단어의 출현횟수
  ck_t.word_counts
  ```

  ![j36](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j36.png?raw=true)

  ```python
  # 정형화
  ck_t.texts_to_sequences(pr_data)
  ```

  ![j37](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j37.png?raw=true)

  ```python
  n = 4
  end_ck = Tokenizer(num_words = n + 2, oov_token = 'OOV')
  end_ck.fit_on_texts(pr_data)
  end_ck.word_index
  ```

  ![j38](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j38.png?raw=true)

  - num_words 매개변수로 보존할 단어의 빈도수 기준을 정한다
  - oov_token으로 제외된 빈도수의 단어들을 해당 문자열로 대체한다

  ```python
  end_ck.texts_to_sequences(pr_data)
  ```

  ![j39](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j39.png?raw=true)

  - 해당 방법을 사용하면 OOV가 1번으로 지정되어 문제가 발생하기 때문에 따로 처리를 해줄 필요가 생기게 된다.

  <br/>

- 패딩 예제

  ```python
  from tensorflow.keras.preprocessing.text import Tokenizer
  from nltk.tokenize import sent_tokenize, word_tokenize
  from nltk.corpus import stopwords
  
  text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."
  ```

  - 전처리

  ```python
  data1 = sent_tokenize(text)
  
  word_l = {}
  pr_data = []
  stop_w = set(stopwords.words('english'))
  
  for sentence in data1:
      t_data = word_tokenize(sentence)
      l = []
      for word in t_data:
          word = word.lower()
          if word not in stop_w:
              if len(word) > 2:
                  l.append(word)
                  if word not in word_l:
                      word_l[word] = 0
                  word_l[word] += 1
      pr_data.append(l)
      
  pr_data
  ```

  ![j40](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j40.png?raw=true)

  - 정형화

  ```python
  tk = Tokenizer()
  tk.fit_on_texts(pr_data)
  encoded = tk.texts_to_sequences(pr_data)
  encoded
  ```

  ![j41](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j41.png?raw=true)

  - 문장의 최대 길이 확인

  ```python
  max_l = max(len(x) for x in encoded)
  max_l
  # 7
  ```

  - 패딩

    ```python
    import numpy as np
    
    for i in encoded:
        while len(i) < max_l:
            i.append(0)
    data = np.array(encoded)
    data
    ```

    ![j42](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j42.png?raw=true)

    - 문장의 길이가 최대 길이보다 작은 경우에 뒤에 0을 추가해주는 방법으로 패딩
    - numpy 배열로 패딩한 방식

  - pad_sequences 사용

    ```python
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    
    tk = Tokenizer()
    tk.fit_on_texts(pr_data)
    encoded = tk.texts_to_sequences(pr_data)
    encoded
    ```
    
    ![j43](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j43.png?raw=true)
    
    - 전처리된 데이터 사용
    
    ```python
    end_data = pad_sequences(encoded)
    end_data
    ```
    
    ![j44](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j44.png?raw=true)
    
    - 아무 설정이 없으면 최대 길이만큼 앞 단에 0을 채움.
    
    ```python
    end_data = pad_sequences(encoded, padding = 'post', truncating = 'post', maxlen = 5)
    end_data
    ```
    
    ![j45](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j45.png?raw=true)
    
    - padding 옵션에 post를, truncating 옵션에 post를 입력하면 뒤에서부터 패딩하고, 삭제한다. maxlen으로 문장의 길이를 제한할 수 있다.
    
    ```python
    v = len(tk.word_index) + 1
    end_data = pad_sequences(encoded, padding = 'post', truncating = 'post', value = v)
    end_data
    ```
    
    ![j46](https://github.com/Cheolyong-Kim/TIL/blob/master/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/%ED%86%B5%EA%B3%84%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%20image%201/j46.png?raw=true)
    
    - value 옵션에 값을 넣어 원하는 값으로 패딩할 수 있다.
  
  <br/>
  
  ---
  
  ### 종합
  
  <br/>
  
  1. 정형화의 순서는 다음으로 진행된다.
     - 문장 토큰화 -> 단어 토큰화 -> 정제, 정규화 -> 정형화
  2. 텍스트 분석에는 nltk, kss, konlpy 라이브러리를 사용한다. API 참고해서 적재적소에 맞게 모듈을 활용하자.
  3. 정형화 즉, 정수 인코딩을 할 때는 다양한 모듈들을 사용할 수 있으니 자신의 상황에 맞게 알맞은 것을 사용한다. 하지만 정수 인코딩을 하는 방법을 꼭 알고 있자.
  
  
  
  


​    
