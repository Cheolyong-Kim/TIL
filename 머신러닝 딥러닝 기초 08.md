# 머신러닝 딥러닝 기초 08

<br/>

### 딥러닝

<br/>

- 신경망

  - 인공 신경망은 생물학적인 신경망에서 영감을 받아 만들어진 컴퓨팅 구조이다.

  <br/>

- 뉴런

  - 신경계를 구성하는 세포

  - 수상돌기에 들어온 전기 신호가 하나로 통합되어 어떤 임계 값을 초과하면 축삭돌기를 통해 이동되고 다음 신경 세포에 전달된다.

    ![i1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i1.png?raw=true)

  <br/>

- 인공 신경망의 장점

  - 데이터만 주어지면 신견망은 예제로부터 배울 수 있다
  - 몇 개의 소자가 오동작하더라도 전체적으로는 큰 문제가 발생하지 않는다.

  <br/>

- 인공 신경망 그림

  ![i2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i2.png?raw=true)

  <br/>

- 퍼셉트론 

  - 가장 간단한 인공 신경망 구조 중 하나로 1957년에 프랑크 로젠블라트가 제안했다

  - 다수의 신호를 입력받아 하나의 신호로 출력한다

    ![i3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i3.png?raw=true)![i4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/m1.png?raw=true) 

    <br/>

    ![i5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i4.png?raw=true)

  - 가중치(weights) : 입력의 강도를 표현

  - 스스로 학습하는 능력

    - 초기 가중치는 임의의 값 지정
    - 뉴런의 결과를 목표값과 비교
    - 오차를 사용하여 다음 단계의 가중치에 반영

  - 임계값 b

    - 가중치와 입력 곱의 합
    - 값이 임계값 b 보다 크면 1 신호 출력
    - 그렇지 않으면 0 신호 출력

  - AND 게이트 구현

    ![i6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i5.png?raw=true)![i7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i6.png?raw=true)

    <br/>

    ![i7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i7.png?raw=true)

    <br/>

    ```python
    from sklearn.linear_model import Perceptron
    
    X = [[0, 0], [0, 1], [1, 0], [1, 1]]
    y = [0, 0, 0, 1]
    
    p = Perceptron(tol = 1e-3, random_state = 10)
    p.fit(X, y)
    p.predict(X)  # array([0, 0, 0, 1])
    ```

    - Perceptron 모듈로 간단하게 구현이 가능하다.

    ```python
    # y=wx+b
    # 뉴런의 계단 함수
    def n_f(in_data):
        global w
        global b
        at_f = b
        
        for i in range(2):
            at_f += w[i] * in_data[i]
            
        if at_f >= 0.0:
            return 1
        else:
            return 0
    ```

    - sklearn에서 제공하는 모듈을 사용하지 않고 퍼셉트론을 구현해본다.
    - 우선 뉴런 내의 활성화 함수를 구현한다.

    ```python
    def t_f(X, y, l_r, epch):
        global w
        global b
        
        for en in range(epch):
            sum_e = 0.0
            for r, t in zip(X, y):
                at = n_f(r)  # 예측값
                err = t - at  # 오차
                b = b + l_r * err
                sum_e += err ** 2
                for i in range(2):
                    w[i] = w[i] + l_r * err * r[i]
                print(w, b)
            print(f'에포크={en} 학습률={l_r} 에러={sum_e}')
        return w
    ```

    - 설정한 에포크만큼 반복하면서 오차를 사용해 가중치를 갱신해주는 함수이다.

    ```python
    X = [[0, 0], [0, 1], [1, 0], [1, 1]]
    y = [0, 0, 0, 1]
    w = [0.0, 0.0]  # 가중치
    b = 0.0  # 임계값(w0)
    l_r = 0.1  # 학습률
    e = 5  # 에포크
    
    w = t_f(X, y, l_r, e)
    ```

    ![j1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/j1.png?raw=true)

    - AND게이트의 가중치와 임계값 b를 계산해낸 결과이다.

    ```python
    X = [[0, 0], [0, 1], [1, 0], [1, 1]]
    y = [0, 1, 1, 1]
    w = [0.0, 0.0]  # 가중치
    b = 0.0  # 임계값(w0)
    l_r = 0.1  # 학습률
    e = 5  # 에포크
    
    w = t_f(X, y, l_r, e)
    ```

    ![j2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/j2.png?raw=true)

    - OR게이트의 가중치와 임계값 b를 계산해낸 결과이다.

    ![i8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i8.png?raw=true)

  - 퍼셉트론은 직선 하나로 나눈 영역만 표현할 수 있다는 한계가 있다.

  - 그렇기 때문에 XOR 연산에 대한 결과는 나타낼 수가 없다.

    ![i9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i9.png?raw=true)

  - AND게이트와 OR게이트 두 개를 사용하여 XOR게이트를 만들 수는 있다.

    ![i10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i10.png?raw=true)

    <br/>

    ![i11](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i11.png?raw=true)

    <br/>

    ```python
    def AND(X):
        and_w = [0.2, 0.1]
        and_b = -0.20000000000000004
        at_f = and_b
        
        for i in range(2):
            at_f += and_w[i] * X[i]
            
        if at_f >= 0.0:
            return 1
        else:
            return 0
    ```

    - 퍼셉트론으로 구한 AND게이트의 가중치와 임계값으로 AND함수를 만든다.

    ```python
    def OR(X):
        or_w = [0.1, 0.1]
        or_b = -0.1
        at_f = or_b
        
        for i in range(2):
            at_f += or_w[i] * X[i]
            
        if at_f >= 0.0:
            return 1
        else:
            return 0
    ```

    - 마찬가지로 OR함수를 만든다

    ```python
    def XOR(X):
        o_1 = not AND(X)
        o_2 = OR(X)
        
        return AND([o_1, o_2])
    
    XOR(X[0]), XOR(X[1]), XOR(X[2]), XOR(X[3])
    # (0, 1, 1, 0)
    ```

    - 두 게이트를 사용해 XOR함수를 만들 수 있다.

  - 퍼셉트론은 하나의 뉴런만을 사용하기 때문에 XOR 연산은 학습할 수가 없다.

<br/>

- 다층 퍼셉트론

  ![i12](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i12.png?raw=true)

  - 가장 왼쪽 줄을 입력층, 가장 오른쪽 줄을 출력층, 중간 줄을 은닉층이라 부른다.

  - 은닉층의 뉴런은 사람 눈에는 보이지 않는다.

  - h(x) 함수는 입력 신호의 총합을 출력 신호로 변환하는 함수를 말한다. 일반적으로 활성화 함수라 한다.

    ![i13](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i13.png?raw=true)

  - 다층 퍼셉트론은 입력층과 출력층 사이에 은닉층을 가지고 있는 신경망을 말한다.

    ![i14](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i14.png?raw=true)

  - 역전파 알고리즘

    - 입력이 주어지면 순방향으로 계산하여 출력을 계산한 후 실제 출력과 원하는 출력 간의 오차를 계산

    - 이 오차를 역방향으로 전파하면서 오차를 줄이는 방향으로 가중치를 변경

      ![i15](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i15.png?raw=true)

  - 정리 이미지

    ![i16](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i16.png?raw=true)

  <br/>

- 오차 역전파

  - 손실함수

    ![m2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/m2.png?raw=true)

    - 전체 오차는 목표 출력값에서 실제 출력값을 빼서 제곱한 값을 모든 출력 노드에 대하여 합한 값

  - 가중치의 변경

    ![m3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/m3.png?raw=true)

  - 계산해야 할 것

    ![m4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/m4.png?raw=true)

  - 경사 하강법으로 보는 가중치의 변경

    ![i16](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i17.png?raw=true)

  - 역전파 알고리즘

    ```python
    신경망의 가중치를 작은 난수로 초기화한다. 
    do 각 훈련 샘플 sample에 대하여 다음을 반복한다.
    	actual = calculate_network(sample) // 순방향 패스
    	target = desired_output(sample)
        각 출력 노드에서 오차(target - actual)을 계산한다.
        은익층에서 출력층으로의 가중치 변경값을 계산한다. // 역방향 패스
    	입력층에서 은닉층으로의 가중치 변경값을 계산한다. // 역방향 패스
    	전체 가중치를 업데이트한다.
    until 모든 샘플이 올바르게 분류될 때까지
    ```

    <br/>

    ![i18](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i18.png?raw=true)

    ![i19](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i19.png?raw=true)

    ![i20](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i20.png?raw=true)

    ![i21](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i21.png?raw=true)

    ![i22](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i22.png?raw=true)

  - 계산 그래프

    - 국소적 계산을 전파함으로써 최종 결과를 얻는 그래프이다.

      ![i23](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i23.png?raw=true)

    - 계산 그래프를 이용한 오차 역전파

      ![i24](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i24.png?raw=true)

      ![i25](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i25.png?raw=true)

      <br/>

      ![i26](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i26.png?raw=true)

      ![i27](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i27.png?raw=true)

    - 덧셈 노드의 역전파

      ![i28](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i28.png?raw=true)

      - 덧셈 노드의 역전은 미분한 값 1이 전파된다.

    - 곱셈 노드의 역전파

      ![i29](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i29.png?raw=true)

      - 곱셈 노드의 역전은 입력으로 들어온 순서를 반대로 한 값이 전파된다.

      ![i30](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i30.png?raw=true)

      ![i31](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i31.png?raw=true)

  - wx+b는 기울기가 w이고 y절편이 b인 1차 함수

  - 활성화 함수

    - 퍼셉트론의 최종 값의 세기를 조절하는 데 사용됨.

    - 활성화 함수의 출력값은 다음 퍼셉트론의 입력으로 사용됨.

    - 퍼셉트론에서 최종 값을 결정하는 함수

    - 내부에서 입력받은 데이터를 근거로 다음 계층으로 출력할 값을 결정

    - 신경망을 구성할 때 설정하며 각각의 레이어를 정의할 때 세부적인 함수를 선택

    - 출력 계층의 활성화 함수는 목표에 부합하는 함수를 선택하여 설정

      ![i32](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i32.png?raw=true)

    - 단위 계단 함수

      ![i33](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i33.png?raw=true)

      - 잘 사용되지는 않음

    - ReLU 함수

      - 양수부분은 입력 그대로 출력, 음수는 0으로 출력

    - 시그모이드 함수와 하이퍼볼릭탄젠트 함수도 주로 사용됨

    - 그래프 이미지

      ![i34](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i34.png?raw=true)

  <br/>

- 다층 신경망

  - 인공신경망을 확장해 가로로 여러 개 연결한 층을 쌓음

  - 은닉층이 생김으로서 계산량이 기하급수적으로 늘어나 그만큼 정확도가 높아진 결과를 얻을 수 있음

  - 은닉층의 단계가 많아질수록 깊은 딥러닝이라 함

  - 손실 함수

    - 비용 함수

    - 학습의 목표가 되는 기준 지표의 계산식, 목적 함수

      ![i35](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i35.png?raw=true)

  - 최적화 함수

    - 손실함수의 결과값을 최소화하는 함수
    - 경사하강법, 확률적 경사하강법(SGD), RMSProp 등이 있음

  - 가중치 계산

    - 초기화된 가중치는 입력이 받복됨에 따라 더욱 적합한 값을 가질 수 있도록 첫 예측값 Y와 진짜 타깃 Y의 오차를 최소화

    - 손실함수로 오차를 구함

    - 최적화함수를 사용하여 조절

    - 활성화함수로 계산하여 다음 입력으로 사용

    - 반복

      ![i36](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/i36.png?raw=true)

  <br/>

- 파이썬 코드(오차 역전파으로 구현한 XOR 게이트)

  ```python
  import numpy as np
  
  # 시그모이드 함수
  def actf(x):
      return 1 / (1 + np.exp(-x))
  
  # 시그모이드 미분 함수
  def d_actf(x):
      return x * (1 - x)
  ```

  - 활성화 함수로 시그모이드 함수를 사용
  - 역전파를 위해 미분 함수도 미리 정의

  ```python
  X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
  Y = np.array([[0], [1], [1], [0]])
  in_n = 3  # 입력으로 들어갈 피쳐의 수
  h_n = 6  # 은닉층의 수
  out_n = 1  # 출력 값의 수
  
  np.random.seed(5)  # 테스트를 위해 랜덤 시드값 고정
  # 가중치를 -1~1까지의 값으로 나오게 함
  w0 = 2 * np.random.random((in_n, h_n)) - 1
  w1 = 2 * np.random.random((h_n, out_n)) - 1
  ```

  - 입력값 X와 타겟 Y를 설정
  - 가중치를 -1~1 사이의 난수로 설정

  ```python
  for i in range(10000):
      l0 = X  # 입력값(레이어 0)
      # 순전파
      net1 = np.dot(l0, w0)  # 입력 -> 은닉층 
      l1 = actf(net1)  # 레이어 1
      l1[:, -1] = 1  # 바이어스 값 1로 수정
      net2 = np.dot(l1, w1)  # 은닉층 -> 은닉층
      l2 = actf(net2)  # 결과값 (레이어 2)
      
      # 역전파
      l2_e = l2 - Y  # 오차값
      l2_d = l2_e * d_actf(l2)  # 출력층의 델타값
      l1_e = np.dot(l2_d, w1.T)  # 은닉오차
      l1_d = l1_e * d_actf(l1)  # 은닉층의 델타값
      
      # 가중치 갱신(학습률 0.2)
      w1 += -0.2 * np.dot(l1.T, l2_d)
      w0 += -0.2 * np.dot(l0.T, l1_d)
  
  print(l2)
  
  # [[0.01144685]
  #  [0.98853856]
  #  [0.98734948]
  #  [0.01447403]]
  ```

  - 학습을 끝난 결과값이 XOR의 결과값과 아주 가까워진 것을 볼 수 있음

  <br/>

- 파이썬 코드(tensorflow를 활용한 딥러닝)

  ```python
  from tensorflow import keras
  
  (t_x, t_y), (tt_x, tt_y) = keras.datasets.fashion_mnist.load_data()
  
  t_x.shape  # (60000, 28, 28)
  ```

  - tensorflow 라이브러에 존재하는 keras 모듈의 데이터셋을 불러와서 사용한다.
  - 28x28 픽셀을 가진 이미지이다.

  ```python
  s_t_x = t_x / 255.0
  s_t_x = s_t_x.reshape(-1, 28 * 28)  # 전처리
  ```

  - 학습을 위해 데이터를 전처리해준다.

  ```python
  from sklearn.linear_model import SGDClassifier
  from sklearn.model_selection import cross_validate
  import numpy as np
  
  sc = SGDClassifier(loss = 'log', max_iter = 5, random_state = 42)
  scr = cross_validate(sc, s_t_x, t_y)
  np.mean(scr['test_score'])  # 0.8192833333333333
  ```

  - 비교를 위한 SGD분류기의 스코어이다.

  ```python
  import tensorflow as tf
  from sklearn.model_selection import train_test_split
  
  t_x, v_x, t_y, v_y = train_test_split(s_t_x, t_y, test_size = 0.2, random_state = 42)
  ```

  ```python
  dense = keras.layers.Dense(10, activation = 'softmax', input_shape = (784,))  # 레이어 층 쌓기
  ```

  - 레이어가 1층이기 때문에 이 층에서 나오는 값이 곧 출력값이다.
  - 그렇기 때문에 활성화 함수는 softmax 함수로 지정.
  - input_shape로 입력으로 들어오는 피쳐의 수를 알려준다.

  ```python
  model = keras.Sequential(dense)
  model.compile(loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')
  ```

  - 쌓은 레이어로 모델을 만들어 준다.

  ```python
  model.fit(t_x, t_y, epochs = 10)
  ```

  ![j3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/j3.png?raw=true)

  - 손실값은 0.41, 정확도는 0.86 정도로 1층 레이어로도 꽤 높은 점수가 나온 것을 볼 수 있다.

  <br/>

- 파이썬 코드(tensorflow를 활용한 다층 딥러닝)

  ```python
  from tensorflow import keras
  import numpy as np
  import tensorflow as tf
  
  (t_x, t_y), (tt_x, tt_y) = keras.datasets.fashion_mnist.load_data()
  
  s_t_x = t_x / 255.0
  s_t_x = s_t_x.reshape(-1, 28 * 28)
  
  from sklearn.model_selection import train_test_split
  
  t_x, v_x, t_y, v_y = train_test_split(s_t_x, t_y, test_size = 0.2, random_state = 42)
  ```

  - 데이터는 위 예제와 같은 데이터를 사용한다

  ```python
  dense1 = keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,))  # 입력노드 784개, 출력노드 100개
  dense2 = keras.layers.Dense(10, activation = 'softmax')
  
  model = keras.Sequential([dense1, dense2])
  ```

  - 레이어를 2층 사용한다.
  - 출력층이 아닌 레이어는 시그모이드 함수를 활성함수로 지정해준다.

  ```python
  model.summary()
  ```

  ![j4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/j4.png?raw=true)

  - summary 메소드로 현재 모델의 정보를 확인할 수 있다.

  ```python
  model = keras.Sequential([keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,), name = 'hidden'), 
                            keras.layers.Dense(10, activation = 'softmax', name = 'output')], name = 'model1')
  ```

  - 직접 레이어를 입력해서 한 줄로 표현할 수도 있다. 
  - 각 레이어들과 모델은 name태그를 따로 설정해서 이름을 지정할 수 있다.

  ```python
  model.summary()
  ```

  ![j5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/j5.png?raw=true)

  ```python
  model = keras.Sequential(name = 'model2')
  model.add(keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,), name = 'hidden'))
  model.add(keras.layers.Dense(10, activation = 'softmax', name = 'output'))
  model.summary()
  ```

  ![j6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/j6.png?raw=true)

  - 모델을 생성한 뒤 add메소드로 레이어를 추가할 수도 있다.

  ```python
  model.compile(loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')
  model.fit(t_x, t_y, epochs = 10)
  ```

  ![j7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/j7.png?raw=true)

  - 2층 레이어를 사용해서 더 높은 정확도를 얻을 수 있다.

  ```python
  model = keras.Sequential(name = 'model3')
  model.add(keras.layers.Flatten(input_shape = (28, 28)))  # 입력 노드를 직접 설정가능.
  model.add(keras.layers.Dense(100, activation = 'relu', name = 'hidden'))
  model.add(keras.layers.Dense(10, activation = 'softmax', name = 'output'))
  model.summary()
  ```

  ![j8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/j8.png?raw=true)

  - 위 예제들에서는 데이터를 전처리하여 입력해줬지만 입력 노드를 직접 설정하여 데이터 전처리 없이 피쳐의 모양만 입력해주면 모델을 생성할 수 있다.

  ```python
  (t_x, t_y), (tt_x, tt_y) = keras.datasets.fashion_mnist.load_data()
  
  s_t_x = t_x / 255.0
  t_x, v_x, t_y, v_y = train_test_split(s_t_x, t_y, test_size = 0.2, random_state = 42)
  
  model.compile(loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')
  model.fit(t_x, t_y, epochs = 5)
  ```

  ![j9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2008/j9.png?raw=true)