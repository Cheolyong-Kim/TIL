# 딥러닝 간단한 복습

<br>

### 선형회귀와 로지스틱회귀

<br>

선형회귀와 로지스틱회귀는 딥러닝의 가장 말단에서 이루어지는 기본적인 계산 원리이다.

<br>

선형회귀는 주어진 데이터를 가지고 ``y = ax + b``의 식에서 a와 b를 찾아내어 이후 값을 예측할 수 있게 하는 기법이다.

로지스틱회귀는 시그모이드 함수를 가지고 주어진 데이터를 직선이 아닌 S자 형태의 선으로 분류하는 기법이다.
<br>

---

### 평균제곱 오차(Mean Square Error, MSE)

<br>

딥러닝 오차 평가 방법 중 가장 많이 사용되는 방법

오차를 통해서 가설을 수정하여 오차가 최소가 될 때 가지 반복하는 방법.

<br>

- Loss function(오차 함수) 종류

  ![오차함수 종류 표](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i1.png?raw=true)

<br>

오차는 ``예측값-실제값``이며 오차들의 합을 구할 때는 각 오차의 값을 제곱하여 합한다.

이 오차의 합을 오차의 수 만큼으로 나누어 주면 평균제곱 오차가 만들어진다.

<br>

이 평균제곱 오차를 최소화하는 a와 b 값(y = ax + b)을 구하는 방법 중에 가장 많이 사용되는 것이 경사 하강법이다.

<br>

---

### 경사 하강법

<br>

평균 제곱 오차를 그래프로 나타내면 2차원 그래프로 나타난다.

이 2차원 그래프에서 y가 가장 낮은 지점을 찾는 방법이 경사 하강법이다.

그래프 상에 어느 한 점에서 시작해서 가장 낮은 점 m에 가까운 쪽으로 점점 이동시키는 과정이 필요하다.

![경사 하강법 그래프](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i2.png?raw=true)

<br>

그렇다면 우선 그래프에서 가장 낮은 점을 찾아야하는데 그 점은 미분을 통해 알아낸다.

미분은 그 점의 기울기를 나타내는 것이고 2차원 그래프에서 골짜기의 끝에 해당하는 점에서의 기울기는 0이기 때문에 미분을 통해서 그래프에서 가장 낮은 점이 어디인지를 알 수 있게 된다.

![경사 하강법 미분](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i3.png?raw=true)

<br>

- 경사하강법 과정

  ![경사 하강법 과정](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i4.png?raw=true)

<br>

경사하강법에서 학습률이라는 개념이 등장하는데, 학습률은 보폭이라고 비유할 수 있다.

학습률이 크면 점이 큰 범위로 활동하고 학습률이 작으면 작은 범위로 활동한다.

학습률이 너무 크면 발산하고, 학습률이 너무 작으면 0이 되는 기울기를 찾기 전에 멈춘다.

<br>

---

### 오차 역전파(Back Propagation)

<br>

오차 역전파는 경사 하강법의 확장 개념이다. 임의의 가중치를 선언하고 결과값을 이용해 오차를 구한 뒤 오차가 최소인 지점으로 계속해서 조금씩 이동시킨다.

오차 역전파는 다층 퍼셉트론에서의 최적화 과정이다.

![오차 역전파 이미지](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i5.png?raw=true)

<br>

![오차 역전파가 적용된 다층 퍼셉트론](딥러닝 간단한 복습 02.assets/i6.png)

<br>

---

### 기울기 소실 문제와 활성화 함수

<br>

오차 역전파에서 활성화 함수로 사용된 시그모이드 함수의 특성 때문에 기울기 소실 문제가 발생한다.

시그모이드 함수를 미분하면 최대값이 0.3으로 1보다 작기 때문에 계속 곱하다 보면 0에 가까워진다. 여러 층을 거칠수록 기울기가 사라져 가중치를 수정하기 어려워지는 것.

![시그모이드 미분 이미지](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i7.png?raw=true)

<br>

이 문제를 해결하기 위해 하이퍼볼릭 탄젠트 함수, 렐루 함수, 소프트플러스 함수 등이 등장했고 렐루 함수를 가장 많이 사용하게 된다.

![여러 활성화함수 이미지](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i8.png?raw=true)

<br>

---

### 고급 경사 하강법

<br>

경사 하강법은 정확하게 가중치를 찾아가지만 한 번 업데이트할 때마다 전체 데이터를 미분해야 하므로 계산량이 매우 많다는 단점이 있다.

이러한 단점을 보완한 것이 고급 경사 하강법.

<br>

- 확률적 경사 하강법(Stochastic Gradient Descent, SGD)

  전체 데이터를 사용하는 것이 아니라, 랜덤하게 추출한 일부 데이터를 사용한다. 일부 데이터를 사용하면서 더 빨리, 자주 업데이트를 할 수 있게 됐다.

  ![확률적 경사 하강법 이미지](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i9.png?raw=true)

  <br>

- 모멘텀(momentum)

  경사 하강법에 탄력을 더해 준 것. 기울기를 구한 뒤 이를 통해 오차를 수정하기 전 바로 앞 수정 값과 방향을 참고하여 같은 방향으로 일정한 비율만 수정되게 하는 방법이다.

  ![모멘텀 이미지](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i10.png?raw=true)

- 이 외 

  ![고급 경사 하강법 이미지](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/image%202/i11.png?raw=true)

<br>

현재는 Adam이 보편적으로 가장 많이 사용되고 있다.