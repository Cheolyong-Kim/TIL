# 워드 임베딩

<br>

워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현으로 변환한다.

<br>

---

#### 희소 표현

<br>

벡터 또는 행렬의 값 대부분이 0으로 표현되는 방법을 희소 표현이라고 한다.

원-핫 벡터가 대표적으로 희소 벡터이다.

희소 표현은 단어가 늘어나면 벡터의 차원이 엄청나게 커진다는 문제점이 있다.

10000개의 단어 중 강아지라는 한 단어를 벡터로 표현하면

예시로 [0 0 0 0 1 0 0 0 0 0 ....] 으로 표현되고 4인덱스의 강아지를 나타내는 벡터 외에 9999개의 공간이 0으로 채워지게 된다.

10000개의 차원에 필요한 정보는 하나로 공간의 낭비가 심하게 된다.

<br>

---

#### 밀집 표현

<br>

희소 표현과 다르게 사용자가 밀집 표현의 차원을 직접 설정한다.

사용자가 밀집 표현의 차원을 128로 설정한다면 모든 단어의 벡터 표현은 128로 바뀌면서 모든 값이 실수가 된다.

예시로 강아지를 128차원의 밀집 표현으로 표현한다면

[0.2 1.8 1.1 -2.1 1.1 2.8 ....] 으로 표현되고 이 벡터의 차원은 128차원을 가진다.

<br>

---

### 워드투벡터(Word2Vec)

<br>

원-핫 벡터는 단어 벡터 간 유사도를 계산할 수 없다.

워드투벡터는 벡터 공간에 단어를 뿌려서 유사한 단어를 위치시키도록 하는 방법이다. 이를 통해 단어 벡터 간 유사도를 계산할 수 있게 된다.

워드투벡터의 학습 방식에는 CBOW, Skip-Gram 두 가지 방식이 있다.

딥러닝 모델이 아니라 은닉층이 1개인 얕은 신경망이고 NNLM과 아주 유사하다.

워드 임베딩 자체에 집중하여 NNLM의 느린 학습 속도와 정확도를 개선했다(은닉층을 없애고 투사층만 존재하게 함).



#### CBOW

<br>

CBOW는 중심 단어를 예측하기 위해 주변 단어를 참고하는 모델이다.

예문 The fat cat sat on the mat이 존재할 때

['The', 'fat', 'cat', 'on', 'the', 'mat']으로부터 'sat'을 예측하는 것이 CBOW의 동작이다.

중심 단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼지 결정하는데 이 범위를 윈도우라고 한다.

예를 들어 윈도우 크기가 2이고 예측하고자 하는 중심 단어가 sat이라고 한다면 앞의 두 단어인 fat, cat 그리고 뒤의 두 단어인 on, the를 입력으로 사용한다.

<br>

![슬라이딩 윈도우](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i1.png?raw=true)

윈도우 크기가 정해지면 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋을 만드는데 이 방법을 슬라이딩 윈도우라고 한다.

<br>

![CBOW 인공 신경망](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i2.png?raw=true)

CBOW의 인공 신경망을 간단히 도식화한 이미지이다.

<br>

<br>

#### Skip-gram

<br>

Skip-gram은 주변 단어를 예측하기 위해 중심 단어를 참고하는 모델이다.

예문 The fat cat sat on the mat이 존재할 때

'cat'으로부터 ['The', 'fat', 'sat', 'on', 'the', 'mat']을 예측하는 것이 Skip-gram의 동작이다.

윈도우 크기가 2일 때, cat과 sat에 대한 데이터셋 구성은 다음 이미지와 같다

![Skip-gram 데이터셋](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i3.png?raw=true)

<br>

인공 신경망을 도식화한 이미지는 다음과 같다.

![Skip-gram 인공신경망](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i4.png?raw=true)

<br>

CBOW보다 성능이 좋다고 알려져있다.

<br>

---

### Word2Vec을 이용한 영화 리뷰 속 유사 단어 판별

<br>

![패키지, 데이터 불러오기](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i5.png?raw=true)

사용할 패키지와 데이터를 불러온다.

<br>

![결측치](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i6.png?raw=true)

데이터의 결측치를 확인하고 결측치를 제거해준다.

그 후에 리뷰에서 한글이 아닌 것들을 공백으로 바꿔준다.

<br>

![불용어 설정, 형태소 분석](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i7.png?raw=true)

불용어를 설정하고 리뷰들을 하나씩 꺼내 형태소 분석한다.

Okt클래스의 morphs 메소드 매개변수에 stem을 True로 설정하면 각 단어에서 어간을 추출한다. (ex, '찍어야지' -> '찍다', '먹을거야' - > '먹다')

<br>

![리뷰 최대 길이, 평균 길이 확인](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i8.png?raw=true)

리뷰의 최대 길이와 평균 길이를 확인한다.

<br>

![Word2Vec 모델 생성, 학습](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i9.png?raw=true)

Word2Vec 패키지를 불러와서 사용

형태소 분석된 리뷰들을 학습시킨다.

학습된 모델의 벡터의 shape을 살펴보면 16475개의 단어가 100개의 벡터 차원으로 표현되는 것을 볼 수 있다.

Word2Vec 클래스의 매개 변수들은 [링크](https://radimrehurek.com/gensim/models/word2vec.html)에서 확인

<br>

![유사도 확인](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EB%B0%98%20%EB%B9%84%EC%A0%95%ED%98%95%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D/Word%20Embedding%20image/i10.png?raw=true)

모델에 단어를 입력해서 그 단어와 유사한 단어를 출력한다.