# 머신러닝 딥러닝 기초 07

<br/>

### 앙상블

<br/>

- 앙상블

  - 하나의 데이터를 넣어서 여러 모델에 학습시키고 여러 가중치 기법을 적용하여 결과를 선택하는 기법

  - 메타 분류기라고도 부름

  - 시간이 굉장히 오래 걸리지만 비교적 좋은 성능을 냄

    ![i1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/i1.png?raw=true)

  <br/>

- 앙상블 기법

  - 바닐라 앙상블
    - 가장 기본적인 앙상블 기법
    - 일반적으로 가중치 평균이나 투표 방식으로 만들어지는 앙상블 모델
  - 부스팅
    - 하나의 모델에서 여러 데이터를 샘플링한 후 그 데이터로 각각의 모델을 만드는 기법
  - 배깅
    - boosting aggregation(부스팅 집합)의 줄임말로 부스팅을 좀 더 발전시킨 기법

  <br/>

- 투표 분류기

  - 여러 개의 모델을 만들어 모두 같은 데이터를 넣고 결과를 취합하여 가장 많이 선택된 결과를 취함

  - 앙상블 모델의 가장 기본적인 형태

  - 다수결 분류기라고도 부름

  - 각 분류기마다 가중치를 주고 해당 가중치를 각 모델에 곱하여 가중치의 합을 구하는 방법으로도 가능

  - 장점 : 다양한 모델을 만든 후, 다음 단계로 매우 쉽게 만들 수 있음

    ![i2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/i2.png?raw=true)

    <br/>

    ```python
    import numpy as np
    from sklearn.linear_model import LogisticRegression
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.naive_bayes import GaussianNB
    from sklearn.ensemble import VotingClassifier
    
    X = np.load('day7_data2_X.npy')
    y = np.load('day7_data2_y.npy')
    
    # 각각 모델은 로지스틱 회귀, 결정트리, 가우시안 나이브 베이즈
    c1 = LogisticRegression(random_state= 1)
    c2 = DecisionTreeClassifier(random_state= 1)
    c3 = GaussianNB()
    
    # 투표 분류기
    ec = VotingClassifier(estimators= [('lr', c1), ('rf', c2), ('gnb', c3)], voting= 'hard')
    ```

    ```python
    from sklearn.model_selection import cross_val_score
    
    cross_val_score(ec, X, y, cv= 5).mean()  # 0.8009268075922046
    
    cross_val_score(c1, X, y, cv= 5).mean()  # 0.8290420872214816
    cross_val_score(c2, X, y, cv= 5).mean()  # 0.7829175395162826
    cross_val_score(c3, X, y, cv= 5).mean()  # 0.4600139655938551
    ```

    - 앙상블 모델이 항상 가장 좋은 성능을 내는 것은 아니다.
    - 위 코드에서는 가우시안 나이브 베이즈 모델의 성능이 아주 낮아서 앙상블의 스코어 값이 상대적으로 낮게 나오는 것을 볼 수 있다.

    ```python
    ec1 = VotingClassifier(estimators= [('lr', c1), ('dt', c2)], voting= 'hard')
    cross_val_score(ec1, X, y, cv= 5).mean()  # 0.8222687742017394
    ```

    - 나이브 베이즈 모델을 투표 분류기에서 제외하여 분류기의 성능을 향상시켰다.

    ```python
    c_params = [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]
    params = {
        'lr__solver': ['liblinear'],
        'lr__penalty': ['l2'],
        'lr__C': c_params,
        'dt__criterion': ['gini', 'entropy'],
        'dt__max_depth': [10, 8, 7, 6, 5, 4, 3, 2],
        'dt__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9]
    }
    
    from sklearn.model_selection import GridSearchCV
    
    g = GridSearchCV(estimator= ec1, param_grid= params, cv= 5)
    g.fit(X, y)
    g.best_score_  # 0.8425569732749316
    ```

    - 투표 분류기에 들어갈 모델의 하이퍼 매개변수를 조정하면 당연히 투표 분류기의 성능도 좋아진다.
    - 교차 검증을 통해 가장 좋은 스코어를 내는 파라미터들을 찾는다.

    ```python
    g.best_params_
    ```

    ![j1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j1.png?raw=true)

    ```python
    c1 = LogisticRegression(solver= 'liblinear', penalty= 'l2', C= 5.0, random_state= 1)
    c2 = DecisionTreeClassifier(criterion= 'gini', max_depth= 10, min_samples_leaf= 5, random_state= 1)
    
    ec3 = VotingClassifier(estimators= [('A', c1), ('B', c2)], voting= 'hard')
    
    cross_val_score(ec3, X, y, cv= 5).mean()  # 0.8425569732749316
    ```

    - 찾은 파라미터들을 적용

    ```python
    import numpy as np
    from sklearn.linear_model import LogisticRegression
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.naive_bayes import GaussianNB
    from sklearn.ensemble import VotingClassifier
    from sklearn.model_selection import cross_val_score, GridSearchCV
    
    X = np.load('day7_data2_X.npy')
    y = np.load('day7_data2_y.npy')
    
    c1 = LogisticRegression(solver= 'liblinear', penalty= 'l2', C= 5.0, random_state= 1)
    c2 = DecisionTreeClassifier(criterion= 'gini', max_depth= 10, min_samples_leaf= 5, random_state= 1)
    
    ec = VotingClassifier(estimators= [('c1', c1), ('c2', c2)], voting= 'hard')
    
    cross_val_score(ec, X, y, cv= 5).mean()
    ```

    - 간단하게 정리

  <br/>

- 배깅

  - 하나의 데이터셋에서 샘플링을 통해서 여러 개의 데이터셋을 만든 다음 각 데이터셋마다 모델을 개발하여 투표 분류기로 만드는 기법
  - 단순하면서 성능이 높아 특히 트리 계열 알고리즘과 함께 많이 사용되며 통계적인 샘플링 기법이나 딥러닝 기법과도 함께 사용
  - 샘플링 : 다루고자 하는 데이터가 전체 모수라면 그 모수에서 일부분을 뽑아서 데이터를 분석
  - 장점 : 다양한 데이터셋에서 강건한 모델을 개발할 수 있다.
  - 배깅 기법은 여러 개의 약분류기로 강분류기를 만드는 것이다
  - 약분류기는 적은 수의 데이터로 학습하기 때문에 과소적합이 발생하지만 이렇게 만들어진 약분류기들을 앙상블하면 좀 더 정확한 경계를 가지는 강분류기가 만들어진다.

  <br/>

- 부트스트래핑

  - 모수 데이터로부터 학습 데이터를 추출할 때 임의의 데이터를 추출한 후 복원추출

  - 복원추출 : 전체 데이터에서 먼저 일부를 추출하여 데이터셋을 만든 후 다시 그 데이터를 모수에 집어넣고 다른 데이터셋을 만듦

    ![i3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/i3.png?raw=true)

  - 전체 데이터 S에서 n번의 데이터를 추출할 때, 이를 많이 추출할수록 각 데이터가 나타날 확률이 63.2%에 가까워짐

  <br/>

- 이미지로 보는 배깅

  ![i4](https://raw.githubusercontent.com/Cheolyong-Kim/TIL/7b909f3c16c35a2f242a5aa606c4ed6fabaeb428/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/i4.png)

  <br/>

- Out-of-bag Error (OOB)

  - 배깅 모델의 성능을 측정하기 위해서 사용하는 지표
  - 학습에 사용되지 않은 데이터셋에 대해서만 성능을 측정하여 배깅 모델의 효과를 측정

  <br/>

- 랜덤 포레스트

  - 배깅알고리즘을 의사결정트리에 적용한 모델

  <br/>

- 부스팅

  - 학습 라운드를 차례로 진행하면서 각 예측이 틀린 데이터에 점점 가중치를 주는 방식

  - 라운드별로 잘못 분류된 데이터를 좀 더 잘 분류하는 모델로 만들어 최종적으로 모델들의 앙상블을 만드는 방식

    ![i5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/i5.png?raw=true)

  - 부스팅 알고리즘은 처음 성능을 측정하기보다 그 후에 높은 성능을 내야하는 상황에서 좋은 선택지가 됨 (초기 성능은 배깅 알고리즘)

  <br/>

- 배깅과 부스팅의 차이점

  - 병렬화 가능 여부

    - 배깅은 이전 모델이 필요없기 때문에 병렬화 가능. 부스팅은 이전 모델이 필요하기 때문에 병렬화 불가능

    - 배깅은 데이터가 n개라면 n개의 CPU로 한번에 처리하도록 구조를 설계할 수 있음
    - 부스팅은 배깅에 비해 속도가 매우 떨어짐

  - 기준 추정치

    - 배깅의 개별 모델들은 높은 과대적합으로 모델의 분산이 높음
    - 부스팅은 각각의 모델에 편향이 높은 기준 추정치를 사용하여 개별 모델들은 과소적합이 발생하지만 전체적으로 높은 성능을 낼 수 있는 방향으로 학습
    - 부스팅 모델의 이러한 특징을 약한 학습자라고 부름

  - 성능 차이

    - 부스팅은 기본적으로 비용이 높은 알고리즘 (비용 = 속도, 시간)
    - 배깅은 데이터의 부분집합에 대해 학습을 수행하기 때문에 부스팅보다 좋은 성능을 내기는 어려움

  <br/>

- 파이썬 코드

  ```python
  import numpy as np
  from sklearn.linear_model import LogisticRegression
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import VotingClassifier, BaggingClassifier
  from sklearn.model_selection import cross_val_score, GridSearchCV
  
  X = np.load('day7_data2_X.npy')
  y = np.load('day7_data2_y.npy')
  
  # 로지스틱 회귀 모델로 배깅
  ec1 = BaggingClassifier(LogisticRegression(random_state= 1), oob_score= True)
  
  cross_val_score(ec1, X, y, cv= 5).mean()  # 0.818917031676506
  ```

  ```python
  from sklearn.model_selection import GridSearchCV
  
  params = {
      'n_estimators': [10, 20, 30, 40, 50, 55],
      'max_samples': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  }
  g= GridSearchCV(ec1, param_grid= params, cv= 5)
  g = g.fit(X, y)
  g.best_score_  # 0.8301720307243065
  ```

  - 배깅에서 사용하는 모델의 하이퍼 매개변수를 조정해서 성능을 향상시킬 수 있다.

  ```python
  g.best_params_  # {'max_samples': 0.9, 'n_estimators': 55}
  ```

  ```python
  # 결정트리를 모델로 배깅 = 랜덤 포레스트
  ec2 = BaggingClassifier(DecisionTreeClassifier(random_state= 1), oob_score= True) 
  cross_val_score(ec2, X, y, cv= 5).mean()  # 0.820040627182124
  ```

  ```python
  from sklearn.ensemble import RandomForestClassifier
  
  ec3 = RandomForestClassifier(oob_score= True)
  cross_val_score(ec3, X, y, cv= 5).mean()  # 0.8054529296007111
  ```

  - BaggingClassifier의 모델로 결정트리를 주어 랜덤 포레스트를 만들어도 되지만
  - RadnomForestClassifier 모듈을 사용하면 편하게 사용할 수 있다.

  ```python
  params = {
      'n_estimators': [10, 20, 30, 50, 100],
      'max_features': [1, 2, 3, 4, 5, 6, 7, 10, 15, 20, 25, len(X[0])]
  }
  g = GridSearchCV(ec3, params, cv= 5)
  g.fit(X, y)
  g.best_params_  # {'max_features': 27, 'n_estimators': 10}
  ```

  - 랜덤 포레스트 모듈에서도 하이퍼 매개변수를 사용해서 성능을 향상시킬 수 있다.

  ```python
  ec4 = RandomForestClassifier(max_features= 27, n_estimators= 10, oob_score= True)
  ec4.fit(X, y)
  ec4.oob_score_  # 0.7919010123734533
  ```

  <br/>

- 에이다부스트(AdaBoost)

  - 매 라운드마다 개별 데이터의 가중치를 계산하는 방식

  - 매 라운드마다 틀린 값이 존재하고 해당 인스턴스에 가중치를 추가로 주어 가중치를 기준으로 재샘플링

  - 스텀프

    - 에이다부스트에서 스텀프는 학습할 때 큰 나무를 사용하여 학습하는 것이 아니라 나무의 그루터기만을 사용하여 학습한다는 개념
    - 1depth 또는 2depth 정도의 매운 간단한 모델을 여러 개 만들어 학습한 후, 해당 모델들의 성능을 에이다부스트 알고리즘을 적용하여 학습하는 형태

    ```python
    import numpy as np
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.model_selection import cross_val_score
    
    X = np.load('day7_data2_X.npy')
    y = np.load('day7_data2_y.npy')
    ```

    ```python
    from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
    
    aec = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth= 2), n_estimators= 500)
    cross_val_score(aec, X, y, cv= 5).mean()  # 0.7874500095219958
    ```

    - 결정트리를 모델로 max_depth를 2로 주어 그루터기를 500번 반복하여 학습

    ```python
    aec1 = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth= 2), n_estimators= 500)
    
    params = {
        'n_estimators': [20, 22, 23, 24, 25],
        'learning_rate': [0.4, 0.45, 0.5, 0.55, 0.6],
        'base_estimator__criterion': ['gini', 'entropy'],
        'base_estimator__max_features': [7, 8],
        'base_estimator__max_depth': [1, 2]
    }
    
    from sklearn.model_selection import GridSearchCV
    g = GridSearchCV(estimator= aec1, param_grid= params, cv= 5)
    g.fit(X, y)
    g.best_score_  # 0.8369516917412557
    ```

    - 하이퍼 매개변수를 찾아서 설정해주면 더 좋은 성능을 낼 수 있다.

    ```python
    g.best_params_
    ```

    ![j2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j2.png?raw=true)

<br/>

---

# K-평균 군집화

<br/>

- k-Nearest Neighbor(kNN) 알고리즘

  - 모든 기계 학습 알고리즘 중에서도 가장 간단하고 이해하기 쉬운 분류 알고리즘

    ![i6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/i6.png?raw=true)

    <br/>

    ![i7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/i7.png?raw=true)

  <br/>

- 군집(Clustering)

  - 비슷한 샘플을 클러스터로 모음
  - 비슷한 샘플을 구별해 하나의 클러스터 또는 비슷한 샘플의 그룹으로 할당하는 작업
  - 정답 데이터가 필요없는 비지도 학습

  <br/>

- k-평균 알고리즘(k-means clustering Algorithm)

  - 비지도 학습 중에서 가장 대표적인 것

  - 주어진 n개의 관측값을 k개의 클러스터로 분할하는 알고리즘

  - 알고리즘 원리 참고

    [k-means 알고리즘](https://hleecaster.com/ml-kmeans-clustering-concept/)

  <br/>

- k-평균 군집화

  - 적은 반복으로 레이블이 없는 데이터셋을 빠르고 효율적으로 클러스터링하는 간단한 알고리즘
  - 프로토타입 기반 군집에 속함
  - 원형 클러스터를 구분하는 데 뛰어나지만, 사전에 클러스터 개수 k를 지정해야 하는 단점이 있음
  - 군집 품질을 평가하는 기법으로 엘보우 방법, 실루엣 그래프가 있음

  <br/>

- 프로토타입 기반 군집

  - 각 클러스터가 하나의 프로토타입으로 표현된다는 뜻
  - 연속적인 특성에서는 비슷한 데이터 포인트의 센트로이드(평균)이거나
  - 범주형 특성에서는 메도이드(최빈값이나 중앙값)가 됨

  <br/>

- 엘보우 방법

  - k를 1부터 증가시키면서 k-means 클러스터링을 수행

  - 각 k의 값에 대해 SSE(sum of squared errors)의 값을 계산

  - 그래프가 갑자기 꺾이는 부분이 최적의 클러스터 개수

    ![i8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/i8.png?raw=true)

- 파이썬 코드(k-means)

  ```python
  import matplotlib.pyplot as plt
  import numpy as np
  from sklearn.cluster import KMeans
  
  X = np.array([[6, 3], [11, 15], [17, 12], [24, 10], [20, 25], [22, 30], [85, 70], [71, 81], [60, 79], [56, 52], [81, 91], [80, 81]])
  
  plt.scatter(X[:,0], X[:,1])
  ```

  ![j3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j3.png?raw=true)

  ```python
  km = KMeans(n_clusters= 2)
  km.fit(X)
  km.cluster_centers_
  
  #array([[72.16666667, 75.66666667],
  #       [16.66666667, 15.83333333]])
  ```

  - 클러스터를 2개로 설정하여 k-means 모델링, 학습
  - 클러스터의 중심 좌표가 2개 나오는 것을 볼 수 있다.

  ```python
  plt.scatter(X[:,0], X[:,1], c= km.labels_)
  ```

  ![j4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j4.png?raw=true)

  ```python
  km.inertia_  # 2348.333333333333
  ```

  - 해당 메소드로 오차값을 찾을 수 있다.

  ```python
  n = range(1, 10)
  km_l = [KMeans(n_clusters= i) for i in n]
  sc = [km_l[i].fit(X).inertia_ for i in range(len(km_l))]
  plt.plot(n, sc)
  plt.show()
  ```

  ![j6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j6.png?raw=true)

  - 엘보우 그래프를 확인해보면 k=2 일 때가 가장 최적의 클러스터 개수인 것을 알 수 있다.

  <br/>

- 파이썬 코드(이미지 데이터의 k-means)

  - 300 개의 사과, 파인애플, 바나나 이미지 데이터를 사용
  - 알고리즘을 사용하지 않고 분류

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  
  data = np.load('data7_data3.npy')
  data.shape  # (300, 100, 100)
  ```

  - 총 300개의 데이터이고 100 * 100의 특징을 가지고 있다. (100x100 픽셀의 이미지 데이터)

  ```python
  plt.imshow(data[0], cmap= 'gray_r')
  ```

  ![j8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j8.png?raw=true)

  ```python
  plt.imshow(data[100], cmap= 'gray_r')
  ```

  ![j9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j9.png?raw=true)

  ```python
  plt.imshow(data[200], cmap= 'gray_r')
  ```

  ![j10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j10.png?raw=true)

  - 0~99번 까지는 사과 이미지, 100~199 까지는 파인애플, 200~299 까지는 바나나인 이미지이다.

  ```python
  a, b, c = data[:100].reshape(-1,100 * 100), data[100:200].reshape(-1,100 * 100), data[200:300].reshape(-1,100 * 100)  # 2차원 이미지 정보를 한 줄로
  a.shape  # (100, 10000)
  ```

  - 2차원으로 구성되어 있는 픽셀을 1차원으로 줄여서 한 줄에 특징을 파악할 수 있게 해준다.

  ```python
  plt.hist(np.mean(a, axis= 1), alpha= 0.8)
  plt.hist(np.mean(b, axis= 1), alpha= 0.8)
  plt.hist(np.mean(c, axis= 1), alpha= 0.8)
  plt.legend(['a', 'b', 'c'])
  plt.show()
  ```

  ![j11](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j11.png?raw=true)

  - 열에 대한 특징을 막대 그래프로 표현

  ```python
  a_m = np.mean(a, axis= 0).reshape(100, 100)
  b_m = np.mean(b, axis= 0).reshape(100, 100)
  c_m = np.mean(c, axis= 0).reshape(100, 100)
  
  plt.imshow(a_m, cmap= 'gray_r')
  ```

  ![j12](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j12.png?raw=true)

  - 실제로 사용할 것은 이미지 한 장 한 장의 평균
  - reshape으로 다시 2차원으로 늘려준 뒤 이미지를 출력해보면
  - 흐린 사과 이미지가 출력되는 것을 볼 수 있음

  ```python
  a_d = np.abs(data - a_m)
  a_m_d = np.mean(a_d, axis= (1, 2))
  a_d.shape, a_m_d.shape  # ((300, 100, 100), (300,))
  ```

  - 데이터에서 각 과일의 평균 이미지를 빼면 해당 과일과 연관성이 높은 이미지일수록 낮은 값이 나올 것임

  ```python
  a_l = np.argsort(a_m_d)[:100]  # a_m_d를 오름차순 정렬 후 100개를 선택하면 가장 낮은 값들 100개가 선택 됨
  fig, axs = plt.subplots(10, 10, figsize= (10, 10))
  
  for i in range(10):
      for j in range(10):
          axs[i, j].imshow(data[a_l[i * 10 + j]], cmap= 'gray_r')
          axs[i, j].axis('off')
  plt.show()
  ```

  ![j13](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j13.png?raw=true)

  - 사과가 분류된 것을 볼 수 있음

  ```python
  b_d = np.abs(data - b_m)
  b_m_d = np.mean(b_d, axis= (1, 2))
  
  b_l = np.argsort(b_m_d)[:100]
  fig, axs = plt.subplots(10, 10, figsize= (10, 10))
  
  for i in range(10):
      for j in range(10):
          axs[i, j].imshow(data[b_l[i * 10 + j]], cmap= 'gray_r')
          axs[i, j].axis('off')
  plt.show()
  ```

  ![j14](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j14.png?raw=true)

  ```python
  c_d = np.abs(data - c_m)
  c_m_d = np.mean(c_d, axis= (1, 2))
  
  c_l = np.argsort(c_m_d)[:100]
  fig, axs = plt.subplots(10, 10, figsize= (10, 10))
  
  for i in range(10):
      for j in range(10):
          axs[i, j].imshow(data[c_l[i * 10 + j]], cmap= 'gray_r')
          axs[i, j].axis('off')
  plt.show()
  ```

  ![i15](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j15.png?raw=true)

  - k-means 사용

    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    
    data = np.load('data7_data3.npy')
    data_2d = data.reshape(-1, 100 * 100)
    ```

    ```python
    from sklearn.cluster import KMeans
    
    X = data_2d
    km = KMeans(n_clusters= 3)
    km.fit(X)
    
    def f(data, r= 1):
        n = len(data)
        rows = int(np.ceil(n / 10))
        cols = n if rows < 2 else 10
        fig, axs = plt.subplots(rows, cols, figsize= (cols * r, rows * r))
        
        for i in range(rows):
            for j in range(cols):
                if i * 10 + j < n:
                    axs[i, j].imshow(data[i * 10 + j], cmap= 'gray_r')
                axs[i, j].axis('off')
                
        plt.show()
    ```

    ```python
    f(data[km.labels_ == 0])
    ```

    ![j16](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j16.png?raw=true)

    ```python
    f(data[km.labels_ == 1])
    ```

    ![j17](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j17.png?raw=true)

    ```python
    f(data[km.labels_ == 2])
    ```

    ![j18](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j18.png?raw=true)

    - 위 두 예제는 모두 100x100픽셀의 이미지를 사용하고 있다.
    - 즉, 이미지에서 10000개의 픽셀을 피쳐로 사용하고 있다는 뜻인데
    - 이는 과대적합을 일으킬 수도 있는 많은 수의 피쳐이다.
    - 과대적합을 방지하기 위해 중요한 피쳐만 사용하도록 해야한다.

  - PCA(주성분 분석)

    - PCA를 통해 차원을 축소하여 과대적합을 방지해준다.

    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    data=np.load('data7_data3.npy')
    re_data=data.reshape(-1,100*100)
    ```

    ```python
    from sklearn.decomposition import PCA
    pca=PCA(n_components=50)
    rr_data=pca.fit_transform(re_data)
    ```

    - PCA의 컴포넌트를 50개로 지정하여 중요한 피쳐를 50개만 사용하도록 지정해준다.

    ```python
    def f(data,r=1):
        n=len(data)
        rows=int(np.ceil(n/10))
        cols=n if rows < 2 else 10
        fig,axs = plt.subplots(rows,cols,figsize=(cols*r,rows*r))
        for i in range(rows):
            for j in range(cols):
                if i*10+j<n:
                    axs[i,j].imshow(data[i*10+j],cmap='gray_r')
                axs[i,j].axis('off')
        plt.show()
    ```

    ```python
    f(pca.components_.reshape(-1,100,100))
    ```

    ![j19](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j19.png?raw=true)

    - PCA를 통해 학습한 중요 피쳐 데이터들을 이미지화한 것이다.

    ```python
    rr_data.shape  # (300, 50)
    ```

    - 학습한 데이터를 확인해보면 50개의 피쳐를 가진 것을 볼 수 있다.

    ```python
    k = pca.inverse_transform(rr_data).reshape(-1,100,100)
    f(k)
    ```

    ![j21](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j21.png?raw=true)

    - 분류 결과

    ```python
    plt.plot(pca.explained_variance_ratio_)
    ```

    ![j20](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2007/j20.png?raw=true)

    - 분산 그래프

    ```python
    y=np.array([0]*100+[1]*100+[2]*100)
    x=rr_data
    
    from sklearn.linear_model import LogisticRegression
    m=LogisticRegression()
    from sklearn.model_selection import cross_validate
    sc=cross_validate(m,x,y,cv=5)
    print(np.mean(sc['test_score']))
    print(np.mean(sc['fit_time']))
    sc=cross_validate(m,re_data,y,cv=5)
    print(np.mean(sc['test_score']))
    print(np.mean(sc['fit_time']))
    
    # 1.0
    # 0.0190122127532959
    # 0.9966666666666667
    # 0.2672078609466553
    ```

    - 차원축소한 데이터와 그렇지 않은 데이터로 로지스틱 회귀 모델에서 학습한 결과이다.
    - 두 데이터 모두 학습 스코어는 비슷하지만 시간 측면에서 큰 차이를 보이는 것을 알 수 있다.

    



