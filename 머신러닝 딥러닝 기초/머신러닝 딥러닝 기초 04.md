# 머신러닝 딥러닝 기초 04

<br/>

### 선형회귀

> 03 문서에서 이어지는 내용

<br/>

- 선형회귀 예제

  - 파일 가져오기

    ```python
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split
    
    # 파일 가져오기
    df = pd.read_csv("perch_full.csv")
    ```

    |      | length | height | width |
    | :--: | :----: | :----: | :---: |
    |  0   |  8.4   |  2.11  | 1.41  |
    |  1   |  13.7  |  3.53  | 2.00  |
    |  2   |  15.0  |  3.82  | 2.43  |
    |  3   |  16.2  |  4.59  | 2.63  |
    |  4   |  17.4  |  4.59  | 2.94  |
    |  5   |  18.0  |  5.22  | 3.32  |
    |  6   |  18.7  |  5.20  | 3.12  |
    |  7   |  19.0  |  5.64  | 3.05  |
    |  8   |  19.6  |  5.14  | 3.04  |
    |  9   |  20.0  |  5.08  | 2.77  |
    |  10  |  21.0  |  5.69  | 3.56  |
    |  11  |  21.0  |  5.92  | 3.31  |
    |  12  |  21.0  |  5.69  | 3.67  |
    |  13  |  21.3  |  6.38  | 3.53  |
    |  14  |  22.0  |  6.11  | 3.41  |
    |  15  |  22.0  |  5.64  | 3.52  |
    |  16  |  22.0  |  6.11  | 3.52  |
    |  17  |  22.0  |  5.88  | 3.52  |
    |  18  |  22.0  |  5.52  | 4.00  |
    |  19  |  22.5  |  5.86  | 3.62  |
    |  20  |  22.5  |  6.79  | 3.62  |
    |  21  |  22.7  |  5.95  | 3.63  |
    |  22  |  23.0  |  5.22  | 3.63  |
    |  23  |  23.5  |  6.28  | 3.72  |
    |  24  |  24.0  |  7.29  | 3.72  |
    |  25  |  24.0  |  6.38  | 3.82  |
    |  26  |  24.6  |  6.73  | 4.17  |
    |  27  |  25.0  |  6.44  | 3.68  |
    |  28  |  25.6  |  6.56  | 4.24  |
    |  29  |  26.5  |  7.17  | 4.14  |
    |  30  |  27.3  |  8.32  | 5.14  |
    |  31  |  27.5  |  7.17  | 4.34  |
    |  32  |  27.5  |  7.05  | 4.34  |
    |  33  |  27.5  |  7.28  | 4.57  |
    |  34  |  28.0  |  7.82  | 4.20  |
    |  35  |  28.7  |  7.59  | 4.64  |
    |  36  |  30.0  |  7.62  | 4.77  |
    |  37  |  32.8  | 10.03  | 6.02  |
    |  38  |  34.5  | 10.26  | 6.39  |
    |  39  |  35.0  | 11.49  | 7.80  |
    |  40  |  36.5  | 10.88  | 6.86  |
    |  41  |  36.0  | 10.61  | 6.74  |
    |  42  |  37.0  | 10.84  | 6.26  |
    |  43  |  37.0  | 10.57  | 6.37  |
    |  44  |  39.0  | 11.14  | 7.49  |
    |  45  |  39.0  | 11.14  | 6.00  |
    |  46  |  39.0  | 12.43  | 7.35  |
    |  47  |  40.0  | 11.93  | 7.11  |
    |  48  |  40.0  | 11.73  | 7.22  |
    |  49  |  40.0  | 12.38  | 7.46  |
    |  50  |  40.0  | 11.14  | 6.63  |
    |  51  |  42.0  | 12.80  | 6.87  |
    |  52  |  43.0  | 11.93  | 7.28  |
    |  53  |  43.0  | 12.51  | 7.42  |
    |  54  |  43.5  | 12.60  | 8.14  |

    - 가져온 파일의 데이터 정보

  - 데이터 설정

    ```python
    X = df.to_numpy()
    Y = np.array(
        [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 
         110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 
         130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 
         197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 
         514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 
         820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 
         1000.0, 1000.0]
         )
    ```

  - 데이터 분류

    ```python
    t_x, tt_x, t_y, tt_y = train_test_split(X, Y, random_state= 42)
    ```

  - 피쳐 확인

    ```python
    t_x.shape  # (42, 3)
    ```

  - 데이터 전처리

    ```python
    from sklearn.preprocessing import PolynomialFeatures
    
    # 피쳐의 개수를 늘려줌
    p_m = PolynomialFeatures(include_bias= False)  # include_bias로 절편은 사용 X
    p_m.fit(t_x)
    p_t_x = p_m.transform(t_x)
    p_tt_x = p_m.transform(tt_x)
    ```

  - 모델 생성 및 학습

    ```python
    from sklearn.linear_model import LinearRegression
    
    m_lr = LinearRegression()
    m_lr.fit(p_t_x, t_y)
    ```

  - 스코어 확인

    ```python
    m_lr.score(p_tt_x, tt_y)  # 0.9714559911594155
    m_lr.score(p_t_x, t_y)  # 0.9903183436982125  # 과적합이 발생했는지 체크
    ```

  - 피쳐 확장을 거치지 않은 데이터를 학습했을 때와 비교

    ```python
    lr = LinearRegression().fit(t_x, t_y)
    lr.score(tt_x, tt_y)  # 0.8796419177546366
    lr.score(t_x, t_y)  # 0.9559326821885706
    ```

    - 스코어가 좀 더 낮은 것을 볼 수 있음

- Ridge, Lasso 예제

  - 데이터는 위 예제와 동일

  - 데이터 전처리

    ```python
    p_m1 = PolynomialFeatures(degree=5, include_bias= False).fit(t_x)  # 제곱 수 조정 degree
    
    d_t_x = p_m1.transform(t_x)
    d_tt_x = p_m1.transform(tt_x)
    ```

  - 피쳐 확인

    ```python
    d_t_x.shape  # (42, 55)
    ```

    - 피쳐가 55개로 굉장히 많아짐

  - 모델 생성 및 학습, 스코어 확인

    ```python
    m_lr2 = LinearRegression().fit(d_t_x, t_y)
    m_lr2.score(d_tt_x, tt_y)  # -144.40744532797535
    m_lr2.score(d_t_x, t_y)  # 0.9999999999938143 
    # 과도한 피쳐의 수에 의한 과대적합 발생
    ```

  - 해결

    ```python
    from sklearn.preprocessing import StandardScaler
    
    ss = StandardScaler().fit(d_t_x)  # 중복되는 피쳐를 정리
    sc_t_x = ss.transform(d_t_x)
    sc_tt_x = ss.transform(d_tt_x)
    sc_t_x.shape  # (42, 55)
    ```

    - 피쳐 수는 동일하지만 중복되는 피쳐의 처리를 정리해서 과대적합이 일어나지 않게 함

  - Ridge 모델 생성 및 학습

    ```python
    from sklearn.linear_model import Ridge
    
    rg = Ridge().fit(sc_t_x, t_y)
    rg.score(sc_tt_x, tt_y)  # 0.9790693977615386
    rg.score(sc_t_x, t_y)  # 0.9896101671037343
    ```

  - Ridge 모델의 학습률에 따른 스코어 비교

    ```python
    import matplotlib.pyplot as plt
    
    t_l = []
    tt_l = []
    ap_l = [0.001, 0.01, 0.1, 1, 10, 100]
    
    for ap in ap_l:
        f_rg = Ridge(alpha= ap).fit(sc_t_x, t_y)
        t_l.append(f_rg.score(sc_t_x, t_y))
        tt_l.append(f_rg.score(sc_tt_x, tt_y))
        
    plt.plot(np.log10(ap_l), t_l)
    plt.plot(np.log10(ap_l), tt_l)
    plt.show()
    ```

    ![i1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/i1.png?raw=true)

    - 학습률이 0.1일 때 가장 스코어가 좋게 나온다는 것을 확인할 수 있음

  - Lasso 모델의 학습률에 따른 스코어 비교

    ```python
    t_l = []
    tt_l = []
    ap_l = [0.001, 0.01, 0.1, 1, 10, 100]
    
    for ap in ap_l:
        f_llr = Lasso(alpha= ap).fit(sc_t_x, t_y)
        t_l.append(f_llr.score(sc_t_x, t_y))
        tt_l.append(f_llr.score(sc_tt_x, tt_y))
        
    plt.plot(np.log10(ap_l), t_l)
    plt.plot(np.log10(ap_l), tt_l)
    plt.show()
    ```

    ![i2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/i2.png?raw=true)

    - 학습률이 10일 때 가장 스코어가 좋은 것을 확인할 수 있다.

<br/>

- 다중 선형 회귀

  - 다중 선형 회귀

    - 여러 개의 특성이 있는 경우로 일반화 한 회귀

      ![m1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/m1.gif?raw=true)

    - 여기서 w0는 y축의 절편이고 x0=1

    - 단변량 회귀와 다변향 회귀는 같은 개념과 평가 기법을 사용한다.

  - 예제1

    - 데이터 가져오기

      ```python
      df = pd.read_csv("perch_full.csv")
      X = df.to_numpy()
      Y = np.array(
          [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 
           110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 
           130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 
           197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 
           514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 
           820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 
           1000.0, 1000.0]
           )
      ```

      | length | height | width |
      | :----: | :----: | :---: |
      |  8.4   |  2.11  | 1.41  |
      |  13.7  |  3.53  |   2   |
      |   15   |  3.82  | 2.43  |
      |  16.2  |  4.59  | 2.63  |
      |  17.4  |  4.59  | 2.94  |
      |   18   |  5.22  | 3.32  |
      |  18.7  |  5.2   | 3.12  |
      |   19   |  5.64  | 3.05  |
      |  19.6  |  5.14  | 3.04  |
      |   20   |  5.08  | 2.77  |
      |   21   |  5.69  | 3.56  |
      |   21   |  5.92  | 3.31  |
      |   21   |  5.69  | 3.67  |
      |  21.3  |  6.38  | 3.53  |
      |   22   |  6.11  | 3.41  |
      |   22   |  5.64  | 3.52  |
      |   22   |  6.11  | 3.52  |
      |   22   |  5.88  | 3.52  |
      |   22   |  5.52  |   4   |
      |  22.5  |  5.86  | 3.62  |
      |  22.5  |  6.79  | 3.62  |
      |  22.7  |  5.95  | 3.63  |
      |   23   |  5.22  | 3.63  |
      |  23.5  |  6.28  | 3.72  |
      |   24   |  7.29  | 3.72  |
      |   24   |  6.38  | 3.82  |
      |  24.6  |  6.73  | 4.17  |
      |   25   |  6.44  | 3.68  |
      |  25.6  |  6.56  | 4.24  |
      |  26.5  |  7.17  | 4.14  |
      |  27.3  |  8.32  | 5.14  |
      |  27.5  |  7.17  | 4.34  |
      |  27.5  |  7.05  | 4.34  |
      |  27.5  |  7.28  | 4.57  |
      |   28   |  7.82  |  4.2  |
      |  28.7  |  7.59  | 4.64  |
      |   30   |  7.62  | 4.77  |
      |  32.8  | 10.03  | 6.02  |
      |  34.5  | 10.26  | 6.39  |
      |   35   | 11.49  |  7.8  |
      |  36.5  | 10.88  | 6.86  |
      |   36   | 10.61  | 6.74  |
      |   37   | 10.84  | 6.26  |
      |   37   | 10.57  | 6.37  |
      |   39   | 11.14  | 7.49  |
      |   39   | 11.14  |   6   |
      |   39   | 12.43  | 7.35  |
      |   40   | 11.93  | 7.11  |
      |   40   | 11.73  | 7.22  |
      |   40   | 12.38  | 7.46  |
      |   40   | 11.14  | 6.63  |
      |   42   |  12.8  | 6.87  |
      |   43   | 11.93  | 7.28  |
      |   43   | 12.51  | 7.42  |
      |  43.5  |  12.6  | 8.14  |
      |   44   | 12.49  |  7.6  |

      - 가져온 데이터 정보

    - 학습 데이터와 테스트 데이터 분할

      ```python
      t_x, tt_x, t_y, tt_y = train_test_split(X, Y, train_size= 0.7, random_state= 42)
      ```

    - 데이터 전처리

      ```python
      # 데이터 피쳐 증가 (과소적합 방지)
      p_m = PolynomialFeatures(degree= 5, include_bias= False).fit(t_x) # 학습 데이터 기반으로 피쳐 증가
      p_t_x = p_m.transform(t_x)
      p_tt_x = p_m.transform(tt_x)
      
      # 데이터 밸런싱 작업 (과대적합 방지)
      ss = StandardScaler().fit(p_t_x)  # 한 쪽으로 치우친 피쳐 정보를 균등하게 정리
      sc_t_x = ss.transform(p_t_x)
      sc_tt_x = ss.transform(p_tt_x)
      ```

    - 모델 생성 및 학습

      ```python
      # 모델 생성
      lr = LinearRegression()
      lso = Lasso(alpha= 10)
      rg = Ridge(alpha= 0.1)
      
      # 학습
      lr.fit(sc_t_x, t_y)
      lso.fit(sc_t_x, t_y)
      rg.fit(sc_t_x, t_y)
      ```

    - 테스트 및 검증

      ```python
      y_p = lr.predict(sc_t_x)  # 예측값 생성
      t_s = lr.score(sc_t_x, t_y)  # 학습 데이터를 이용하여 점수 확인
      tt_s = lr.score(sc_tt_x, tt_y)  # 테스트 데이터를 이용하여 점수 확인
      print(t_s, tt_s)  # 1.0 -26.192517921905765
      
      t_s = rg.score(sc_t_x, t_y)
      tt_s = rg.score(sc_tt_x, tt_y) 
      print(t_s, tt_s)  # 0.9897983159614501 0.9842243738800773
      
      t_s = lso.score(sc_t_x, t_y)
      tt_s = lso.score(sc_tt_x, tt_y)
      print(t_s, tt_s)  # 0.9882059522438204 0.9834044009315154
      ```

      - 단순 선형 회귀에서는 과도한 피쳐로 인해 과대적합이 발생한 것을 볼 수 있다.
      - 패널티텀이 있는 Ridge, Lasso는 과대적합이 발생하지 않았다.

  - 예제2(단순 선형 휘귀, Ridge, Lasso, SGD 들의 성능평가 및 오차 확인)

    - 데이터 가져오기

      ```python
      import numpy as np
      import pandas as pd
      from sklearn.model_selection import train_test_split
      from sklearn.preprocessing import PolynomialFeatures, StandardScaler
      from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDRegressor
      import matplotlib.pyplot as plt
      from sklearn.datasets import load_boston
      
      b_data = load_boston()
      
      X = b_data.data
      Y = b_data.target
      ```

      - 보스턴 집 값에 대한 데이터를 불러온다. 
      - [boston dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)

    - 데이터 전처리

      ```python
      y_data = Y.reshape(-1, 1)
      
      # min-max 스케일로 데이터 전처리
      from sklearn.preprocessing import MinMaxScaler
      
      mm_m = MinMaxScaler(feature_range= (0, 5)).fit(X)
      sc_x_data = mm_m.transform(X)
      ```

    - 데이터 분류

      ```python
      t_x, tt_x, t_y, tt_y = train_test_split(sc_x_data, y_data, random_state= 10)
      ```

    - 모델 생성 및 학습

      ```python
      lr = LinearRegression(fit_intercept= True, normalize= False)
      rg = Ridge(fit_intercept= True, alpha= 0.1, normalize= False)
      lso = Lasso(fit_intercept= True, alpha= 0.1, normalize= False)
      sgd = SGDRegressor(fit_intercept= True, alpha= 0.1, max_iter= 1000)
      
      lr.fit(t_x, t_y)
      rg.fit(t_x, t_y)
      lso.fit(t_x, t_y)
      sgd.fit(t_x, t_y)
      ```

    - 성능 평가

      ```python
      lr_t_s = lr.score(t_x, t_y)
      lr_tt_s = lr.score(tt_x, tt_y)
      print(lr_t_s, lr_tt_s)  # 0.7575686094674801 0.6745585065949402
      
      rg_t_s = rg.score(t_x, t_y)
      rg_tt_s = rg.score(tt_x, tt_y)
      print(rg_t_s, rg_tt_s)  # 0.7575684841957252 0.6745476506883203
      
      lso_t_s = lso.score(t_x, t_y)
      lso_tt_s = lso.score(tt_x, tt_y)
      print(lso_t_s, lso_tt_s)  # 0.7533819825998691 0.6721307653867756
      
      sgd_t_s = sgd.score(t_x, t_y)
      sgd_tt_s = sgd.score(tt_x, tt_y)
      print(sgd_t_s, sgd_tt_s)  # 0.7014985114235619 0.5881283162637609
      ```

    - 오차값 확인(MAE, RMSE, R-squared)

      ```python
      from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
      
      lr_y_pred = lr.predict(tt_x)
      print(mean_absolute_error(tt_y, lr_y_pred), mean_squared_error(tt_y, lr_y_pred, squared= False), r2_score(tt_y, lr_y_pred))
      # 3.857300907995506 5.695835030617262 0.6745585065949402
      
      rg_y_pred = rg.predict(tt_x)
      print(mean_absolute_error(tt_y, rg_y_pred), mean_squared_error(tt_y, rg_y_pred, squared= False), r2_score(tt_y, rg_y_pred))
      # 3.857252955096328 5.6959300291635815 0.6745476506883203
      
      lso_y_pred = lso.predict(tt_x)
      print(mean_absolute_error(tt_y, lso_y_pred), mean_squared_error(tt_y, lso_y_pred, squared= False), r2_score(tt_y, lso_y_pred))
      # 3.847956035420201 5.7170405633365124 0.6721307653867756
      
      sgd_y_pred = sgd.predict(tt_x)
      print(mean_absolute_error(tt_y, sgd_y_pred), mean_squared_error(tt_y, sgd_y_pred, squared= False), r2_score(tt_y, sgd_y_pred))
      # 4.268502300866182 6.407696124061427 0.5881283162637609
      ```

    - 그래프로 확인(SGD)

      ```python
      plt.scatter(tt_y, sgd_y_pred)
      plt.show()
      ```

      ![i3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/i3.png?raw=true)

      - 예측값과 실제값으로 그래프를 그렸을 때 점들의 모임이 y=x에 가까운 직선 모양이 나타나면 학습이 잘 된 것이다.

  <br/>

- 로지스틱 회귀

  - 로지스틱 휘귀는 예측을 위한 선형 모델을 만드는게 아니라 분류를 위한 선형 모델을 만든다.

  - 선을 기준으로 데이터들을 나눔

  - 아래 수식으로 기존 선형회귀 모델을 적용

    ![m2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/m2.gif?raw=true)

    ![m3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/m3.gif?raw=true)

  - 문제점

    - f(x)의 값이 1 이상이나 0 이하로 나올 수 있음
    - 각 피쳐들이 Y에 영향을 주는 것을 해석하는 문제
    - 사건의 발생 여부는 이산적인데 실제 f(x) 수식은 연속적

  - 로지스틱 회귀의 개념

    - 이진 분류 문제를 확률로 표현

    - 어떤 사건이 일어날 확률을 P(X)로 나타냄

    - 오즈비(odds ratio) : 어떤 사건이 일어날 확률과 일어나지 않을 확률의 비율

      ![m4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/m4.gif?raw=true)

    - 확률이 올라갈수록 오즈비도 급속히 상승

  - 로짓 함수

    - 오즈비에 상용로그를 붙인 수식

      ![m5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/m5.gif?raw=true)

    - X 값으로 확률을 넣으면 logit(P) 꼴로 나타남

    - 확률을 구하려면 기존 함수의 역함수를 취하여 연산

      ![m6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/m6.gif?raw=true)

      ![m7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/m7.gif?raw=true)

      ![m8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2004/m8.gif?raw=true)

  - 로지스틱 함수

    - 로짓 함수의 역함수
    - 그래프가 S자 커브 형태인 시그모이드 함수

  - 로지스틱 회귀

    - 종속변수가 이분형일 때 수행할 수 있는, 예측 분석을 위한 회귀분석 기법
    - 시그모이드 함수 수식
      - y값을 확률 p로 표현
      - z값은 선형회귀와 같이 가중치와 피쳐의 선형 결합으로 표현 가능

  - 예제

    ```python
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    import matplotlib.pyplot as plt
    
    data = pd.read_csv('data1_all.csv')
    data_X = data[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
    data_Y = data['Name'].to_numpy()
    
    ss = StandardScaler().fit(data_X)
    data_X = ss.transform(data_X)
    
    t_x, tt_x, t_y, tt_y = train_test_split(data_X, data_Y, random_state= 42)
    
    lr = LogisticRegression(C= 20, max_iter= 1000)  # 다중 분류
    lr.fit(t_x, t_y)
    print(lr.score(t_x, t_y), lr.score(tt_x, tt_y))  # 0.9243697478991597 0.925
    
    d = lr.decision_function(t_x[:1])
    
    from scipy.special import softmax  # 다중 분류 소프트맥스 함수
    np.round(softmax(d), decimals=100)  
    # array([[9.97344580e-01, 2.40091388e-05, 1.94420730e-03, 6.86917141e-04,
    #        1.33350861e-07, 1.53023678e-07, 4.07376103e-15]]) 
    ```

    - 소프트맥스 함수는 다중으로 존재하는 특성들의 가중치를 각각 확률을 부여하여 나눈다. 모든 확률을 더하면 1이 된다.

    ```python
    i=(t_y == 'A')|(t_y == 'G')
    b_t_x=t_x[i]
    b_t_y=t_y[i]
    ```

    - 데이터를 다중 데이터에서 2중 데이터로 변경

    ```python
    lr = LogisticRegression()
    lr.fit(b_t_x,b_t_y)
    d=lr.decision_function(b_t_x[:1])  # array([-5.96982853])
    # 학습된 공식에 x값을 넣어 결과를 출력
    ```

    - 학습된 공식에 값을 넣어 출력을 확인하니 확률이 아닌 특정 값이 나오는 것을 볼 수 있다.

    ``` python
    from scipy.special import expit
    expit(d)  # array([0.00254817])
    ```

    - 위에서 나온 값을 내장되어 있는 시그모이드 함수에 넣어 확률을 출력한다.

  - 2중 분류는 시그모이드 함수, 다중 분류는 소프트맥스 함수를 사용하는 것을 기억한다.



