# 머신러닝 딥러닝 기초 05

<br/>

### 로지스틱 회귀

> 04 문서에서 이어지는 내용

<br/>

- 가설함수

  ![m1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m1.gif?raw=true)

  - z는 가중치 값과 피쳐 값의 선형 결합

  - 가중치 값을 찾는 학습을 위해 경사하강법 알고리즘 사용

    ![m2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m2.gif?raw=true)

  <br/>

- 비용함수

  - 먼저 비용함수를 정의하고 예측값과 실제값 간의 차이를 최소화하는 방향으로 학습

  - 실제값이 1일 때와 실제값이 0일 때 각각 다르게 비용함수를 정의

    ![m3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m3.gif?raw=true)

    <br/>

    ![i1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i1.png?raw=true)

  - (a)는 y=1일 때, (b)는 y=0일 때 비용함수 그래프 (0 <= h <= 1)

  - (a)에서 h값이 1에 가까워질수록 비용함수가 0에 가까워짐

  - (b)에서 h값이 0에 가까워질수록 비용함수가 0에 가까워짐

  <br/>

- 두 경우의 비용함수를 하나로 통합

  ![m5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m5.png?raw=true)

  <br/>

- 비용함수의 미분과 가중치 업데이트

  - 세타의 최적값을 구하기 위해 J값을 세타에 대해 미분

  - 세타는 z값 안에 있는 wj의 집합

    ![m6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m6.png?raw=true)

  <br/>

- 가중치 값을 업데이트

  - 선형회귀와 동일하게 모든 세타에 대해 동시에 가중치가 업데이트됨

    ![m7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m7.png?raw=true)

  <br/>

- 모델을 평가하는 성능지표들

  - 회귀 : MAE, MSE, RMSE, SSE
  - 분류 : 정확도, 정밀도, 민감도, F1 스코어, ROC 커브, 리프트 차트
  - 클러스트링 : DBI, 엘보우 메서드, 실루엣계수

  <br/>

- 여러가지 상황을 고려하여 모델의 성능지표를 선택해야 함

  - 모델이 다른 모델보다 경제적으로 나은가?
  - 모델이 사용하는 데이터가 많은가? 또는 적은가?
  - 모델이 용량이 작은 컴퓨터에서도 작동하는가?
  - 모델의 손해가 얼마나 나는가

  <br/>

- 혼동행렬

  - 예측값이 실제값 대비 얼마나 잘 맞는지 2x2 행렬로 표현

  - 일반적으로 질문의 '예'에 해당하는 값은 True 또는 1, '아니오'에 해당하는 값은 False 또는 0

    ![i2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i2.png?raw=true)

    <br/>

    ```python
    from sklearn.metrics import confusion_matrix
    
    y_t = [1, 0, 1, 1, 0, 1]
    y_p = [0, 0, 1, 1, 0, 1]
    
    confusion_matrix(y_t, y_p)
    
    #array([[2, 0],
    #       [1, 3]], dtype=int64)
    ```

  <br/>

- 실제값과 예측값의 조합으로 발생 가능한 4가지 경우

  - True Positive(TP)
    - 예측값과 실제값이 모두 1로 동일할 때, 즉 모델의 예측값이 정답이고 예측 대상이 1일 때
  - True Negative(TN)
    - 예측값과 실제값이 모두 0으로 동일할 때, 즉 모델의 예측값이 정답이고 예측 대상이 0일 때
  - False Negative(FN)
    - 실제값은 1이지만 예측값이 0으로, 모델의 예측값이 오답이고 예측값이 0을 예측할 때
  - False Positive(FP)
    - 실제값은 0이지만 예측값이 1로, 모델의 예측값이 오답이고 예측값이 1을 예측할 때

  ```python
  tn, fp, fn, tp = confusion_matrix(y_t, y_p).ravel()
  
  # (2, 0, 1, 3)
  ```

  <br/>

- 혼동행렬표를 사용한 지표

  - 정확도

    - 전체 데이터 개수 대비 정답을 맞춘 데이터의 개수

      ![m9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m9.png?raw=true)

      <br/>

      ![i3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i3.png?raw=true)

      <br/>

      ```python
      from sklearn.metrics import accuracy_score
      
      accuracy_score(y_t, y_p)
      
      # 0.8333333333333334
      ```

  - 정밀도

    - 모델이 1이라고 예측했을 때 얼마나 잘 맞을지에 대한 비율

      ![m10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m10.png?raw=true)

      <br/>

      ![i4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i4.png?raw=true)

      <br/>

      ```python
      from sklearn.metrics import precision_score
      
      precision_score(y_t, y_p)
      
      # 1.0
      ```

  - 민감도

    - 실제 1인 값을 가진 데이터를 모델이 얼마나 1이라고 잘 예측했는지에 대한 비율

    - 반환율 또는 재현율이라고도 부름

      ![m11](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m11.png?raw=true)

      <br/>

      ![i5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i5.png?raw=true)

      <br/>

      ```python
      from sklearn.metrics import recall_score
      
      recall_score(y_t, y_p)
      
      # 0.75
      ```

    - 정밀도와 민감도는 불균일한 데이터셋을 다룰 때 유용

      - 데이터에서 1과 0의 비율이 7:3 또는 3:7 이상 차이나는 상태

  - F1 스코어

    - 정밀도와 민감도의 조화평균 값

      ![m12](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m12.png?raw=true)

      <br/>

      ```python
      from sklearn.metrics import f1_score
      
      
      f1_score(y_t, y_p)
      
      # 0.8571428571428571
      ```

  <br/>

- 예제

  ```python
  import pandas as pd
  from sklearn.preprocessing import MinMaxScaler
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import confusion_matrix
  from sklearn.metrics import accuracy_score
  
  # 데이터 수집
  df = pd.read_csv('day5_data1.csv')
  
  # 데이터 전처리
  # 필요 유무에 따른 정리
  df.pop('who')
  df.pop('Country')
  df.pop('Years on Internet')
  
  # 결측치 확인
  # 원-핫 인코딩을 위한 오브젝트->카테고리화
  ck_c = ['Gender', 'Household Income', 'Sexual Preference', 'Education Attainment', 'Major Occupation', 'Marital Status']
  
  for i in ck_c:
      df[i] = df[i].astype('category')
      
  df_one_hot = pd.get_dummies(df)  # 원-핫 인코딩
  df_one_hot.loc[pd.isnull(df_one_hot['Age']), 'Age'] = df_one_hot['Age'].mean()  # Age의 평균으로 결측치를 채움
  
  #입력 데이터 정리
  X = df_one_hot.iloc[:, 1:].values
  Y = df_one_hot.iloc[:, 0].values.reshape(-1, 1)
  
  m_m_s = MinMaxScaler()
  X_data = m_m_s.fit_transform(X)  # 민-맥스 정규화
  
  t_x, tt_x, t_y, tt_y = train_test_split(X_data, Y, test_size= 0.3, random_state= 42)  # 데이터 분류
  
  #모델 생성 및 학습
  lo_g = LogisticRegression(fit_intercept= True)
  lo_g.fit(t_x, t_y.flatten())
  
  #테스트 및 검증
  lo_g.predict(tt_x[:5])
  
  lo_g.predict_proba(tt_x[:5])  # 확률 확인
  
  y_t = tt_y.copy()
  y_p = lo_g.predict(tt_x)
  
  confusion_matrix(y_t, y_p)  # 혼동 행렬 생성
  accuracy_score(y_t, y_p)  # 정확도 확인
  ```

  <br/>

- 다중클래스 분류

  - 2개 이상의 클래스를 가진 y 값에 대한 분류

  <br/>

- 다중클래스와 다중레이블

  - 다중클래스는 중복 선택이 불가능
  - 다중레이블은 중복 선택이 가능

  <br/>

- One-vs-All

  - m개의 클래스가 존재할 때 각 클래스마다 분류기를 생성하여 분류
  - One-vs-Rest라고도 부름
  - 대표적으로 소프트맥스 분류

  <br/>

- One-vs-One

  - m개의 클래스가 있다면, 이 클래스의 분류기를 하나의 클래스로 하고 나머지 클래스의 분류기들을 만들어 최종적으로 각 분류기들의 결과를 투표로 결정
  - 총 m(m-1)/2 개 만큼의 분류기를 생성
  - 분류기가 많아질수록 정확도가 높아지지만 비용도 증가

  <br/>

- 소프트맥스 함수

  - 시그모이드 함수로 다중클래스 분류 문제를 다룰 수 있음

  - 각각의 클래스에 속하는지 속하지 않는지에 대한 이진분류기를 m개 생성한 후, 가장 높은 확률이 나오는 클래스를 선택

  - 그러나 모든 확률의 합이 1 이상이 된다는 문제가 발생함

    - 문제 해결로 모든 클래스들의 발생 확률을 1로 정규

    ![m13](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m13.png?raw=true)

    <br/>

    ```python
    # 소프트맥스 함수 구현
    
    import numpy as np
    
    def s_max(z):
        a_v = np.exp(z)
        return a_v / sum(a_v)
    
    z = [2, 1, 5, 0.5]
    
    y = s_max(z)
    
    # array([0.04613281, 0.01697131, 0.92660226, 0.01029362])

  - 다중클래스 분류에서 여러 선형회귀의 출력 결과를 정규화하여 합이 1이 되도록 만드는 함수

  <br/>

- 소프트맥스 분류

  - 오즈비에 logit함수를 붙여 최종적으로 구한 가중치 값

    ![m14](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m14.png?raw=true)

  - 기존의 오즈비는 이진분류이지만, 다중클래스 분류는 j번째 대상에 대한 전체 대비 비율을 나타냄

    ![m15](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m15.png?raw=true)

  - 다중클래스 분류에서는 j개의 클래스가 있다면 클래스의 개수만큼 가중치에 대한 벡터를 구함

  - 피쳐 벡터와 각 클래스별로 존재하는 가중치 행렬의 선형결합을 z로 나타냄

    ![m16](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m16.png?raw=true)

    ![m17](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m17.png?raw=true)

    ![m18](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m18.png?raw=true)

    ![m19](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m19.png?raw=true)

  <br/>

- 소프트맥스 함수로 학습

  - 수식에서 세타를 학습, 즉 각 클래스마다 적절한 세타j를 찾기

  - 각 가설함수는 각 클래스와 발생확률로 표현 가능

    ![m20](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m20.png?raw=true)

  - 최대우도추정법(MLE)을 사용해서 Pj 확률을 최대화하는 세타를 찾기

    ![m21](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m21.png?raw=true)

  - 수식을 손실로 생각하여 수식 L로 표현하고 해당 값을 최대화하는 방향으로 정리

    ![m22](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m22.png?raw=true)

    ![m23](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m23.png?raw=true)

  - 최종적으로 아래를 구하고자 함

    ![m24](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m24.png?raw=true)

  <br/>

- 예제

  ```python
  from sklearn.datasets import load_digits
  
  # 데이터 가져오기
  data = load_digits()
  
  data['images'][0]  # 이미지라서 가로 세로로 피쳐가 있음
  
  # 데이터 전처리
  print(data['data'][0].shape)  # dataset에서 이미 전처리가 되어 있는 것을 확인할 수 있음
  ```

  ![j2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j2.png?raw=true)

  ```python
  # 데이터 전처리
  data['data'][0].shape  # (64,)  # dataset에서 이미 전처리가 되어 있는 것을 확인할 수 있음
  ```

  ```python
  # 데이터 분류
  
  X = data['data']
  Y = data['target']
  
  from sklearn.model_selection import train_test_split
  
  t_x, tt_x, t_y, tt_y = train_test_split(X, Y, random_state= 42)
  ```

  ````python
  # 모델 생성 및 학습
  
  from sklearn.linear_model import LogisticRegression
  
  lo_g_ovr = LogisticRegression(multi_class= 'ovr')
  lo_g_s_max = LogisticRegression(multi_class= 'multinomial', solver= 'sag')
  
  lo_g_ovr.fit(t_x, t_y)
  lo_g_s_max.fit(t_x, t_y)
  ````

  ```python
  # 테스트 및 검증
  
  from sklearn.metrics import confusion_matrix
  
  y_t = tt_y.copy()
  y_p = lo_g_ovr.predict(tt_x)
  
  confusion_matrix(y_t, y_p)
  ```

  ![j3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j3.png?raw=true)

  ```python
  # 평가 지표들을 해당 모듈로 확인할 수 있음
  from sklearn.metrics import classification_report
  
  print(classification_report(y_t, y_p))
  ```

  ![j4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j4.png?raw=true)

  <br/>

- TPR과 FPR을 각각 y축, x축에 나타내여 그래프를 작성

  ![i6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i6.png?raw=true)

  <br/>

  ```python
  import numpy as np
  from sklearn.metrics import roc_curve
  
  y = np.array([1, 1, 2, 2])
  sc = np.array([0.1, 0.4, 0.35, 0.8])
  
  fpr, tpr, th = roc_curve(y, sc, pos_label= 2)
  
  #(array([0. , 0. , 0.5, 0.5, 1. ]),
  # array([0. , 0.5, 0.5, 1. , 1. ]),
  # array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ]))
  ```

  <br/>

- AUC

  - ROC 커브 하단의 넓이

  - 대표적인 ROC 커브로 모델들의 성능을 단 하나의 숫자로 표현할 수 있다는 점에서 불균형 데이터셋의 성능을 평가할 때 많이 사용

    ```python
    from sklearn.metrics import auc
    
    roc_auc= auc(fpr, tpr)
    # 0.75
    ```

<br/>

---

### NBC(나이브 베이지안 분류기)

<br/>

- 확률의 표현

  - 이산형 값의 확률

    ![m25](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m25.png?raw=true)

  <br/>

- 연속형 값의 확률

  - 해당 데이터를 적절히 표현하는 함수를 생성한 후 해당 함수의 적분 값을 취한다.
  - 특정 위치의 값은 없고 적분 값을 취해 구간의 확률을 계산

  <br/>

- 확률의 기본 성질

  - 확률은 모든 사건에 대해 반드시 0에서 1 사이의 값을 가짐
  - 각 사건들이 서로 관계가 없는 경우, 즉 각 사건들이 일어날 확률이 다른 사건이 일어날 확률에 영향을 미치지 않을 때 각 사건들이 ''독립''되었다고 정의

  <br/>

- 조건부 확률

  - 어떤 사건이 일어난다고 가정했을 때 다른 사건이 일어날 확률

  - P(A|B)

    - B라는 사건이 발생했을 때 A와 B 사건의 교집합이 발생할 확률

      ![m26](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m26.png?raw=true)

  <br/>

- 베이즈 정리

  - 두 확률 변수의 사전확률과 사후확률 사이의 관계를 나타내는 정리

    ![m27](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m27.png?raw=true)

  - H는 가설, D는 데이터일 때 P(H|D)는 사후확률

    - 사후확률 : 데이터가 주어졌을 때 해당 가설이 맞는지에 대한 확률
    - 가능도 : 어떤 사건이 발생했을 때 다음 사건이 발생할 수 있는 모든 확률의 발생가능한 정도를 확률로 나타냄
    - P(D|H) : 가설이 주어졌을 때 해당 데이터가 존재할 확률

  <br/>

- 메일에 비아그라라는 단어가 들어가면 어느 정도의 확률로 스팸메일인지 판단하는 베이즈 분류기

  ![m28](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m28.png?raw=true)

  <br/>

  ```python
  from pandas import Series, DataFrame
  import pandas as pd
  import numpy as np
  
  viagra_spam = {
      'viagra': [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1],
      'spam': [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
  }
  
  df = pd.DataFrame(viagra_spam, columns= ['viagra', 'spam'])
  np_data = df.values
  ```

  ```python
  # P(viagra and spam)
  p_v_cap_s = sum((np_data[:, 0] == 1) & (np_data[:, 1] == 1)) / 20
  
  # 0.15
  ```

  ```python
  # P(viagra)
  p_v = sum((np_data[:, 0] == 1)) / 20
  
  # 0.3
  ```

  ```python
  # P(spam)
  p_s = sum((np_data[:, 1] == 1)) / 20
  
  # 0.3
  ```

  ```python
  # P(not viagra and spam)
  p_n_v_cap_s = sum((np_data[:, 0] == 0) & (np_data[:, 1] == 1)) / 20
  
  # 0.15
  ```

  ```python
  # P(spam|viagra)
  (p_v_cap_s / p_s) * p_s / p_v
  
  # 0.5
  ```

  ```python
  # P(spam|not viagra)
  (p_n_v_cap_s / p_s) * p_s / (1 - p_v)
  
  # 0.214
  ```

  - 위 데이터에서 viagra라는 단어가 포함되었을 때 스팸메일일 확률(=0.5)은 viagra라는 단어가 포함되지 않았을 때 스팸메일일 확률(=0.241)보다 높음
  - 따라서 viagra라는 단어가 있으면 스팸메일로 분류하는 것이 합리적이라는 것을 알 수 있다.
  - 하지만 viagra라는 단어 외에 스팸메일인지에 영향을 주는 단어가 있을 수 있다.
  - 또는 스팸메일으로 잘못 제외되는 경우도 생길 수 있다. (의학적 용도로 사용하는 경우 등)
  - 나이브 베이지안 분류기가 위 문제점을 해결

  <br/>

- 나이브 베이지안 분류기

  - 여러 개의 열을 사용하여 분류기를 구성

  - Y값을 따로 빼내고 X 데이터들을 원핫인코딩으로 처리

    ![m29](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m29.png?raw=true)

    <br/>

    ```python
    from pandas import Series, DataFrame
    import pandas as pd
    import numpy as np
    
    # 데이터 가져오기
    df = pd.read_csv('day5_data2.csv')
    ```

    ![j5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j5.png?raw=true)

    ```python
    # 데이터 전처리
    del df['ID']
    ```

    ![j6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j6.png?raw=true)

    ```python
    # Y 값들을 따로 빼내기
    Y_data = df.pop('Fraud')
    Y_data = Y_data.values
    ```

    ```python
    # X 데이터들을 원핫인코딩으로 처리
    X_df = pd.get_dummies(df)
    X_df.head(5).T
    ```

    ![j7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j7.png?raw=true)

    ```python
    X_data = X_df.values
    ```

    ![j8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j8.png?raw=true)

    ```python
    P_Y_T = sum(Y_data == True) / len(Y_data)  # 0.3
    P_Y_F = 1 - P_Y_T  # 0.7
    ```

    ```python
    idx_Y_T = np.where(Y_data == True)
    idx_Y_F = np.where(Y_data == False)
    
    p_x_y_t = (X_data[idx_Y_T].sum(axis= 0)) / sum(Y_data == True)
    p_x_y_f = (X_data[idx_Y_F].sum(axis= 0)) / sum(Y_data == False)
    ```

    ![j9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j9.png?raw=true)

    ```python
    tt_x = [0, 1, 0, 0, 0, 1, 0, 0, 1, 0]
    tt_y_t = P_Y_T + p_x_y_t.dot(tt_x)  # 1.6333333333333333
    tt_y_f = P_Y_F + p_x_y_f.dot(tt_x)  # 1.7714285714285714
    ```

  <br/>

- BoW

  - 단어별로 인덱스가 부여되어 있을 때 한 문장 또는 한 문서에 대한 벡터를 표현하는 기법

  - 하나의 단어를 벡터화시킬 때는 원핫인코딩 기법을 사용

    ![i7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i7.png?raw=true)

  - 전체 문서에 있는 모든 단어들에 이미 인덱스가 부여되어 있고 출현한 단어에 대해서만 단어의 개수를 벡터로 표현

  <br/>

- 베르누이 나이브 베이지안 분류기

  - 다루고자 하는 모든 데이터가 불린 피쳐
  - 사용되는 데이터 타입은 이산형 데이터인데, 이러한 데이터를 모두 불린 타입으로 변경하여 학습
    - 정수 타입 숫자라면 임계값 기준으로 True 또는 False로 변환
  - class_log_prior_는 각 클래스마다 prior의 값에 log를 붙여서 값을 출력

    ```python
    y_example_text = ["Sports", "Not sports","Sports","Sports","Not sports"]
    y_example = [1 if c=="Sports" else 0 for c in y_example_text ]
    text_example = ["A great game game", "The The election was over","Very clean game match","A clean but forgettable game game","It was a close election", ]
    ```

    ```python
    from sklearn.feature_extraction.text import CountVectorizer
    
    c_v = CountVectorizer()
    X_example = c_v.fit_transform(text_example)
    
    c_v.get_feature_names()
    ```

    ![j10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j10.png?raw=true)

    ```python
    X_example.toarray()
    ```

    ![j11](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j11.png?raw=true)

    ```python
    c_v.vocabulary_
    ```

    ![j12](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j12.png?raw=true)

    ```python
    from sklearn.naive_bayes import *
    
    clf = BernoulliNB(alpha= 1, binarize= 0)
    clf.fit(X_example, y_example)
    ```

    ```python
    clf.class_log_prior_  # array([-0.91629073, -0.51082562])
    ```

  <br/>

- 다항 나이브 베이지안 분류기

  - 베르누이 분류기와 달리 각 피쳐들이 이산형이지만, 이진값이 아닌 여러 개의 값을 가질 수 있다.

  - 나이브 베이지안 식을 변형하여 사용

    ![m32](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m32.png?raw=true)

  - 가능도

    ![m31](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m31.png?raw=true)

  <br/>

- 가우시안 나이브 베이지안 분류기

  - 연속형 값을 피쳐로 가진 데이터의 확률을 구하기 위해 y의 분포를 정규분포로 가정

  - 확률밀도 함수 상의 해당 값 x가 나올 확률로 나이브 베이지안을 구현

  - 가능도

    ![m32](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m32.png?raw=true)

  <br/>

- 예제

  - 20개의 뉴스 텍스트 데이터를 주제별로 분류하는 문제

    - 사이킷런에서 제공하는 20newsgroup 데이터셋과 나이브 베이지안 분류기를 사용

    ![i8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i8.png?raw=true)

  - 벡터화

    - BoW를 생성하는 CountVectorizer를 약간 변형한 TF-idfVectorizer를 생성하여 텍스트를 벡터화

  - 교차 검증

    - 모델 성능을 여러 번 측정하여 평균치를 측정

  - 파이프라인

    - 데이터 전처리부터 성능 측정까지 연결된 코드로 나타냄

  - BoW에 해당하는 CountVectorizer 외의 벡터화 기법들

    - tfidf : 전체 문서에서 많이 나오는 단어의 중요도는 줄이고 해당 문서에만 많이 나오는 단어의 중요도를 올리는 기법

    - TF : 문서에서 해당 단어가 얼마나 나왔는지 나타내주는 빈도 수

    - DF : 해당 단어가 있는 문서의 수

    - IDF : 해당 단어가 있는 문서의 수가 높아질수록 가중치를 축소하기 위해 역수를 취함

      ![m33](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m33.png?raw=true)

      ![m34](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/m34.png?raw=true)

    - 여러 문서에서 단어가 많이 나오면 밑 수식에서 로그 값이 작아지면서 중요도를 떨어뜨림

    - 사이킷런에서는 TfidfVectorizer 클래스 사용

  - 토큰

    - 인덱스를 지정해야 하는 단어들의 리스트를 정리하는 기법

  - 어간 추출

    - 띄어쓰기 기준이 아닌 의미나 역할이 다른 단어들을 기준으로 분리

  - 문법적 기준을 기반으로 어근이나 어미를 토큰으로 사용

  - 모델링

    ![i9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i9.png?raw=true)

    <br/>

    ![i10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/i10.png?raw=true)

  ```python
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns
  import re
  
  # 데이터 준비
  from sklearn.datasets import fetch_20newsgroups
  new = fetch_20newsgroups(subset='all')
  new.keys()
  ```

  ```python
  new.target  # array([10,  3, 17, ...,  3,  1,  7])
  ```

  ```python
  new.target_names
  ```

  ![j13](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j13.png?raw=true)

  ```python
  # 데이터 전처리
  new_df = pd.DataFrame({'News':new.data,'Target':new.target})
  ```

  ![j14](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j14.png?raw=true)

  ```python
  t_dict ={idx:name for  idx,name in enumerate(new.target_names)}
  ```

  ![j15](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j15.png?raw=true)

  ```python
  new_df['Target'] = new_df['Target'].replace(t_dict)
  
  def data_cleansing(df):
      delete_email = re.sub(r'\b[\w\+]+@[\w]+.[\w]+.[\w]+.[\w]+\b', ' ', df)
      delete_number = re.sub(r'\b|\d+|\b', ' ',delete_email)
      delete_non_word = re.sub(r'\b[\W]+\b', ' ', delete_number)
      cleaning_result = ' '.join(delete_non_word.split())
      return cleaning_result
  new_df.loc[:,'News'] = new_df['News'].apply(data_cleansing)
  ```

  ![j16](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j16.png?raw=true)

  ```python
  # nltk 예시
  from nltk import stem
  stmmer = stem.SnowballStemmer('english')
  ex_t='looking looks looked'
  [stmmer.stem(i) for i in ex_t.split()]  # ['look', 'look', 'look'] 
  ```

  ```python
  # 벡터화
  from sklearn.feature_extraction.text import CountVectorizer
  import nltk
  
  stmmer = nltk.stem.SnowballStemmer('english')
  
  class S_C_V(CountVectorizer):
      def build_analyzer(self):
          analyzer = super(S_C_V,self).build_analyzer()
          return lambda doc:(stmmer.stem(w)for w in analyzer(doc))
      
  from sklearn.feature_extraction.text import TfidfVectorizer
  stmmer = nltk.stem.SnowballStemmer('english')
  class S_T_V(TfidfVectorizer):
      def build_analyzer(self):
          analyzer = super(S_T_V,self).build_analyzer()
          return lambda doc:(stmmer.stem(w)for w in analyzer(doc))
      
  from sklearn.naive_bayes import MultinomialNB, BernoulliNB,GaussianNB
  from sklearn.linear_model import LogisticRegression
  from sklearn.pipeline import Pipeline,make_pipeline
  vectorzizer = [CountVectorizer(),TfidfVectorizer(),S_C_V(),S_T_V()]
  algth = [MultinomialNB(),LogisticRegression()]
  pipe_l = []
  
  import itertools
  for i in list(itertools.product(vectorzizer,algth)):
      pipe_l.append(make_pipeline(*i))
      
      
  n_par=[(1,1),(1,3)]
  stopw_par=['english']
  l_case_par=[True,False]
  max_par=np.linspace(0.4,0.6,num=6)
  min_par=np.linspace(0.0,0.0,num=1)
  
  
  att={'ngram_range':n_par,'max_df':max_par,'min_df':min_par,
      'lowercase':l_case_par,'stop_words':stopw_par,
      }
  vectorzizer_names=['countvectorizer','tfidfvectorizer','s_c_v','s_t_v']
  vectorzizer_par_dict={}
  for i in vectorzizer_names:
      vectorzizer_par_dict[i]={}
      for key,v in att.items():
          par_name = i + "__" + key
          vectorzizer_par_dict[i][par_name] = v        
          
          
  algorithm_names = ["multinomialnb", "logisticregression"]
  
  algorithm_params_dict = {}
  alpha_params = np.linspace(1.0, 1.0, num=1)
  for i in range(1):
      algorithm_params_dict[algorithm_names[i]] = {
          algorithm_names[i]+ "__alpha" : alpha_params
      }
  c_params = [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]
  
  algorithm_params_dict[algorithm_names[1]] = [{
      "logisticregression__multi_class" : ["multinomial"],
      "logisticregression__solver" : ["saga"],
      "logisticregression__penalty" : ["l1"],
      "logisticregression__C" : c_params
      },{ 
      "logisticregression__multi_class" : ["ovr"],
      "logisticregression__solver" : ['liblinear'],
      "logisticregression__penalty" : ["l2"],
      "logisticregression__C" : c_params
      }
      ]    
  pipeline_params= []
  for case in list(itertools.product(vectorzizer_names, algorithm_names)):
      vect_params = vectorzizer_par_dict[case[0]].copy()
      algo_params = algorithm_params_dict[case[1]]  
      
      if isinstance(algo_params, dict):
          vect_params.update(algo_params)
          pipeline_params.append(vect_params)
      else:
          temp = []
          for param in algo_params:
              vect_params.update(param)
              temp.append(vect_params)
          pipeline_params.append(temp)
          
          
  from sklearn.preprocessing import LabelEncoder
  
  X_data = new_df.loc[:, 'News'].tolist()
  y_data = new_df['Target'].tolist()
  y = LabelEncoder().fit_transform(y_data)
  
  from sklearn.model_selection import GridSearchCV
  from sklearn.metrics import classification_report, accuracy_score
  
  scoring = ['accuracy']
  estimator_results = []
  for i, (estimator, params) in enumerate(zip(pipe_l,pipeline_params)):
      n_jobs = -1
      gs_estimator = GridSearchCV(
              refit="accuracy", estimator=estimator,param_grid=params,
              scoring=scoring, cv=5, verbose=1, n_jobs=n_jobs)
      print(gs_estimator)
      
      gs_estimator.fit(X_data, y)
      estimator_results.append(gs_estimator)
  ```

<br/>

---

# 결정트리

<br/>

- 간단한 예시

  ```python
  import pandas as pd
  wine = pd.read_csv('day5_data4.csv')
  ```

  ![j17](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j17.png?raw=true)

  ```python
  wine.describe()
  ```

  ![j18](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j18.png?raw=true)

  ```python
  X = wine[['alcohol', 'sugar', 'pH']].to_numpy()
  Y = wine['class'].to_numpy()
  
  # 데이터 정규화
  from sklearn.preprocessing import StandardScaler
  x_data = StandardScaler().fit_transform(X)
  
  # 데이터 분류
  from sklearn.model_selection import train_test_split
  t_x, tt_x, t_y, tt_y = train_test_split(x_data, Y, random_state= 42)
  
  # 모델링 및 학습
  from sklearn.linear_model import LogisticRegression
  lo_g = LogisticRegression()
  lo_g.fit(t_x, t_y)
  ```

  ```python
  lo_g.score(t_x, t_y), lo_g.score(tt_x, tt_y)  # (0.7859195402298851, 0.7655384615384615)
  ```

  - 학습이 그다지 잘 되어 보이지 않음

  ```python
  # 모델링 및 학습 (결정트리)
  from sklearn.tree import DecisionTreeClassifier
  
  dt = DecisionTreeClassifier(random_state= 42)
  dt.fit(t_x, t_y)
  ```

  ```python
  dt.score(t_x, t_y), dt.score(tt_x, tt_y)  # (0.9973316912972086, 0.8498461538461538)
  ```

  - 과대적합이 발생

  ```python
  import matplotlib.pyplot as plt
  from sklearn.tree import plot_tree
  
  plt.figure(figsize= (10, 7))
  plot_tree(dt)  # 트리 구조를 이미지로 볼 수 있게 해줌
  plt.show()
  ```

  ![j19](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j19.png?raw=true)

  ```python
  plt.figure(figsize= (10, 7))
  plot_tree(dt, max_depth= 1, filled= True, feature_names= ['alcohol', 'sugar', 'pH'])
  plt.show()
  ```

  ![j20](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2005/j20.png?raw=true)

  - depth를 조정해서 노드를 자세하게 볼 수 있게 함

  ```python
  # 모델링 및 학습
  dt1 = DecisionTreeClassifier(max_depth= 5, random_state= 42)
  dt1.fit(t_x, t_y)
  ```

  - 학습에 제약을 걸어 과대적합이 발생하지 않게 함

  ```python
  dt1.score(t_x, t_y), dt1.score(tt_x, tt_y)  # (0.8725369458128078, 0.8584615384615385)
  ```

  ```python
  # 변수 중요도 출력
  dt1.feature_importances_  # array([0.13860406, 0.72615713, 0.1352388 ])



