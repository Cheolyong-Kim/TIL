# 머신러닝 딥러닝 기초 09

<br/>

- 옵티마이저

  - 텐서플로우로 모델을 컴파일할 때 옵티마이저를 설정해줄 수 있다.

    ```python
    model.compile(optimizer= 'sgd', loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')
    ```

  - 모델은 08 문서의 예제에서 이어짐.

  - 옵티마이저를 직접 설정해서 넣을 수도 있다.

    ```python
    sgd = keras.optimizers.SGD(learning_rate = 0.1)
    model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')
    ```

    - 직접 설정해 넣으면서 SGD의 하이퍼 매개변수를 조절해줄 수 있게 된다.

    ```python
    # 모멘텀 기법 추가 (https://beoks.tistory.com/30)
    sgd = keras.optimizers.SGD(momentum = 0.9, nesterov = True)
    ```

  - 옵티마이저를 따로 설정해주지 않으면 RMSprop가 기본적으로 설정된다.

  - 보통 Adagrad를 가장 많이 사용한다.

  - 항상 좋은 옵티마이저는 존재하지 않는다. 입력한 데이터에 맞게 옵티마이저를 선택하는 것이 중요하다.

  - 예제

    ```python
    import tensorflow as tf
    from tensorflow import keras
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    
    (t_x, t_y), (tt_x, tt_y) = keras.datasets.fashion_mnist.load_data()
    s_t_x = t_x / 255.0
    s_tt_x = tt_x / 255.0
    
    from sklearn.model_selection import train_test_split
    t_x, v_x, t_y, v_y = train_test_split(s_t_x, t_y, test_size = 0.2, random_state = 42)
    ```

    ```python
    m1=keras.Sequential()
    m1.add(keras.layers.Flatten(input_shape=(28,28)))
    m1.add(keras.layers.Dense(100,activation='relu'))
    m1.add(keras.layers.Dense(300,activation='relu'))
    m1.add(keras.layers.Dense(10,activation='softmax'))
    m1.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')
    
    m2=keras.Sequential()
    m2.add(keras.layers.Flatten(input_shape=(28,28)))
    m2.add(keras.layers.Dense(300,activation='relu'))
    m2.add(keras.layers.Dense(100,activation='relu'))
    m2.add(keras.layers.Dense(10,activation='softmax'))
    m2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')
    
    m3=keras.Sequential()
    m3.add(keras.layers.Flatten(input_shape=(28,28)))
    m3.add(keras.layers.Dense(300,activation='relu'))
    m3.add(keras.layers.Dense(100,activation='relu'))
    m3.add(keras.layers.Dense(10,activation='softmax'))
    m3.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics='accuracy')
    
    history1=m1.fit(t_x,t_y,epochs=30,validation_data=(v_x,v_y))
    history2=m2.fit(t_x,t_y,epochs=30,validation_data=(v_x,v_y))
    history3=m3.fit(t_x,t_y,epochs=20,validation_data=(v_x,v_y))
    ```

    ```python
    pd.DataFrame(history1.history).plot(figsize=(8,5))
    plt.grid(True)
    plt.gca().set_ylim(0,1)
    plt.show()
    
    pd.DataFrame(history2.history).plot(figsize=(8,5))
    plt.grid(True)
    plt.gca().set_ylim(0,1)
    plt.show()
        
    pd.DataFrame(history3.history).plot(figsize=(8,5))
    plt.grid(True)
    plt.gca().set_ylim(0,1)
    plt.show()
    ```

    ![j2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/j2.png?raw=true)

    ![j3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/j3.png?raw=true)

    ![j4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/j4.png?raw=true)

    - m1 모델의 경우 과대적합이 발생한 것을 볼 수 있다. (검증 데이터의 loss값이 줄어들다가 상승함)
    - m2 모델의 경우 m1 모델보다 뉴런의 개수가 많아져(학습을 과도하게 하여) 더 빠른 속도로 과대적합이 발생했다.
    - m3 모델의 경우 m2 모델과 뉴런의 수는 같지만 과대적합되지는 않았다.
    - 모델의 옵티마이저에 따라 손실률과 정확도가 달라지는 것을 눈으로 확인할 수 있다.

    ```python
    m1.evaluate(s_tt_x,tt_y),m2.evaluate(s_tt_x,tt_y),m3.evaluate(s_tt_x,tt_y)
    
    # ([0.469424307346344, 0.8902000188827515],
    #  [0.5168681144714355, 0.8847000002861023],
    #  [0.3469190299510956, 0.8754000067710876])
    ```

<br/>

- DropOut

  - 과대적합을 방지하기 위해 뉴런의 수를 줄이면?

    ```python
    from sklearn.datasets import fetch_california_housing
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from tensorflow import keras
    import tensorflow as tf
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    
    np.random.seed(42)
    tf.random.set_seed(42)
    
    data = fetch_california_housing()
    in_data = StandardScaler().fit_transform(data.data)
    x_data, tt_x, y_data, tt_y = train_test_split(in_data, data.target, random_state = 42)
    t_x, v_x, t_y, v_y = train_test_split(x_data, y_data, random_state = 42, test_size = 0.2)
    
    t_x.shape[1:]  # (8,)  # 입력 데이터의 차원의 형상
    ```

    ```python
    m = keras.models.Sequential()
    m.add(keras.layers.Flatten(input_shape = t_x.shape[1:]))
    m.add(keras.layers.Dense(30, activation = 'relu'))
    m.add(keras.layers.Dense(1))
    m.summary()
    ```

    ![j9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/j9.png?raw=true)

    ```python
    m.compile(optimizer = keras.optimizers.SGD(learning_rate = 1e-3), loss = 'mean_squared_error')
    hy = m.fit(t_x, t_y, validation_data = (v_x, v_y), epochs = 20)
    
    plt.plot(pd.DataFrame(hy.history))
    ```

    ![j10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/j10.png?raw=true)

    - loss값이 0으로 수렴하지 않고 0.5 근처에서 유지되고 있다.
    - 뉴런의 수가 너무 적어 제대로 학습이 되고 있지 않음. (과소적합)

  - DropOut을 통한 과대적합 방지

    ```python
    import tensorflow as tf
    from tensorflow import keras
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    
    (t_x,t_y),(tt_x,tt_y) = keras.datasets.fashion_mnist.load_data()
    s_t_x = t_x / 255.0
    s_tt_x = tt_x / 255.0
    
    from sklearn.model_selection import train_test_split
    t_x, v_x, t_y, v_y = train_test_split(s_t_x, t_y, test_size = 0.2, random_state = 42)
    
    m = keras.Sequential()
    m.add(keras.layers.Flatten(input_shape = (28, 28)))
    m.add(keras.layers.Dense(100, activation = 'relu'))
    m.add(keras.layers.Dense(300, activation = 'relu'))
    m.add(keras.layers.Dense(10, activation = 'softmax'))
    m.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')
    
    hy = m.fit(t_x, t_y, epochs = 20, validation_data = (v_x, v_y))
    
    plt.plot(hy.history['loss'])
    plt.plot(hy.history['val_loss'])
    ```

    ![j11](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/j11.png?raw=true)

    - 과도한 뉴런의 수로 과대적합이 발생하는 것을 볼 수 있다.

    ```python
    m = keras.Sequential()
    m.add(keras.layers.Flatten(input_shape = (28,28)))#784
    m.add(keras.layers.Dense(100,activation = 'relu'))#100
    m.add(keras.layers.Dropout(0.3))  # 입력 데이터의 노드 30퍼센트를 무작위로 0으로 만듦
    m.add(keras.layers.Dense(300,activation = 'relu'))#300
    m.add(keras.layers.Dense(10,activation = 'softmax'))
    m.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')
    
    hy = m.fit(t_x, t_y, epochs = 20, validation_data = (v_x, v_y))
    
    plt.plot(hy.history['loss'])
    plt.plot(hy.history['val_loss'])
    ```

    ![j12](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/j12.png?raw=true)

    - dropout으로 은닉층1 에서 은닉층2로 가는 입력 데이터의 노드 30퍼센트를 무작위로 0으로 만든다.
    - dropout으로 과도한 학습을 방지하여 과대적합되지 않도록 한다.

    ```python
    m.evaluate(t_x, t_y)  # 0.23151524364948273
    ```

    <br/>

- 모델 저장

  - 학습한 모델의 가중치는 프로그램을 종료하면 사라진다. 다시 사용하려면 모델의 학습을 다시 해야한다는 뜻.

  - save_weights로 모든 가중치를 저장할 수 있고 save로 모델 자체를 저장할 수도 있다.

  - 저장한 가중치나 모델은 각각 load_weights나 load_model로 불러올 수 있다.

    ```python
    m.save_weights('m_1_w.h5')
    m.save('m1.h5')
    ```

    - 위 코드에서 만든 모델의 가중치와 모델을 저장

    ```python
    m1 = keras.Sequential()
    m1.add(keras.layers.Flatten(input_shape = (28,28)))
    m1.add(keras.layers.Dense(100,activation = 'relu'))
    m1.add(keras.layers.Dropout(0.3))
    m1.add(keras.layers.Dense(300,activation = 'relu'))
    m1.add(keras.layers.Dense(10,activation = 'softmax'))
    m1.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')
    
    m1.load_weights('m_1_w.h5')
    ```

    ```python
    m1.evaluate(t_x, t_y)  # 0.23151524364948273
    ```

    - 로드해온 가중치를 m1에 적용시키면 m의 스코어와 동일한 것을 볼 수 있다.

    ```python
    m2 = keras.models.load_model('m1.h5')
    m2.evaluate(t_x, t_y)  # 0.23151524364948273
    ```

    - 로드해온 모델도 동일

  <br/>

- 콜백함수 CheckPoint와 EarlyStopping

  - fit메소드의 매개변수로 callbacks에 콜백함수를 구조로 전달해준다.

  - CheckPoint는 학습 중에 가장 결과값을 좋을 때의 모델을 저장해놓는다.

    ```python
    import tensorflow as tf
    from tensorflow import keras
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    
    (t_x,t_y),(tt_x,tt_y) = keras.datasets.fashion_mnist.load_data()
    s_t_x = t_x / 255.0
    s_tt_x = tt_x / 255.0
    
    from sklearn.model_selection import train_test_split
    t_x, v_x, t_y, v_y = train_test_split(s_t_x, t_y, test_size = 0.2, random_state = 42)
    t_x.shape[1:]  # (28, 28)
    ```

    ```python
    m = keras.Sequential()
    m.add(keras.layers.Flatten(input_shape = (28, 28)))
    m.add(keras.layers.Dense(100, activation = 'relu'))
    m.add(keras.layers.Dropout(0.3))
    m.add(keras.layers.Dense(10, activation = 'softmax'))
    m.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')
    ck_p = keras.callbacks.ModelCheckpoint('best_m.h5', save_best_only = True)
    hy = m.fit(t_x, t_y, epochs = 20, validation_data = (v_x, v_y), callbacks = [ck_p]) 
    plt.plot(hy.history['loss'])
    plt.plot(hy.history['val_loss'])
    ```

    ![j13](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/j13.png?raw=true)

    ```python
    m.evaluate(v_x, v_y)  # [0.32464003562927246, 0.8845000267028809]
    ```

    - 모델 m의 각각 손실률과 정확도이다.

    ```python
    m2 = keras.models.load_model('best_m.h5')
    m2.evaluate(v_x, v_y)  # [0.31579387187957764, 0.8859999775886536]
    ```

    - CheckPoint를 통해 가장 성능이 좋을 때의 모델을 저장했고, 그 저장한 모델을 m2에 적용했다.
    - m2의 손실률과 정확도를 보면 m보다 성능이 좋다는 것을 알 수 있다.

  - EarlyStopping은 설정한 값 만큼 평가점수 그래프가 발산하면 학습을 멈춘다. 즉, 과대적합을 예방하는 역할을 한다.

    ```python
    def my_m(l = None):
        m = keras.Sequential()
        m.add(keras.layers.Flatten(input_shape = (28, 28)))
        if l:
            for i in l:
                m.add(i)
        m.add(keras.layers.Dense(10, activation = 'softmax'))
        m.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')
        return m
    ```

    ```python
    m3 = my_m([
        keras.layers.Dense(100, activation = 'relu'), 
        keras.layers.Dropout(0.3)
    ])
    
    ck_p = keras.callbacks.ModelCheckpoint('best1_m.h5', save_best_only = True)
    e_st = keras.callbacks.EarlyStopping(patience = 3)
    m3.fit(t_x, t_y, epochs = 20, validation_data = (v_x, v_y), callbacks = [ck_p, e_st])
    ```

    - epoch를 20으로 설정했지만 코드를 실행하면 19번째 epoch에서 학습이 중단되는 것을 확인할 수 있다.

    ```python
    m3.evaluate(v_x, v_y)  # [0.3221418559551239, 0.8826666474342346]
    ```

    - 성능보다는 과대적합을 방지하는데에 더 의의가 있다.

  - 섞어서 사용

    ```python
    m4 = my_m([
        keras.layers.Dense(100, activation = 'relu'), 
        keras.layers.Dropout(0.3)
    ])
    
    ck_p = keras.callbacks.ModelCheckpoint('best2_m.h5', save_best_only = True)
    e_st = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
    m4.fit(t_x, t_y, epochs = 20, validation_data = (v_x, v_y), callbacks = [ck_p, e_st])
    ```

    ```python
    m4.evaluate(v_x, v_y)  # [0.31886282563209534, 0.8843333125114441]
    ```

    - 두 콜백함수를 모두 사용하면 EarlyStopping으로 인해 조기 종료되고 그 동안 가장 성능이 좋았던 가중치를 가진 모델을 CheckPoint가 저장해놓는다.

<br/>

---

# 컨볼루션 신경망 (CNN)

<br/>

- CNN

  - CNN은 생물체의 영상 처리 구조에서 힌트를 얻어서 만들어졌다.

    ![i1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/i1.png?raw=true)

  - 여러가지의 특징을 가지고 물체를 분류한다.

    ![i2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/i2.png?raw=true)

  - 컨볼루션 연산을 통해 특징들을 뽑아내고 풀링을 통해 차원을 줄인다(필요없는 피쳐 제거). 이 과정을 반복

    ![i3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/i3.png?raw=true)

  - CNN 신경망에서는 하위 레이어의 노드들과 상위 레이어의 노드들이 부분적으로만 연결되어 있다.

    - 배깅과 비슷한 부분

    ![i4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/i4.png?raw=true)

  - 입력층

    - 입력층에서 컨볼루션 연산을 통하여 특징을 뽑아내는 특징맵이 존재
    - 그 후 풀링 연산을 진행
    - 컨볼루션 레이어와 풀링 레이어는 여러 번 되풀이 된다.
    - 신경망의 끝에는 완전히 연결된 구조의 전체적인 분류 신경망이 있어서 추출된 특징을 바탕으로 물체를 인식한다.

  - 컨볼루션

    - 주변 화소값들에 가중치를 곱해서 더한 후에 이것을 새로운 화소값으로 하는 연산

    ![i6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/i6.png?raw=true)

    - 여러개의 필터를 사용할 수 있다. 필터의 값은 미리 정해진 것이 아니고 학습된다.

      ![i9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/i9.png?raw=true)

      - 필터가 많을 수록 많은 피쳐가 추출되기 때문에 풀링이 필수적이다.

  - 풀링

    - 서브 샘플링이라고도 함

    - 입력 데이터의 크기를 줄인다. 중요한 데이터만 추출해 내는 역할을 한다.

      ![i7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/i7.png?raw=true)

      <br/>

      ![i8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/i8.png?raw=true)

    - 풀링의 장점

      - 레이어의 크기가 작아지므로 계산이 빨라진다
      - 신경망의 매개변수가 작아지면서 과적합이 나올 가능성이 줄어든다.
      - 공간에서 물체의 이동이 있어도 결과는 변하지 않는다. 고양이를 인식하려고 할 때 고양이가 이미지의 어떤 곳으로 이동해도 고양이를 인식할 수 있다.

  - 간단한 이미지

    ![i10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/i10.png?raw=true)

  - 파이썬 코드

    ```python
    from tensorflow import keras
    from sklearn.model_selection import train_test_split
    from keras.datasets.fashion_mnist import load_data
    
    (x_data, y_data), (t_x_data, t_y_data) = load_data()
    x_data = x_data.reshape(-1, 28, 28, 1) / 255.0
    
    t_x, v_x, t_y, v_y = train_test_split(x_data, y_data, test_size = 0.2, random_state = 42)
    ```

    ```python
    m = keras.Sequential()
    m.add(keras.layers.Conv2D(32, kernel_size = 3, activation = 'relu', padding = 'same', input_shape = (28, 28, 1)))
    m.add(keras.layers.MaxPooling2D(2))
    m.add(keras.layers.Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))
    m.add(keras.layers.MaxPooling2D(2))
    ```

    - 레이어를 쌓는 방법은 동일.
    - 차원에 맞는 컨볼루션과 풀링을 설정해주면 된다.

    ```python
    m.add(keras.layers.Flatten())
    m.add(keras.layers.Dense(100, activation = 'relu'))
    m.add(keras.layers.Dropout(0.4))
    m.add(keras.layers.Dense(10, activation = 'softmax'))
    m.summary()
    ```

    ![j14](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2009/j14.png?raw=true)

    - 컨볼루션 레이어와 풀링 레이어를 설정해줬으면 그 후에는 딥러닝과 동일하다.

    ```python
    m.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')
    ck_p = keras.callbacks.ModelCheckpoint('best_m.h5', save_best_only = True)
    e_st = keras.callbacks.EarlyStopping(patience = 2)
    m.fit(t_x, t_y, epochs = 20, validation_data = (v_x, v_y), callbacks = [ck_p, e_st])
    ```

    ```python
    m.evaluate(v_x, v_y)  # [0.2320510298013687, 0.9192500114440918]
    ```