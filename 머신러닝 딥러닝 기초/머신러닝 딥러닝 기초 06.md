# 머신러닝 딥러닝 기초 06

<br/>

### 의사결정트리

<br/>

- 의사결정트리

  - 어떤 규칙을 하나의 트리 형태로 표현한 후 이를 바탕으로 분류나 회귀 문제를 해결
  - 규칙은 if-else 문으로 표현이 가능
  - 트리 구조의 리프 노드에는 분류 문제에서 클래스, 회귀 문제에서는 예측치가 들어감
  - 딥러닝 기반을 제외한 전통적인 통계 기반의 머신러닝 모델 중 효과와 실용성이 가장 좋음

- 의사결정트리 분류기

  - 의사결정트리의 노드 구성이 가장 중요
  - 마지막 노드에 클래스나 예측치를 기입하고 상위의 부모 노드들에게는 if-else 문의 조건에 해당하는 정보 기입
  - 분할 속성
    - 부모 노드에 들어가는 if-else 문의 조건들
    - 어떤 분할 속성이 가장 모호성을 줄일 것인지 파악
  - 재귀적 지역 최적화 방법
    - 첫 문제로 분할 속성을 설정하고, 그 다음 남은 데이터 속에서 최적의 분할 속성을 찾아냄

- 엔트로피

  - 어떤 목적 달성을 위한 경우의 수를 정량적으로 표현하는 수치
  - 현재의 정보 제공 상태를 측정
  - 어떤 분할 속성을 선택하였을 때 정보를 제공하는 기준 값을 정하고, 그 값을 최소화 또는 최대화하는 방향으로 알고리즘 실행
  - 낮은 엔트로피 = 경우의 수가 적음 = 낮은 불확실성
  - 높은 엔트로피 = 경우의 수가 많음 = 높은 불확실성

- 엔트로피의 측정법

  - 샤논 공식을 사용

    ![m1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m1.png?raw=true)

    ![i1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i1.png?raw=true)

- 정보 이득

  - 엔트로피를 사용하여 속성별 분류 시 데이터가 얼마나 순수한지를 측정하는 지표
  - 각 속성을 기준으로 데이터를 분류했을 때 엔트로피를 측정

- 정리

  - 전체 엔트로피

    ![m2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m2.png?raw=true)

  - 속성별 엔트로피

    ![m3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m3.png?raw=true)

    - 속성 A로 데이터를 분류했을 때 속성 A가 가진 모든 클래스의 각 엔트로피를 계산한 후, 데이터의 개수만큼 가중치를 줌

  - 속성별 정보 이득

    ![m4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m4.png?raw=true)

    - 정보 이득이 크면 클수록 A를 기준으로 데이터를 분류했을 때 얻을 수 있는 정보량이 많다는 뜻
    - A를 기준으로 데이터를 나눌 때 엔트로피가 작다면 해당 속성을 기준으로 데이터를 나누기 좋다고 볼 수 있음

- 파이썬 코드로 확인

  ```python
  import pandas as pd
  import numpy as np
  
  data = pd.read_csv('day6_data2.csv')
  ```

  ![j1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/j1.png?raw=true)

  ```python
  # 엔트로피를 구하는 함수
  def get_info(df):
      buy = df.loc[df['class_buys_computer'] == 'yes']
      not_buy = df.loc[df['class_buys_computer'] == 'no']
      x = np.array([len(buy) / len(df), len(not_buy) / len(df)])
      y = np.log2(x[x != 0])
      info_all = -sum(x[x != 0] * y)
      return info_all
  
  get_info(data)  # 0.9402859586706311  # 전체 엔트로피
  ```

  ```python
  youth = data.loc[data['age'] == 'youth']
  middle_aged = data.loc[data['age'] == 'middle_aged']
  senior = data.loc[data['age'] == 'senior']
  
  get_info(youth)  # 0.9709505944546686  # youth 속성 엔트로피
  
  get_info(middle_aged)  # -0.0  # middle_aged 속성 엔트로피
  
  get_info(senior)  # 0.9709505944546686  # senior 속성 엔트로피
  ```

- ID3 알고리즘을 활용한 의사결정트리의 성장

  - 성장

    - 일반적으로 의사결정트리를 생성하는 방법을 성장이라고 부름

  - ID3

    - 반복적으로 데이터를 나누는 알고리즘
    - 톱-다운 방식으로 데이터를 나누면서 탐욕적으로 현재 상태에서 최적화를 추진하는 방법을 선택

  - 기본적인 알고리즘

    - 위 예제에서 사용한 데이터를 예제로 설명

    - 모든 데이터가 동일한 클래스가 아님

    - 최적 분류 기준이 되는 속성을 선정하기 위해, 정보 이득을 기반으로 속성별 데이터 분류의 기준을 정함

      ![i2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i2.png?raw=true)

    - 각 속성들의 정보 이득과 가지 속성

      ![i3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i3.png?raw=true)

    - ID3 알고리즘의 순서에 따라 가장 많은 정보를 주는 age 속성을 첫 번째 가지 속성이라고 함

    - age 속성 기준으로 데이터를 나누어 새로운 트리 생성

      ![i4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i4.png?raw=true)

    - youth와 senior는 3:2 비율로 컴퓨터 구매 여부가 나누어짐

    - middle_aged의 경우 모두 컴퓨터를 구입한다는 데이터이므로 더 이상 데이터를 분류할 필요가 없음

    - age가 youth로 분류된 5개의 데이터에 대한 정보 이득

      ![i5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i5.png?raw=true)

    - student를 기준으로 데이터를 분리했을 때 가장 많은 정보를 획득함

      ![i6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i6.png?raw=true)

    - 이후에는 분류할 필요가 없기 때문에

      ![i7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i7.png?raw=true)

- 의사결정트리 알고리즘의 특징

  - 재귀적 작동
    - 가지가 되는 속성을 선택한 후 해당 가지로 데이터를 나누면, 이전에 작용되었던 알고리즘이 남은 데이터에 적용됨
    - 남은 데이터에서만 최적의 모델을 찾는 방법으로 작동
  - 속성 기준으로 가지치기 수행
    - 가장 불확실성이 적은 속성을 기준으로 가지치기를 수행
  - 중요한 속성 정보 제공
    - 처음 분리 대상이 되는 속성이 가장 중요한 속성
    - 이 특징을 '해석 가능한 머신러닝'이라고 부름

- 의사결정트리의 장점

  - 불필요한 속성 값에 대한 스케일링
    - 전처리 단계 없이 바로 사용할 수 있다.
  - 강건한 이상치
    - 관측치의 절대값이 아닌 순서가 중요하기 때문에 필요 이상으로 큰 값이나 작은 값에 대해서도 분류 성능이 크게 떨어지지 않는다.
  - 자동적인 변수 선택
    - 알고리즘에 의해 중요한 변수들이 우선적으로 선택되어 조금 더 쉽게 중요한 속성을 확인할 수 있다. 의사결정트리 계열의 알고리즘이 가지고 있는 가장 큰 장점 중 하나

- 파이썬 코드로 확인(일부만)

  - 위 파이썬 코드와 동일한 데이터 사용

  ```python
  data['age'].unique()  # array(['youth', 'middle_aged', 'senior'], dtype=object)
  # 클래스만 뽑아서 출력
  ```

  ```python
  # 속성별 엔트로피를 계산하는 함수
  def get_attribute_info(df, attribute_name):
      att_v = data[attribute_name].unique()  # 클래스 이름 가져오기
      get_infos = []
      
      for i in att_v:
          split_df = df.loc[df[attribute_name] == i]
          get_infos.append((len(split_df) / len(df)) * get_info(split_df))
      return sum(get_infos)
  ```

  ```python
  # 각 속성별 정보 이득
  get_info(data) - get_attribute_info(data, 'age')  # 0.24674981977443933
  
  get_info(data) - get_attribute_info(data, 'income')  # 0.02922256565895487
  
  get_info(data) - get_attribute_info(data, 'student')  # 0.15183550136234159
  
  get_info(data) - get_attribute_info(data, 'credit_rating')  # 0.04812703040826949
  ```

  ```python
  # age가 youth로 분류된 5개의 데이터에 대한 정보 이득
  youth = data.loc[data['age']=='youth']
  
  get_info(youth) - get_attribute_info(youth,'income')  # -1.580026905978025
  
  get_info(youth) - get_attribute_info(youth,'student')  # -1.2367106860085422
  
  get_info(youth) - get_attribute_info(youth,'credit_rating')  # -1.527094404679944
  ```

- 정보 이득의 문제점

  - 수식의 특성상 속성의 값이 다양할수록 선택의 확률이 높아지는 문제가 발생. 즉, 중요한 속성이 티가 안나게 됨

    ![m3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m3.png?raw=true)

  - 데이터가 매우 많고 속성이 다양할 때 위 수식의 |Dj|/|D|의 값이 작아짐

  - 해당 속성의 엔트로피가 낮아져 단순히 속성 안에 있는 값의 종류를 늘리는 것만으로 정보 이득이 높아짐

- C4.5 알고리즘

  - 정보 이득을 측정하는 방식을 좀 더 평준화시켜 단순한 정보 값을 대신 사용

  - 기존 정보 이득의 분모에 평준화 함수 SplitInfo 추가

    ![m5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m5.png?raw=true)

    <br/>

    ![m6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m6.png?raw=true)

  - 정교한 불순도 지표 활용

  - 범주형 변수 뿐 아니라 연속형 변수를 사용 가능

  - 결측치가 포함된 데이터도 사용 가능

  - 과적합을 방지하기 위한 가지치기

  - 클래스가 많을수록 |Dj|/|D|값이 작아지고 -log(|Dj|/|D|)값은 커져 정규화됨

    ![i8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i8.png?raw=true)

  - SplitInfo 함수 값이 분모에 들어가면서 클래스 불균형에 의해 생기는 불합리한 속성 분류를 보정

- 지니 지수

  - 경제학에서 소득의 불평등도를 측정할 때 사용하는 지표

  - 의사결정트리에서 각 속성의 불순도를 측정하는 방법으로 사용

    ![i9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i9.png?raw=true)

- 이진 분할

  - CART 알고리즘의 핵심은 불확실성을 추정하는 기준 값이 엔트로피에서 지니 지수로 바뀐 것

  - 구현 측면에서 가장 큰 차이점은 이진 분할을 실시한다는 것

  - k가 속성 내에 있는 데이터의 개수일 때, 2^k-1 - 1 개의 분할이 생성됨

  - 각 속성별 지니 지수 정보

    ![m7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m7.png?raw=true)

  - 예제

    ![i10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i10.png?raw=true)

    <br/>

    ![i11](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i11.png?raw=true)

    - age, credit, income, student의 4가지 속성들을 정의

      ![m8](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m8.png?raw=true)

    - age 속성에 3가지 클래스가 존재하기 때문에 3가지 종류의 이진 분할 경우의 수로 나눌 수 있음

      ![m9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m9.png?raw=true)

    - Age -> youth O

      ![i12](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i12.png?raw=true)

    - Age -> youth X

      ![i13](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i13.png?raw=true)

    - age_1, age_2, age_3을 각각 연산하면 0.393, 0.357, 0.457의 값이 나옴

    - age 속성의 지니 지수는 이 중 가장 작은 값인 0.357

    - 각 속성들의 지니 지수

      ![m10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m10.png?raw=true)

- 트리 가지치기

  - 클래스의 마지막 노드인 리프 노드의 개수를 개발자가 직접 결정

    - 1개로 이루어진 리프 노드가 많을 경우 과대적합되어 있는 상태
    - 리프 노드의 개수와 관계 없이 해당 가지에 불확실성이 너무 높을 경우 의사결정트리의 성능에 문제를 줄 수 있음

  - 트리 가지치기

    - 의사결정트리의 마지막 노드의 개수를 지정하여 트리의 깊이를 조정하는 방법

  - 사전 가지치기

    - 처음 트리를 만들 때 트리의 깊이나 마지막 노드의 최소 개수 등을 사전에 결정하여 입력
    - 데이터 분석가가 하이퍼 매개변수로 모든 값을 입력해야 하는 점이 어려움
    - 계산 효율이 좋고 작은 데이터셋에서도 쉽게 작동
    - 사용자가 중요한 속성 값을 놓치거나 과소적합 문제가 발생할 수 있다.

  - 사후 가지치기

    - 트리를 먼저 생성한 후 실험적으로 하이퍼 매개변수를 조정
    - 하나의 지표를 정해두고 실험적으로 다양한 하이퍼 매개변수를 조정하며 최적의 값을 찾음
    - '최종 노드의 개수', '트리의 깊이' 또는 '선택되는 속성의 개수' 등을 하이퍼 매개변수로 보고 조정하며 성능을 비교
    - 먼저 전체 데이터를 훈련셋, 검증셋, 테스트셋으로 분류하고, 훈련셋과 테스트셋의 성능을 비교

  - 교차 검증

    ```python
    import pandas as pd
    import numpy as np
    
    data = pd.read_csv('day6_data1.csv')
    ```

    ![j2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/j2.png?raw=true)

    ```python
    X = data[['alcohol', 'sugar', 'pH']].to_numpy()
    Y = data['class'].to_numpy()
    ```

    ```python
    from sklearn.model_selection import train_test_split
    
    t_x, tt_x, t_y, tt_y = train_test_split(X, Y, random_state= 42, test_size= 0.2)  # 검증 데이터 분류
    ```

    ```python
    s_t_x, v_t_x, s_t_y, v_t_y = train_test_split(t_x, t_y, random_state= 42, test_size= 0.2)  # 학습, 테스트 데이터 분류
    ```

    - 첫번째로 분류한 데이터에서 한 그룹을 다시 분류하여 학습, 테스트, 검증 데이터로 각각 분류한다.

    ```python
    from sklearn.tree import DecisionTreeClassifier
    
    dt = DecisionTreeClassifier(random_state= 42)
    dt.fit(s_t_x, s_t_y)
    dt.score(s_t_x, s_t_y), dt.score(v_t_x, v_t_y)
    # (0.9971133028626413, 0.864423076923077)
    ```

    ```python
    # 교차 검증
    from sklearn.model_selection import cross_validate
    
    sc = cross_validate(dt, t_x, t_y)
    ```

    ![j3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/j3.png?raw=true)

    - cross_validate로 다중 평가 지표를 확인할 수 있다.

    ```python
    np.mean(sc['test_score'])  # 0.855300214703487
    ```

    ```python
    from sklearn.model_selection import StratifiedKFold
    
    sc1 = cross_validate(dt, t_x, t_y, cv=StratifiedKFold())
    np.mean(sc1['test_score'])  # 0.855300214703487
    ```

    - cross_validate는 교차 검증에 필요한 데이터를 분류할 때 데이터가 치우쳐지게 분류될 수 있기 때문에 StratifiedKFold를 사용

    ```python
    # 10폴드 교차 검증
    sc_ck = StratifiedKFold(n_splits= 10, shuffle= True, random_state= 42)
    sc2 = cross_validate(dt, t_x, t_y, cv=sc_ck)
    np.mean(sc2['test_score'])  # 0.8574181117533719
    ```

    - 그리드 서치

    ```python
    from sklearn.model_selection import GridSearchCV
    
    params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
    gs = GridSearchCV(DecisionTreeClassifier(random_state= 42), params)
    gs.fit(t_x, t_y)
    ```

    - GridSearchCV로 사전 가지치기가 가능.
    - 미리 파리미터값을 정해놓고 모델과 파라미터값을 지정해준 뒤 모델을 생성

    ```python
    dt = gs.best_estimator_
    dt.score(t_x, t_y), dt.score(tt_x, tt_y)
    # (0.9615162593804117, 0.8653846153846154)
    ```

    ```python
    gs.best_params_  # {'min_impurity_decrease': 0.0001}
    ```

    - 지정해준 파라미터 중에서 가장 좋은 결과를 낸 값을 찾아준다.

    ```python
    gs.cv_results_['mean_test_score']
    # array([0.86819297, 0.86453617, 0.86492226, 0.86780891, 0.86761605])
    ```

    - 각 파라미터 값들로 테스트 했을 때 스코어들을 평균한 값들을 나타내준다.

    ```python
    i = np.argmax(gs.cv_results_['mean_test_score'])
    gs.cv_results_['params'][i]
    # {'min_impurity_decrease': 0.0001}
    ```

    - 위 방법으로도 출력 가능

    ```python
    params = {'max_depth': range(5, 20, 1),
              'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
              'min_samples_split': range(2, 100, 10)
    }
    
    gs1 = GridSearchCV(DecisionTreeClassifier(random_state= 42), params)
    gs1.fit(t_x, t_y)
    ```

    - 연속적인 값들을 파라미터로 다양아게 지정

    ```python
    gs1.best_params_
    # {'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}
    ```

    ```python
    np.max(gs1.cv_results_['mean_test_score'])  # 0.8683865773302731
    ```

    - 랜덤서치

    ```python
    from scipy.stats import uniform, randint
    
    np.unique(d.rvs(1000), return_counts= True)
    
    #(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
    # array([ 86, 115,  96, 102, 103, 111,  92,  91, 112,  92], dtype=int64))
    ```

    - rvs로 각 숫자가 몇 개나 나왔는지 확인할 수 있음

    ```python
    params = {'max_depth': randint(20, 50),
              'min_impurity_decrease': uniform(0.0001, 0.001),
              'min_samples_split': randint(2, 25),
              'min_samples_leaf': randint(1, 25) 
    }
    
    from sklearn.model_selection import RandomizedSearchCV
    
    rs = RandomizedSearchCV(DecisionTreeClassifier(random_state= 42), params, n_iter= 100, random_state= 42)
    rs.fit(t_x, t_y)
    ```

    - 랜덤서치로 파라미터 값을 랜덤수치로 주어 모델과 파라미터와 함께 학습

    ```python
    rs.best_params_
    ```

    ![j4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/j4.png?raw=true)

    - 랜덤서치로 사후 가지치기가 가능
    - 랜덤값을 파라미터로 준 뒤 결과값으로 나온 파라미터들을 모델에 적용

  - 교차 검증 예시

    ```python
    import pandas as pd
    import numpy as np
    train_df = pd.read_csv("train.csv")
    test_df = pd.read_csv("test.csv")
    
    train_id = train_df["PassengerId"].values
    test_id = test_df["PassengerId"].values
    
    all_df = train_df.append(test_df).set_index('PassengerId')
    all_df["Sex"] = all_df["Sex"].replace({"male":0,"female":1})
    
    all_df["Age"].fillna(
        all_df.groupby("Pclass")["Age"].transform("mean"), inplace=True)
    all_df["cabin_count"] = all_df["Cabin"].map(
             lambda x : len(x.split()) if type(x) == str else 0)
    def transform_status(x):
        if "Mrs" in x or "Ms" in x:
            return "Mrs"
        elif "Mr" in x:
            return "Mr"
        elif "Miss" in x:
            return "Miss"
        elif "Master" in x:
            return "Master"
        elif "Dr" in x:
            return "Dr"
        elif "Rev" in x:
            return "Rev"
        elif "Col" in x:
            return "Col"
        else:
            return "0"
    
    all_df["social_status"] = all_df["Name"].map(lambda x : transform_status(x))
    all_df["social_status"].value_counts()
    #all_df[all_df["Embarked"].isnull()]
    all_df = all_df.drop([62,830])
    train_id =np.delete(train_id, [62-1,830-1])
    #all_df[all_df["Fare"].isnull()]
    all_df.groupby(["Pclass","Sex"])["Fare"].mean()
    all_df.loc[all_df["Fare"].isnull(), "Fare"] = 12.415462
    all_df["cabin_type"] = all_df["Cabin"].map(lambda x : x[0] if type(x) == str else "99")
    del all_df["Cabin"]
    del all_df["Name"]
    del all_df["Ticket"]
    y = all_df.loc[train_id, "Survived"].values
    del all_df["Survived"]
    X_df = pd.get_dummies(all_df)
    X = X_df.values
    from sklearn.preprocessing import MinMaxScaler
    mm=MinMaxScaler()
    X=mm.fit_transform(X)
    ```

    - 타이타닉 데이터셋을 사용. 데이터 전처리 과정

    ```python
    t_x=X[:len(train_id)]
    tt_x=X[len(train_id):]
    
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.model_selection import cross_val_score
    from sklearn.metrics import accuracy_score
    
    t_l = []  # 오차율 기록
    tt_l = []
    
    for i in range(3, 20):
        dt = DecisionTreeClassifier(min_samples_leaf= i)
        acc = cross_val_score(dt, t_x, y, scoring= 'accuracy', cv= 5).mean()
        t_l.append(accuracy_score(dt.fit(t_x, y).predict(t_x), y))
        tt_l.append(acc)
        
    r = pd.DataFrame(t_l, index= range(3, 20), columns=['train'])
    r['test'] = tt_l
    r.plot()
    ```

    ![j5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/j5.png?raw=true)

    - 10~12 사이의 값에서 테스트 스코어가 가장 좋은 것을 볼 수 있음

    <br/>

- 연속형 변수 나누기

  ![i14](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i14.png?raw=true)

  - 연속형 데이터를 나누는 기준

    - 모든 데이터를 기준점으로 하여 데이터를 나누기
      - 너무 많은 기준점이 생겨 과대적합 문제가 발생하거나 분류의 정확도가 떨어짐
    - 통계적 수치로 중위값이나 4분위수를 기준점으로 나누기
      - 25%씩 데이터를 나눠서 분류 기준을 변경
      - 과소적합 문제가 발생하여 분류의 성능을 떨어뜨릴 수 있음
    - 가장 많이 쓰는 방법으로 Y 클래스의 값을 기준으로 해당 값이 변할 때를 기준점으로 삼아 분기

  - 데이터 중 분류 대상이 되는 기준점 찾기

    ![i15](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i15.png?raw=true)

  - 데이터를 자르는 기준값 정하기

    - 구간별 경계 평균값

    ![i16](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i16.png?raw=true)

  - 구간별 경계값을 기준으로 엔트로피를 산출

    - 4개의 기준점 각각의 정보 이득을 구했을 때 가장 큰 값이 ELEVATION의 대표 정보 이득이 되어 다른 속성값 정보 이득과 비교하여 최종적으로 분기가 일어나는 속성으로 선택됨

    ![m11](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m11.png?raw=true)

  - 위 계산 결과는 각각 0.3059, 0.1813, 0.5916, 0.8631이고 STREAM, SLOPE 속성의 정보 이득은 각각 0.3059, 0.5774로 가장 먼저 ELEVATION이 4175인 값을 기준으로 트리를 분기

    ![i17](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i17.png?raw=true)

- 회귀 트리

  - Y 데이터의 값이 연속형일 때의 의사결정트리 생성 방법

    ![i18](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i18.png?raw=true)

  - Y 값의 각 속성별 분산이 얼마나 작은지를 측정

  - 최종 결과 노드에서는 결과 노드들의 Y 평균값으로 최종 예상치를 반환

    ![m12](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m12.png?raw=true)

  - 속성별 분산 구하기 : 각 클래스값들의 분산을 구하고 해당 클래스가 가진 데이터 개수의 비율만큼 곱함

    ![m13](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/m13.png?raw=true)

    <br/>

    ![i19](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i19.png?raw=true)

  - SEASON 속성이 WORK_DAY에 비해서 분산이 낮으므로 해당 값으로 분기가 일어남

    ![i20](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/i20.png?raw=true)

  - 각 분기별로는 WORK_DAY로 데이터를 나누고, 나누어진 데이터는 평균값으로 최종 예측값이 반환됨

<br/>

---

### 랜덤 포레스트, 앙상블 맛보기

<br/>

```python
import pandas as pd
import numpy as np

data = pd.read_csv('day6_data1.csv')
```

![j2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2006/j2.png?raw=true)

```python
X = data[['alcohol', 'sugar', 'pH']].to_numpy()
Y = data['class'].to_numpy()

from sklearn.model_selection import train_test_split
t_x, tt_x, t_y, tt_y = train_test_split(X, Y, random_state= 42, test_size= 0.2)

from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier  # 랜덤 포레스트 분류
rf = RandomForestClassifier(random_state= 42)
sc = cross_validate(rf, t_x, t_y, return_train_score= True)
np.mean(sc['train_score']), np.mean(sc['test_score'])
# (0.9973541965122431, 0.8905151032797809)
```

```python
rf.fit(t_x, t_y)
rf.feature_importances_
# array([0.23167441, 0.50039841, 0.26792718])
```

```python
rf.score(t_x, t_y) , rf.score(tt_x, tt_y)
# (0.996921300750433, 0.8892307692307693)
```

```python
rf1 = RandomForestClassifier(oob_score= True, random_state= 42)
rf1.fit(t_x, t_y)

rf1.oob_score_  # 0.8934000384837406
```

```python
from sklearn.ensemble import ExtraTreesClassifier  # ExtraTrees 분류
et = ExtraTreesClassifier(random_state= 42)

sc = cross_validate(et, t_x, t_y, return_train_score= True)
np.mean(sc['train_score']), np.mean(sc['test_score'])
# (0.9974503966084433, 0.8887848893166506)
```

```python
et.fit(t_x, t_y)
rf.feature_importances_, et.feature_importances_
# (array([0.23167441, 0.50039841, 0.26792718]),
#  array([0.20183568, 0.52242907, 0.27573525]))
```

```python
from sklearn.ensemble import GradientBoostingClassifier  # 앙상블
gb = GradientBoostingClassifier(random_state= 42)

sc = cross_validate(gb, t_x, t_y, return_train_score= True)
np.mean(sc['train_score']), np.mean(sc['test_score'])
# (0.8881086892152563, 0.8720430147331015)
```

```python
gb = GradientBoostingClassifier(random_state= 42, n_estimators= 500, learning_rate= 0.2)

sc = cross_validate(gb, t_x, t_y, return_train_score= True)
np.mean(sc['train_score']), np.mean(sc['test_score'])
# (0.9464595437171814, 0.8780082549788999)
```

