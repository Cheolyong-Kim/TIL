# 머신러닝 딥러닝 기초 10

<br/>

- CNN

  - 피쳐맵 확인

    - 컨볼루션 층을 통과한 피쳐맵을 시각화하여 확인해본다.
    - 모델은 이전 09 문서에서 만든 모델을 불러와서 사용한다.

    ```python
    import numpy as np
    import tensorflow as tf
    import matplotlib.pyplot as plt
    from tensorflow import keras
    from sklearn.model_selection import train_test_split
    np.random.seed(42)
    tf.random.set_seed(42)
    
    m = keras.models.load_model('best-cnn-model.h5')
    ```

    ```python
    m.layers
    ```

    ![j30](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j30.png?raw=true)

    - layers로 모델의 레이어층에 대한 정보를 확인할 수 있다.

    ```python
    conv = m.layers[0]
    conv.weights[0].shape, conv.weights[1].shape
    
    # (TensorShape([3, 3, 1, 32]), TensorShape([32]))
    ```

    - ``layers[0]``으로 첫 컨볼루션층을 지정하여 정보들을 확인할 수 있다. 

    ```python
    w = conv.weights[0].numpy()
    w.mean(), w.std()
    
    # (-0.025312617, 0.25047907)
    ```

    - 컨볼루션 1층의 가중치의 평균과 분산값이다.

    ```python
    fig, axs = plt.subplots(2, 16, figsize = (15, 2))
    
    for i in range(2):
        for j in range(16):
            axs[i, j].imshow(w[:, :, 0, i * 16 + j], vmin = -0.5, vmax = 0.5)
            axs[i, j].axis('off')
    ```

    ![j1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j1.png?raw=true)

    - 컨볼루션 1층의 피쳐맵들을 시각화한 이미지들이다. 특징이 강할 수록 색이 밝다

    ```python
    n_m = keras.Sequential()
    n_m.add(keras.layers.Conv2D(32, kernel_size = 3, activation = 'relu', padding = 'same', input_shape = (28, 28, 1)))
    
    n_conv = n_m.layers[0]
    n_w = n_conv.weights[0].numpy()
    
    fig, axs = plt.subplots(2, 16, figsize = (15, 2))
    
    for i in range(2):
        for j in range(16):
            axs[i, j].imshow(n_w[:, :, 0, i * 16 + j], vmin = -0.5, vmax = 0.5)
            axs[i, j].axis('off')
    ```

    ![j2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j2.png?raw=true)

    - 학습하지 않은 모델의 피쳐맵을 시각화한 이미지이다. 특징점이 없는 것을 볼 수 있다.

    ```python
    plt.hist(w.reshape(-1, 1))
    plt.show()
    
    plt.hist(n_w.reshape(-1, 1))
    plt.show()
    ```

    ![j3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j3.png?raw=true)

    ![j4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j4.png?raw=true)

    - 두 피쳐맵을 히스토그램으로 표현한 것이다. 학습이 된 모델과 그렇지 않은 모델 간의 차이가 명확히 보인다.

    ```python
    m.input
    
    # <KerasTensor: shape=(None, 28, 28, 1) dtype=float32 (created by layer 'conv2d_input')>
    ```

    - input으로 해당 모델에 입력되는 인풋값의 정보를 알 수 있다.

    ```python
    conv_act1 = keras.Model(m.input, m.layers[0].output)
    
    (x_data, y_data), (t_x_data, t_y_data) = keras.datasets.fashion_mnist.load_data()
    in_data = x_data.reshape(-1, 28, 28, 1) / 255.0
    ```

    - 첫번째 컨볼루션 층을 모델로 만든다. 해당 모델의 출력은 피쳐맵이다.
    - 데이터는 케라스에서 제공하는 데이터를 불러와서 사용한다.

    ```python
    plt.imshow(x_data[0], cmap = 'gray_r')
    ```

    ![j5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j5.png?raw=true)

    - 불러온 데이터는 의류에 관련된 사진들이 담겨있다.

    ```python
    f_map = conv_act1.predict(in_data[0:1])
    f_map.shape
    
    # (1, 28, 28, 32)
    ```

    - 피쳐맵의 모양 확인
    - 1장의 이미지에 28x28픽셀을 가진 특징이 32개 존재한다.

    ```python
    fig, axs = plt.subplots(4, 8, figsize = (15, 8))
    
    for i in range(4):
        for j in range(8):
            axs[i, j].imshow(f_map[0, :, :, i * 8 + j])
            axs[i, j].axis('off')
    ```

    ![j6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j6.png?raw=true)

    - 피쳐맵을 시각화한 이미지이다.

    ```python
    conv_act2 = keras.Model(m.input, m.layers[2].output)
    f2_map = conv_act2.predict(in_data[0:1])
    f2_map.shape
    
    # (1, 14, 14, 64)
    ```

    - 두번째 컨볼루션 층을 통과한 피쳐맵의 모양
    - 풀링과 컨볼루션을 거쳐서 이미지의 크기 14x14로 줄고 특징은 64개로 증가했다.

    ```python
    fig, axs = plt.subplots(8, 8, figsize = (8, 8))
    
    for i in range(8):
        for j in range(8):
            axs[i, j].imshow(f2_map[0, :, :, i * 8 + j])
            axs[i, j].axis('off')
    ```

    ![j7](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j7.png?raw=true)

    - 두번째 컨볼루션 층의 피쳐맵을 시각화한 이미지

  - 실습

    - 데이터를 불러와 데이터 전처리, CNN 모델 생성 및 학습, 테스트 데이터를 이용한 결과 시각화, 각 컨볼루션 층의 피쳐맵을 시각화해본다.

    ```python
    import matplotlib.pyplot as plt
    import tensorflow as tf
    import numpy as np
    from keras.datasets.mnist import load_data
    from tensorflow import keras
    from sklearn.model_selection import train_test_split
    np.random.seed(42)
    tf.random.set_seed(42)
    
    (x_data, y_data), (t_x_data, t_y_data) = load_data()
    x_data.shape, np.unique(y_data)
    
    # ((60000, 28, 28), array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8))
    ```

    - 데이터는 케라스에서 제공하는 손글씨 데이터를 불러와 사용한다.
    - 해당 데이터는 28x28 픽셀의 이미지이고 타겟 데이터는 총 10개인 것을 확인할 수 있다.

    ```python
    x_data = x_data.reshape(-1, 28, 28, 1) / 255.0
    tt_x = t_x_data.reshape(-1, 28, 28, 1) / 255.0
    tt_y = t_y_data
    
    t_x, v_x, t_y, v_y = train_test_split(x_data, y_data, test_size = 0.2, random_state = 42)
    ```

    - 이미지 데이터의 특성에 맞게 전처리해준다.
    - 모든 데이터의 채널을 하나 더 늘려주고 255를 나눠 0~1로 스케일링해준다.
    - 전처리된 데이터로 테스트 데이터, 검증 데이터로 분류한다.

    ```python
    m = keras.Sequential()
    m.add(keras.layers.Conv2D(64, kernel_size = 3, activation = 'relu', padding = 'same', input_shape = (28, 28, 1)))
    m.add(keras.layers.MaxPool2D(2))
    m.add(keras.layers.Conv2D(128, kernel_size = 3, activation = 'relu', padding = 'same'))
    m.add(keras.layers.MaxPool2D(2))
    m.add(keras.layers.Flatten())
    m.add(keras.layers.Dense(100, activation = 'relu'))
    m.add(keras.layers.Dropout(0.3))
    m.add(keras.layers.Dense(10, activation = 'softmax'))
    m.summary()
    ```

    - 컨볼루션은 총 2층, 커널은 각각 64개, 128개로
    - 뉴럴 네트워크는 2층,  과대적합을 방지하기 위해 dropout을 설정해준다.

    ```python
    m.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')
    ck_p = keras.callbacks.ModelCheckpoint('best_q2m.h5', save_best_only = True)
    e_st = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
    m.fit(t_x, t_y, epochs = 20, validation_data = (v_x, v_y), callbacks = [ck_p, e_st])
    ```

    - 학습할 때 콜백함수를 지정해 최적의 모델을 찾을 수 있게 한다.

    ```python
    m.evaluate(v_x, v_y)
    
    # [0.03373720869421959, 0.9916666746139526]
    ```

    - 검증 데이터에 대한 손실률과 정확도

    ```python
    for i in range(5):
        rl = m.predict(tt_x[i:i+1])
        print(np.argmax(rl))
        plt.imshow(tt_x[i].reshape(28, 28), cmap = 'gray_r')
        plt.show()
        print()
        
    # 7 2 1 0 4
    ```

    ![j9](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j9.png?raw=true)

    ![j10](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j10.png?raw=true)

    ![j11](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j11.png?raw=true)

    ![j12](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j12.png?raw=true)

    ![j13](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j13.png?raw=true)

    - 테스트 데이터를 이용해 결과를 시각화한 이미지이다.
    - 실제값과 예측값이 잘 맞아 떨어지는 것을 볼 수 있다.

    ```python
    m.evaluate(tt_x, tt_y)
    
    # [0.023752376437187195, 0.9922999739646912]
    ```

    - 테스트 데이터에 대한 손실률과 정확도

    ```python
    conv_act1 = keras.Model(m.input, m.layers[0].output)
    
    for x in range(5):
        f_map = conv_act1.predict(x_data[x:x+1])
        
        fig, axs = plt.subplots(8, 8, figsize = (8, 8))
        
        for i in range(8):
            for j in range(8):
                axs[i, j].imshow(f_map[0,:,:,i * 8 + j])
                axs[i, j].axis('off')
    ```

    ![j14](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j14.png?raw=true)

    ![j15](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j15.png?raw=true)

    ![j16](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j16.png?raw=true)

    ![j17](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j17.png?raw=true)

    ![j18](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j18.png?raw=true)

    - 불러온 데이터의 0~4번까지에 해당하는 컨볼루션 1층을 통과한 피쳐맵을 시각화한 이미지이다.

    ```python
    conv_act2 = keras.Model(m.input, m.layers[2].output)
    
    for x in range(5):
        f2_map = conv_act2.predict(x_data[x:x+1])
        
        fig, axs = plt.subplots(8, 16, figsize = (8, 4))
        
        for i in range(8):
            for j in range(16):
                axs[i, j].imshow(f2_map[0,:,:,i * 8 + j])
                axs[i, j].axis('off')
    ```

    ![j19](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j19.png?raw=true)

    ![j20](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j20.png?raw=true)

    ![j21](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j21.png?raw=true)

    ![j22](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j22.png?raw=true)

    ![j23](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j23.png?raw=true)

    - 불러온 데이터의 0~4번까지에 해당하는 컨볼루션 1층을 통과한 피쳐맵을 시각화한 이미지이다.

<br/>

---

### RNN

<br/>

- 순차 데이터

  - 순서를 고려한다
  - 시퀀스 원소들은 특정 순서가 있으므로 상호 독립적이지 않다.
  - 텍스트, 시계열 데이터들이 대표적

  <br/>

- RNN

  - 시퀀스를 다룰 때는 지금까지 해왔던 가정들이 유효하지 않음
  - 시퀀스의 정의가 순서를 고려하기 때문
  - 특정 주식의 가격을 예측하는 것이 이런 경우에 해당

  <br/>

- 시퀀스 모델링의 종류

  - 시퀀스 모델링에는 언어 번역, 이미지 캡셔닝, 텍스트 생성 같은 다양한 애플리케이션이 존재함

  - 적절한 구조와 방법을 찾으려면 여러 종류의 시퀀스 모델링 작업 사이의 차이점을 이해하고 구별할 수 있어야 함

    ![i1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/i1.png?raw=true)

  - 다대일

    - 입력 데이터가 시퀀스이지만 출력은 시퀀스가 아니고 고정
    - 예시) 감성 분석에서 입력은 텍스트(영화 리뷰 등)이고 출력은 클래스

  - 일대다

    - 입력 데이터가 시퀀스가 아니라 일반적인 형태이고 출력은 시퀀스
    - 예시) 이미지 캡셔닝 - 입력이 이미지, 출력은 이미지 내용을 요약한 영어 문장

  <br/>

- RNN 반복구조

  - 기본 피드포워드 네트워크에서 정보는 입력에서 은닉층으로 흐른 후 은닉층에서 출력층으로 전달

  - 반면 순환 네트워크에서는 은닉층이 현재 타임 스템의 입력층과 이전 타임 스텝의 은닉층으로부터 정보를 받음

  - 즉, 이전의 정보를 기억하고 지금의 정보를 처리함

    ![i2](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/i2.png?raw=true)

  - 단일층 RNN과 다층 RNN

    ![i3](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/i3.png?raw=true)

  - 수식

    ![i4](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/i4.png?raw=true)

    ![m1](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/m1.png?raw=true)

  - RNN의 옵티마이저 함수는 하이퍼볼릭 탄젠트를 많이 사용한다.

  <br/>

- 시계열

  - 타임 스텝마다 하나 이상의 값을 가진 시퀀스
  - 단변량 시계열 : 웹사이트에서 시간당 접속 사용자의 수, 도시의 날짜별 온도 등 타임 스텝마다 하나의 값이 존재하는 시계열
  - 다변량 시계열 : 기업의 분기별 재정 안정성 지표(회사의 수입, 부채 등)처럼, 타임 스텝마다 여러 값이 존재하는 시계열

  <br/>

- RNN의 문제

  - 불안정한 그레디언트 문제
  - 학습이 지속되면 이전 기억이 점점 소실됨
  - 그로 인한 단기 기억이 발생

  <br/>

- 단기 기억 문제 해결

  - LSTM 셀
  - 핍홀 연결
  - GRU 셀
  - 1D 합성곱 층을 사용해 시퀀스 처리
  - WAVENET

  <br/>

- LST(장단기 메모리)셀

  ![i5](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/i5.png?raw=true)

  - 학습이 진행될 때마다 소실되는 이전 기억의 비중을 높이고 입력되는 데이터의 비중을 낮춰 이전 기억의 소실을 늦추는 방법

  <br/>

- GRU(게이트 순환 유닛)셀

  ![i6](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/i6.png?raw=true)

  - LST와 비슷하지만 출력게이트가 없어 상대적으로 속도가 빠르다

  <br/>

- 파이썬 코드

  ```python
  from tensorflow.keras.datasets.imdb import load_data
  (x_data, y_data), (tt_x, tt_y) = load_data(num_words = 500)  # 500번 단어까지만 가져옴
  x_data.shape
  
  # (25000,)
  ```

  - 케라스에서 제공하는 리뷰 데이터를 불러와 사용한다.
  - num_words를 500으로 설정해 500번까지의 단어만 가져오게 한다.

  ```python
  len(x_data[0]),len(x_data[1])
  
  # (218, 189)
  ```

  - 길이 제약을 주지 않았기 때문에 리뷰의 길이는 모두 제각각 다르다

  ```python
  y_data[:5]
  
  # array([1, 0, 0, 1, 0], dtype=int64)
  ```

  - 타겟데이터는 0과 1로 다대일 모델이다.

  ```python
  from sklearn.model_selection import train_test_split
  t_x, v_x, t_y, v_y = train_test_split(x_data, y_data, test_size = 0.2, random_state = 42)
  
  import numpy as np
  l = np.array([len(i) for i in t_x])
  import matplotlib.pyplot as plt
  plt.hist(l)
  ```

  ![j24](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j24.png?raw=true)

  - 리뷰 데이터의 수와 각각의 단어 수를 히스토그램으로 이미지화했다.

  ```python
  from tensorflow.keras.preprocessing.sequence import pad_sequences
  s_t_x = pad_sequences(t_x, maxlen = 100)
  
  s_t_x.shape
  
  # (20000, 100)
  ```

  - pad_sequences를 통해 리뷰의 글자 수를 100자로 줄였다.
  - 이 때 보통 텍스트의 앞 부분을 자르는 식으로 동작하는데, 이는 리뷰의 앞부분 내용은 대부분 본인의 소개와 같이 학습에 필요없는 내용이기 때문이다.

  ```python
  s_v_x=pad_sequences(v_x,maxlen=100)
  ```

  - 검증 데이터도 동일하게 전처리

  ```python
  from tensorflow import keras
  m = keras.Sequential()
  m.add(keras.layers.SimpleRNN(8, input_shape = (100, 500)))
  m.add(keras.layers.Dense(1, activation = 'sigmoid'))
  ```

  - 모델의 입력은 (100, 500)으로 들어오도록 한다.
  - 리뷰 길이는 100, 단어 종류는 500개로 설정했기 때문

  ```python
  t_oh = keras.utils.to_categorical(s_t_x)  # 원핫 인코딩
  t_oh.shape
  
  # (20000, 100, 500)
  ```

  - 전처리한 데이터를 입력 모양에 맞게 원핫 인코딩한다.

  ```python
  v_oh=keras.utils.to_categorical(s_v_x)
  ```

  - 검증 데이터도 마찬가지

  ```python
  ot = keras.optimizers.RMSprop(learning_rate = 1e-4)
  m.compile(optimizer = ot,loss = 'binary_crossentropy', metrics = 'accuracy')
  ck_p = keras.callbacks.ModelCheckpoint('Ex2.h5', save_best_only = True)
  e_stopping = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
  hy = m.fit(t_oh, t_y, epochs = 100, batch_size = 64, validation_data = (v_oh,v_y)
          , callbacks = [ck_p,e_stopping])
  ```

  - 옵티마이저는 RMS를 사용, 직접 설정하여 학습률을 조정해준다.
  - 콜백함수를 지정하여 최적의 모델을 저장한다.
  - 층이 얕기때문에 epoch를 100번 반복한다.

  ```python
  plt.plot(hy.history['loss'])
  plt.plot(hy.history['val_loss'])
  ```

  ![j25](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j25.png?raw=true)

  - 10번째 epoch부터 검증데이터의 손실률이 0으로 느리게 수렴하는 것을 볼 수 있다.
  - 35번째 epoch부터 검증데이터의 손실률이 줄어들지 않고 있는 것을 볼 수 있다.
  - 반면에 테스트데이터의 손실률은 계속해서 줄어들고 있다.(=과대적합)

  ```python
  m1 = keras.Sequential()
  m1.add(keras.layers.Embedding(500, 16, input_length = 100))
  m1.add(keras.layers.SimpleRNN(8))
  m1.add(keras.layers.Dense(1, activation = 'sigmoid'))
  ot = keras.optimizers.RMSprop(learning_rate = 1e-4)
  m1.compile(optimizer = ot, loss = 'binary_crossentropy', metrics = 'accuracy')
  ck_p = keras.callbacks.ModelCheckpoint('Ex2_emd.h5', save_best_only = True)
  e_stopping = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
  hy1 = m1.fit(s_t_x, t_y, epochs = 100, batch_size = 64, validation_data = (s_v_x,v_y), callbacks = [ck_p,e_stopping])
  ```

  - 과대적합을 방지하기 위해 단어 임베딩 후에 학습을 진행한다.

    ![j26](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j26.png?raw=true)

  ```python
  m2 = keras.Sequential()
  m2.add(keras.layers.Embedding(500, 16, input_length = 100))
  m2.add(keras.layers.LSTM(8, dropout = 0.3))
  m2.add(keras.layers.Dense(1, activation = 'sigmoid'))
  
  ot = keras.optimizers.RMSprop(learning_rate = 1e-4)
  m2.compile(optimizer = ot, loss = 'binary_crossentropy', metrics = 'accuracy')
  ck_p = keras.callbacks.ModelCheckpoint('Ex2_LSTM.h5', save_best_only = True)
  e_stopping = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
  hy2 = m2.fit(s_t_x, t_y, epochs = 100, batch_size = 64, validation_data = (s_v_x,v_y), callbacks = [ck_p,e_stopping])
  ```

  - SimpleRNN 모델 대신 LST모델을 사용해본다.

    ![j27](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j27.png?raw=true)

  ```python
  m3 = keras.Sequential()
  m3.add(keras.layers.Embedding(500, 16, input_length = 100))
  m3.add(keras.layers.LSTM(8, dropout = 0.3, return_sequences = True))
  m3.add(keras.layers.LSTM(8, dropout = 0.3))
  m3.add(keras.layers.Dense(1, activation = 'sigmoid'))
  
  ot = keras.optimizers.RMSprop(learning_rate = 1e-4)
  m3.compile(optimizer = ot,loss = 'binary_crossentropy', metrics = 'accuracy')
  ck_p = keras.callbacks.ModelCheckpoint('Ex2_LSTM_M.h5', save_best_only = True)
  e_stopping = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
  hy3 = m3.fit(s_t_x, t_y, epochs = 100, batch_size = 64, validation_data = (s_v_x,v_y), callbacks = [ck_p, e_stopping])
  ```

  - LST모델을 사용한 다층 RNN이다.
  - RNN 셀들은 시퀀스 데이터를 입력으로 받아야하기 때문에 첫번째 LST층에 return_sequences 옵션을 True로 하여 출력되는 데이터가 시퀀스이도록 한다.

  ```python
  plt.plot(hy3.history['loss'])
  plt.plot(hy3.history['val_loss'])
  ```

  ![j28](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j28.png?raw=true)

  ```python
  m4 = keras.Sequential()
  m4.add(keras.layers.Embedding(500, 16, input_length = 100))
  m4.add(keras.layers.GRU(8, dropout = 0.3, return_sequences = True))
  m4.add(keras.layers.GRU(8, dropout = 0.3))
  m4.add(keras.layers.Dense(1, activation = 'sigmoid'))
  
  ot = keras.optimizers.RMSprop(learning_rate = 1e-4)
  m4.compile(optimizer = ot, loss = 'binary_crossentropy', metrics = 'accuracy')
  ck_p = keras.callbacks.ModelCheckpoint('Ex2_GRU_M.h5', save_best_only = True)
  e_stopping = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)
  hy4 = m4.fit(s_t_x, t_y, epochs = 100, batch_size = 64, validation_data = (s_v_x, v_y), callbacks=[ck_p, e_stopping])
  ```

  - GRU 셀을 이용한 다층 RNN이다.
  - 셀이 달라진 것 외에 위 코드와 달라진 것은 없다.

  ```python
  plt.plot(hy4.history['loss'])
  plt.plot(hy4.history['val_loss'])
  ```

  ![j29](https://github.com/Cheolyong-Kim/TIL/blob/master/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B8%B0%EC%B4%88%2010/j29.png?raw=true)

  


​    

